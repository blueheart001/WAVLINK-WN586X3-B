From 2313cbd9a9972e4fe4e07d91cf6328907851c8f9 Mon Sep 17 00:00:00 2001
From: Luo Jie <quic_luoj@quicinc.com>
Date: Thu, 22 May 2025 11:16:12 +0800
Subject: [PATCH 2/3] driver: net: qualcomm: Update IPQ9574 PPE driver

Update PPE driver to keep aligned with the upstream patches. The
updated PPE driver also fix some known issues such as GRO/GSO/TSO
issue for the performance improvement.

Signed-off-by: Luo Jie <quic_luoj@quicinc.com>
---
 .../bindings/net/qcom,ipq9574-ppe.yaml        |  406 +++++++
 .../device_drivers/ethernet/index.rst         |    1 +
 .../ethernet/qualcomm/ppe/ppe.rst             |  445 +++++++
 drivers/net/ethernet/qualcomm/Kconfig         |    4 +-
 drivers/net/ethernet/qualcomm/ppe/Makefile    |    2 +-
 drivers/net/ethernet/qualcomm/ppe/edma.c      |  115 +-
 drivers/net/ethernet/qualcomm/ppe/edma.h      |    8 +-
 .../net/ethernet/qualcomm/ppe/edma_cfg_rx.c   |  137 ++-
 .../net/ethernet/qualcomm/ppe/edma_cfg_rx.h   |    3 +-
 .../net/ethernet/qualcomm/ppe/edma_cfg_tx.c   |   21 +-
 .../net/ethernet/qualcomm/ppe/edma_cfg_tx.h   |    2 +-
 .../net/ethernet/qualcomm/ppe/edma_debugfs.c  |    2 +-
 .../net/ethernet/qualcomm/ppe/edma_ethtool.c  |   31 +-
 drivers/net/ethernet/qualcomm/ppe/edma_port.c |   20 +-
 drivers/net/ethernet/qualcomm/ppe/edma_port.h |    4 +-
 drivers/net/ethernet/qualcomm/ppe/edma_rx.c   |   65 +-
 drivers/net/ethernet/qualcomm/ppe/edma_rx.h   |    2 +-
 drivers/net/ethernet/qualcomm/ppe/edma_tx.c   |   98 +-
 drivers/net/ethernet/qualcomm/ppe/edma_tx.h   |   16 +-
 drivers/net/ethernet/qualcomm/ppe/ppe.c       |   39 +-
 drivers/net/ethernet/qualcomm/ppe/ppe.h       |    6 +-
 drivers/net/ethernet/qualcomm/ppe/ppe_api.c   |   60 +-
 drivers/net/ethernet/qualcomm/ppe/ppe_api.h   |   57 +-
 .../net/ethernet/qualcomm/ppe/ppe_config.c    | 1070 +++++++++--------
 .../net/ethernet/qualcomm/ppe/ppe_config.h    |  139 ++-
 .../net/ethernet/qualcomm/ppe/ppe_debugfs.c   |  778 ++++++------
 .../net/ethernet/qualcomm/ppe/ppe_debugfs.h   |    2 +-
 drivers/net/ethernet/qualcomm/ppe/ppe_port.c  |  721 ++++++-----
 drivers/net/ethernet/qualcomm/ppe/ppe_port.h  |   24 +-
 drivers/net/ethernet/qualcomm/ppe/ppe_regs.h  |  473 ++++----
 30 files changed, 2932 insertions(+), 1819 deletions(-)
 create mode 100644 Documentation/devicetree/bindings/net/qcom,ipq9574-ppe.yaml
 create mode 100644 Documentation/networking/device_drivers/ethernet/qualcomm/ppe/ppe.rst

diff --git a/Documentation/devicetree/bindings/net/qcom,ipq9574-ppe.yaml b/Documentation/devicetree/bindings/net/qcom,ipq9574-ppe.yaml
new file mode 100644
index 000000000..f36f4d180
--- /dev/null
+++ b/Documentation/devicetree/bindings/net/qcom,ipq9574-ppe.yaml
@@ -0,0 +1,406 @@
+# SPDX-License-Identifier: GPL-2.0 OR BSD-2-Clause
+%YAML 1.2
+---
+$id: http://devicetree.org/schemas/net/qcom,ipq9574-ppe.yaml#
+$schema: http://devicetree.org/meta-schemas/core.yaml#
+
+title: Qualcomm IPQ packet process engine (PPE)
+
+maintainers:
+  - Luo Jie <quic_luoj@quicinc.com>
+  - Lei Wei <quic_leiwei@quicinc.com>
+  - Suruchi Agarwal <quic_suruchia@quicinc.com>
+  - Pavithra R <quic_pavir@quicinc.com>>
+
+description:
+  The Ethernet functionality in the PPE (Packet Process Engine) is comprised
+  of three components, the switch core, port wrapper and Ethernet DMA.
+
+  The Switch core in the IPQ9574 PPE has maximum of 6 front panel ports and
+  two FIFO interfaces. One of the two FIFO interfaces is used for Ethernet
+  port to host CPU communication using Ethernet DMA. The other is used
+  communicating to the EIP engine which is used for IPsec offload. On the
+  IPQ9574, the PPE includes 6 GMAC/XGMACs that can be connected with external
+  Ethernet PHY. Switch core also includes BM (Buffer Management), QM (Queue
+  Management) and SCH (Scheduler) modules for supporting the packet processing.
+
+  The port wrapper provides connections from the 6 GMAC/XGMACS to UNIPHY (PCS)
+  supporting various modes such as SGMII/QSGMII/PSGMII/USXGMII/10G-BASER. There
+  are 3 UNIPHY (PCS) instances supported on the IPQ9574.
+
+  Ethernet DMA is used to transmit and receive packets between the six Ethernet
+  ports and ARM host CPU.
+
+  The follow diagram shows the PPE hardware block along with its connectivity
+  to the external hardware blocks such clock hardware blocks (CMNPLL, GCC,
+  NSS clock controller) and ethernet PCS/PHY blocks. For depicting the PHY
+  connectivity, one 4x1 Gbps PHY (QCA8075) and two 10 GBps PHYs are used as an
+  example.
+  - |
+           +---------+
+           |  48 MHZ |
+           +----+----+
+                |(clock)
+                v
+           +----+----+
+    +------| CMN PLL |
+    |      +----+----+
+    |           |(clock)
+    |           v
+    |      +----+----+           +----+----+  (clock) +----+----+
+    |  +---|  NSSCC  |           |   GCC   |--------->|   MDIO  |
+    |  |   +----+----+           +----+----+          +----+----+
+    |  |        |(clock & reset)      |(clock)
+    |  |        v                     v
+    |  |   +-----------------------------+----------+----------+---------+
+    |  |   |       +-----+               |EDMA FIFO |          | EIP FIFO|
+    |  |   |       | SCH |               +----------+          +---------+
+    |  |   |       +-----+                        |              |       |
+    |  |   |  +------+   +------+               +-------------------+    |
+    |  |   |  |  BM  |   |  QM  |  IPQ9574-PPE  |    L2/L3 Process  |    |
+    |  |   |  +------+   +------+               +-------------------+    |
+    |  |   |                                             |               |
+    |  |   | +-------+ +-------+ +-------+ +-------+ +-------+ +-------+ |
+    |  |   | |  MAC0 | |  MAC1 | |  MAC2 | |  MAC3 | | XGMAC4| |XGMAC5 | |
+    |  |   | +---+---+ +---+---+ +---+---+ +---+---+ +---+---+ +---+---+ |
+    |  |   |     |         |         |         |         |         |     |
+    |  |   +-----+---------+---------+---------+---------+---------+-----+
+    |  |         |         |         |         |         |         |
+    |  |     +---+---------+---------+---------+---+ +---+---+ +---+---+
+    +--+---->|                PCS0                 | |  PCS1 | | PCS2  |
+    |(clock) +---+---------+---------+---------+---+ +---+---+ +---+---+
+    |            |         |         |         |         |         |
+    |        +---+---------+---------+---------+---+ +---+---+ +---+---+
+    +------->|             QCA8075 PHY             | | PHY4  | | PHY5  |
+     (clock) +-------------------------------------+ +-------+ +-------+
+
+properties:
+  compatible:
+    enum:
+      - qcom,ipq9574-ppe
+
+  reg:
+    maxItems: 1
+
+  clocks:
+    items:
+      - description: PPE core clock from NSS clock controller
+      - description: PPE APB (Advanced Peripheral Bus) clock from NSS clock controller
+      - description: PPE ingress process engine clock from NSS clock controller
+      - description: PPE BM, QM and scheduler clock from NSS clock controller
+
+  clock-names:
+    items:
+      - const: ppe
+      - const: apb
+      - const: ipe
+      - const: btq
+
+  resets:
+    maxItems: 1
+    description: PPE reset, which is necessary before configuring PPE hardware
+
+  interconnects:
+    items:
+      - description: Clock path leading to PPE switch core function
+      - description: Clock path leading to PPE register access
+      - description: Clock path leading to QoS generation
+      - description: Clock path leading to timeout reference
+      - description: Clock path leading to NSS NOC from memory NOC
+      - description: Clock path leading to memory NOC from NSS NOC
+      - description: Clock path leading to enhanced memory NOC from NSS NOC
+
+  interconnect-names:
+    items:
+      - const: ppe
+      - const: ppe_cfg
+      - const: qos_gen
+      - const: timeout_ref
+      - const: nssnoc_memnoc
+      - const: memnoc_nssnoc
+      - const: memnoc_nssnoc_1
+
+  ethernet-dma:
+    type: object
+    additionalProperties: false
+    description:
+      EDMA (Ethernet DMA) is used to transmit packets between PPE and ARM
+      host CPU. There are 32 TX descriptor rings, 32 TX completion rings,
+      24 RX descriptor rings and 8 RX fill rings supported.
+
+    properties:
+      clocks:
+        items:
+          - description: EDMA system clock from NSS Clock Controller
+          - description: EDMA APB (Advanced Peripheral Bus) clock from
+              NSS Clock Controller
+
+      clock-names:
+        items:
+          - const: sys
+          - const: apb
+
+      resets:
+        maxItems: 1
+        description: EDMA reset from NSS clock controller
+
+      interrupts:
+        minItems: 29
+        maxItems: 57
+
+      interrupt-names:
+        minItems: 29
+        maxItems: 57
+        items:
+          pattern: '^(txcmpl_([0-9]|[1-2][0-9]|3[0-1])|rxdesc_([0-9]|1[0-9]|2[0-3])|misc)$'
+        description:
+          Interrupts "txcmpl_[0-31]" are the Ethernet DMA Tx completion ring interrupts.
+          Interrupts "rxdesc_[0-23]" are the Ethernet DMA Rx Descriptor ring interrupts.
+          Interrupt "misc" is the Ethernet DMA miscellaneous error interrupt.
+
+    required:
+      - clocks
+      - clock-names
+      - resets
+      - interrupts
+      - interrupt-names
+
+required:
+  - compatible
+  - reg
+  - clocks
+  - clock-names
+  - resets
+  - interconnects
+  - interconnect-names
+  - ethernet-dma
+
+allOf:
+  - $ref: ethernet-switch.yaml
+
+unevaluatedProperties: false
+
+examples:
+  - |
+    #include <dt-bindings/clock/qcom,ipq9574-gcc.h>
+    #include <dt-bindings/interconnect/qcom,ipq9574.h>
+    #include <dt-bindings/interrupt-controller/arm-gic.h>
+
+    ethernet-switch@3a000000 {
+        compatible = "qcom,ipq9574-ppe";
+        reg = <0x3a000000 0xbef800>;
+        clocks = <&nsscc 80>,
+                 <&nsscc 79>,
+                 <&nsscc 81>,
+                 <&nsscc 78>;
+        clock-names = "ppe",
+                      "apb",
+                      "ipe",
+                      "btq";
+        resets = <&nsscc 108>;
+        interconnects = <&nsscc MASTER_NSSNOC_PPE &nsscc SLAVE_NSSNOC_PPE>,
+                        <&nsscc MASTER_NSSNOC_PPE_CFG &nsscc SLAVE_NSSNOC_PPE_CFG>,
+                        <&gcc MASTER_NSSNOC_QOSGEN_REF &gcc SLAVE_NSSNOC_QOSGEN_REF>,
+                        <&gcc MASTER_NSSNOC_TIMEOUT_REF &gcc SLAVE_NSSNOC_TIMEOUT_REF>,
+                        <&gcc MASTER_MEM_NOC_NSSNOC &gcc SLAVE_MEM_NOC_NSSNOC>,
+                        <&gcc MASTER_NSSNOC_MEMNOC &gcc SLAVE_NSSNOC_MEMNOC>,
+                        <&gcc MASTER_NSSNOC_MEM_NOC_1 &gcc SLAVE_NSSNOC_MEM_NOC_1>;
+        interconnect-names = "ppe",
+                             "ppe_cfg",
+                             "qos_gen",
+                             "timeout_ref",
+                             "nssnoc_memnoc",
+                             "memnoc_nssnoc",
+                             "memnoc_nssnoc_1";
+
+        ethernet-dma {
+            clocks = <&nsscc 77>,
+                     <&nsscc 76>;
+            clock-names = "sys",
+                          "apb";
+            resets = <&nsscc 0>;
+            interrupts = <GIC_SPI 371 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 372 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 373 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 374 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 375 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 376 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 377 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 378 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 379 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 380 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 381 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 382 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 383 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 384 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 509 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 508 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 507 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 506 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 505 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 504 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 503 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 502 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 501 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 500 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 351 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 352 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 353 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 354 IRQ_TYPE_LEVEL_HIGH>,
+                         <GIC_SPI 499 IRQ_TYPE_LEVEL_HIGH>;
+            interrupt-names = "txcmpl_8",
+                              "txcmpl_9",
+                              "txcmpl_10",
+                              "txcmpl_11",
+                              "txcmpl_12",
+                              "txcmpl_13",
+                              "txcmpl_14",
+                              "txcmpl_15",
+                              "txcmpl_16",
+                              "txcmpl_17",
+                              "txcmpl_18",
+                              "txcmpl_19",
+                              "txcmpl_20",
+                              "txcmpl_21",
+                              "txcmpl_22",
+                              "txcmpl_23",
+                              "txcmpl_24",
+                              "txcmpl_25",
+                              "txcmpl_26",
+                              "txcmpl_27",
+                              "txcmpl_28",
+                              "txcmpl_29",
+                              "txcmpl_30",
+                              "txcmpl_31",
+                              "rxdesc_20",
+                              "rxdesc_21",
+                              "rxdesc_22",
+                              "rxdesc_23",
+                              "misc";
+        };
+
+        ethernet-ports {
+            #address-cells = <1>;
+            #size-cells = <0>;
+
+            port@1 {
+                reg = <1>;
+                phy-mode = "qsgmii";
+                managed = "in-band-status";
+                phy-handle = <&phy0>;
+                pcs-handle = <&pcs0_mii0>;
+                clocks = <&nsscc 33>,
+                         <&nsscc 34>,
+                         <&nsscc 37>;
+                clock-names = "mac",
+                              "rx",
+                              "tx";
+                resets = <&nsscc 29>,
+                         <&nsscc 96>,
+                         <&nsscc 97>;
+                reset-names = "mac",
+                              "rx",
+                              "tx";
+            };
+
+            port@2 {
+                reg = <2>;
+                phy-mode = "qsgmii";
+                managed = "in-band-status";
+                phy-handle = <&phy1>;
+                pcs-handle = <&pcs0_mii1>;
+                clocks = <&nsscc 40>,
+                         <&nsscc 41>,
+                         <&nsscc 44>;
+                clock-names = "mac",
+                              "rx",
+                              "tx";
+                resets = <&nsscc 30>,
+                         <&nsscc 98>,
+                         <&nsscc 99>;
+                reset-names = "mac",
+                              "rx",
+                              "tx";
+            };
+
+            port@3 {
+                reg = <3>;
+                phy-mode = "qsgmii";
+                managed = "in-band-status";
+                phy-handle = <&phy2>;
+                pcs-handle = <&pcs0_mii2>;
+                clocks = <&nsscc 47>,
+                         <&nsscc 48>,
+                         <&nsscc 51>;
+                clock-names = "mac",
+                              "rx",
+                              "tx";
+                resets = <&nsscc 31>,
+                         <&nsscc 100>,
+                         <&nsscc 101>;
+                reset-names = "mac",
+                              "rx",
+                              "tx";
+            };
+
+            port@4 {
+                reg = <4>;
+                phy-mode = "qsgmii";
+                managed = "in-band-status";
+                phy-handle = <&phy3>;
+                pcs-handle = <&pcs0_mii3>;
+                clocks = <&nsscc 54>,
+                         <&nsscc 55>,
+                         <&nsscc 58>;
+                clock-names = "mac",
+                              "rx",
+                              "tx";
+                resets = <&nsscc 32>,
+                         <&nsscc 102>,
+                         <&nsscc 103>;
+                reset-names = "mac",
+                              "rx",
+                              "tx";
+            };
+
+            port@5 {
+                reg = <5>;
+                phy-mode = "usxgmii";
+                managed = "in-band-status";
+                phy-handle = <&phy4>;
+                pcs-handle = <&pcs1_mii0>;
+                clocks = <&nsscc 61>,
+                         <&nsscc 62>,
+                         <&nsscc 65>;
+                clock-names = "mac",
+                              "rx",
+                              "tx";
+                resets = <&nsscc 33>,
+                         <&nsscc 104>,
+                         <&nsscc 105>;
+                reset-names = "mac",
+                              "rx",
+                              "tx";
+            };
+
+            port@6 {
+                reg = <6>;
+                phy-mode = "usxgmii";
+                managed = "in-band-status";
+                phy-handle = <&phy5>;
+                pcs-handle = <&pcs2_mii0>;
+                clocks = <&nsscc 68>,
+                         <&nsscc 69>,
+                         <&nsscc 72>;
+                clock-names = "mac",
+                              "rx",
+                              "tx";
+                resets = <&nsscc 34>,
+                         <&nsscc 106>,
+                         <&nsscc 107>;
+                reset-names = "mac",
+                              "rx",
+                              "tx";
+            };
+        };
+    };
diff --git a/Documentation/networking/device_drivers/ethernet/index.rst b/Documentation/networking/device_drivers/ethernet/index.rst
index 9827e8160..17af0603d 100644
--- a/Documentation/networking/device_drivers/ethernet/index.rst
+++ b/Documentation/networking/device_drivers/ethernet/index.rst
@@ -46,6 +46,7 @@ Contents:
    neterion/s2io
    netronome/nfp
    pensando/ionic
+   qualcomm/ppe/ppe
    smsc/smc9
    stmicro/stmmac
    ti/cpsw
diff --git a/Documentation/networking/device_drivers/ethernet/qualcomm/ppe/ppe.rst b/Documentation/networking/device_drivers/ethernet/qualcomm/ppe/ppe.rst
new file mode 100644
index 000000000..5ac336c17
--- /dev/null
+++ b/Documentation/networking/device_drivers/ethernet/qualcomm/ppe/ppe.rst
@@ -0,0 +1,445 @@
+.. SPDX-License-Identifier: GPL-2.0
+
+===============================================
+PPE Ethernet Driver for Qualcomm IPQ SoC Family
+===============================================
+
+Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
+
+Author: Lei Wei <quic_leiwei@quicinc.com>
+
+
+Contents
+========
+
+- `PPE Overview`_
+- `PPE Driver Overview`_
+- `PPE Port MAC Interface`_
+- `PPE Driver Supported SoCs`_
+- `Enabling the Driver`_
+- `Command Line Parameters`_
+- `Ethernet DMA (EDMA)`_
+- `Debugging`_
+
+
+PPE Overview
+============
+
+IPQ (Qualcomm Internet Processor) SoC (System-on-Chip) series is Qualcomm's series of
+networking SoC for Wi-Fi access points. The PPE (Packet Process Engine) is the Ethernet
+packet process engine in the IPQ SoC.
+
+Below is a simplified hardware diagram of IPQ9574 SoC which includes the PPE engine and
+other blocks which are in the SoC but outside the PPE engine. These blocks work together
+to enable the Ethernet for the IPQ SoC::
+
+             +------+ +------+ +------+ +------+ +------+  +------+ start +-------+
+             |netdev| |netdev| |netdev| |netdev| |netdev|  |netdev|<------|PHYLINK|
+             +------+ +------+ +------+ +------+ +------+  +------+ stop  +-+-+-+-+
+                                           |                                | | ^
+ +-------+   +-------------------------+--------+----------------------+    | | |
+ | GCC   |   |                         |  EDMA  |                      |    | | |
+ +---+---+   |  PPE                    +---+----+                      |    | | |
+     | clk   |                             |                           |    | | |
+     +------>| +-----------------------+------+-----+---------------+  |    | | |
+             | |   Switch Core         |Port0 |     |Port7(EIP FIFO)|  |    | | |
+             | |                       +---+--+     +------+--------+  |    | | |
+             | |                           |               |        |  |    | | |
+ +-------+   | |                    +------+---------------+----+   |  |    | | |
+ |CMN PLL|   | | +---+ +---+ +----+ | +--------+                |   |  |    | | |
+ +---+---+   | | |BM | |QM | |SCH | | | L2/L3  |  .......       |   |  |    | | |
+ |   |       | | +---+ +---+ +----+ | +--------+                |   |  |    | | |
+ |   |       | |                    +------+--------------------+   |  |    | | |
+ |   |       | |                           |                        |  |    | | |
+ |   v       | | +-----+-+-----+-+-----+-+-+---+--+-----+-+-----+   |  |    | | |
+ | +------+  | | |Port1| |Port2| |Port3| |Port4|  |Port5| |Port6|   |  |    | | |
+ | |NSSCC |  | | +-----+ +-----+ +-----+ +-----+  +-----+ +-----+   |  | mac| | |
+ | +-+-+--+  | | |MAC0 | |MAC1 | |MAC2 | |MAC3 |  |MAC4 | |MAC5 |   |  |<---+ | |
+ | ^ | |clk  | | +-----+-+-----+-+-----+-+-----+--+-----+-+-----+   |  | ops  | |
+ | | | +---->| +----|------|-------|-------|---------|--------|-----+  |      | |
+ | | |       +---------------------------------------------------------+      | |
+ | | |              |      |       |       |         |        |               | |
+ | | |   MII clk    |      QSGMII               USXGMII   USXGMII             | |
+ | | +------------->|      |       |       |         |        |               | |
+ | |              +-------------------------+ +---------+ +---------+         | |
+ | |125/312.5M clk|       (PCS0)            | | (PCS1)  | | (PCS2)  | pcs ops | |
+ | +--------------+       UNIPHY0           | | UNIPHY1 | | UNIPHY2 |<--------+ |
+ +--------------->|                         | |         | |         |           |
+ | 31.25M ref clk +-------------------------+ +---------+ +---------+           |
+ |                   |     |      |      |          |          |                |
+ |              +-----------------------------------------------------+         |
+ |25/50M ref clk| +-------------------------+    +------+   +------+  | link    |
+ +------------->| |      QUAD PHY           |    | PHY4 |   | PHY5 |  |---------+
+                | +-------------------------+    +------+   +------+  | change
+                |                                                     |
+                |                       MDIO bus                      |
+                +-----------------------------------------------------+
+
+The CMN (Common) PLL, NSSCC (Networking Sub System Clock Controller) and GCC (Global
+Clock Controller) blocks are in the SoC and act as clock providers.
+
+The UNIPHY block is in the SoC and provides the PCS (Physical Coding Sublayer) and
+XPCS (10-Gigabit Physical Coding Sublayer) functions to support different interface
+modes between the PPE MAC and the external PHY.
+
+This documentation focuses on the descriptions of PPE engine and the PPE driver.
+
+The Ethernet functionality in the PPE (Packet Process Engine) is comprised of three
+components: the switch core, port wrapper and Ethernet DMA.
+
+The Switch core in the IPQ9574 PPE has maximum of 6 front panel ports and two FIFO
+interfaces. One of the two FIFO interfaces is used for Ethernet port to host CPU
+communication using Ethernet DMA. The other one is used communicating to the EIP
+engine which is used for IPsec offload. On the IPQ9574, the PPE includes 6 GMAC/XGMACs
+that can be connected with external Ethernet PHY. Switch core also includes BM (Buffer
+Management), QM (Queue Management) and SCH (Scheduler) modules for supporting the
+packet processing.
+
+The port wrapper provides connections from the 6 GMAC/XGMACS to UNIPHY (PCS) supporting
+various modes such as SGMII/QSGMII/PSGMII/USXGMII/10G-BASER. There are 3 UNIPHY (PCS)
+instances supported on the IPQ9574.
+
+Ethernet DMA is used to transmit and receive packets between the Ethernet subsystem
+and ARM host CPU.
+
+The following lists the main blocks in the PPE engine which will be driven by this
+PPE driver:
+
+- BM
+    BM is the hardware buffer manager for the PPE switch ports.
+- QM
+    Queue Manager for managing the egress hardware queues of the PPE switch ports.
+- SCH
+    The scheduler which manages the hardware traffic scheduling for the PPE switch ports.
+- L2
+    The L2 block performs the packet bridging in the switch core. The bridge domain is
+    represented by the VSI (Virtual Switch Instance) domain in PPE. FDB learning can be
+    enabled based on the VSI domain and bridge forwarding occurs within the VSI domain.
+- MAC
+    The PPE in the IPQ9574 supports up to six MACs (MAC0 to MAC5) which are corresponding
+    to six switch ports (port1 to port6). The MAC block is connected with external PHY
+    through the UNIPHY PCS block. Each MAC block includes the GMAC and XGMAC blocks and
+    the switch port can select to use GMAC or XMAC through a MUX selection according to
+    the external PHY's capability.
+- EDMA (Ethernet DMA)
+    The Ethernet DMA is used to transmit and receive Ethernet packets between the PPE
+    ports and the ARM cores.
+
+The received packet on a PPE MAC port can be forwarded to another PPE MAC port. It can
+be also forwarded to internal switch port0 so that the packet can be delivered to the
+ARM cores using the Ethernet DMA (EDMA) engine. The Ethernet DMA driver will deliver the
+packet to the corresponding 'netdevice' interface.
+
+The software instantiations of the PPE MAC (netdevice), PCS and external PHYs interact
+with the Linux PHYLINK framework to manage the connectivity between the PPE ports and
+the connected PHYs, and the port link states. This is also illustrated in above diagram.
+
+
+PPE Driver Overview
+===================
+PPE driver is Ethernet driver for the Qualcomm IPQ SoC. It is a single platform driver
+which includes the PPE part and Ethernet DMA part. The PPE part initializes and drives the
+various blocks in PPE switch core such as BM/QM/L2 blocks and the PPE MACs. The EDMA part
+drives the Ethernet DMA for packet transfer between PPE ports and ARM cores, and enables
+the netdevice driver for the PPE ports.
+
+The PPE driver files in drivers/net/ethernet/qualcomm/ppe/ are listed as below:
+
+- Makefile
+- ppe.c
+- ppe.h
+- ppe_config.c
+- ppe_config.h
+- ppe_debugfs.c
+- ppe_debugfs.h
+- ppe_port.c
+- ppe_port.h
+- ppe_regs.h
+
+The ppe.c file contains the main PPE platform driver and undertakes the initialization of
+PPE switch core blocks such as QM, BM and L2. The configuration APIs for these hardware
+blocks are provided in the ppe_config.c file.
+
+The ppe.h defines the PPE device data structure which will be used by PPE driver functions.
+
+The ppe_debugfs.c enables the PPE statistics counters such as PPE port Rx and Tx counters,
+CPU code counters and queue counters.
+
+The ppe_port.c initializes PPE MAC ports and also provides a set of port phylink and MAC
+functions.
+
+Command Line Parameters
+=======================
+
+If the driver is built as a module, module parameters can be used by providing
+them in the command line as mentioned below:
+
+    insmod qcom-ppe.ko [<param1>=<VAL1> <param2>=<VAL2>]
+
+Default values for these parameters are set in the driver itself.
+Some of the parameters that can be changed by user are described below:
+
+page_mode
+---------
+:param: page_mode
+:Valid Range: 0-1 (0=off, 1=on)
+:Default Value: 0
+
+This parameter enables page mode.
+
+rx_buff_size
+------------
+:param: rx_buff_size
+:Valid Range: 0-9000
+:Default Value: 0
+
+This parameter sets the Rx buffer size for Jumbo MRU.
+
+edma_rx_napi_budget
+-------------------
+:Valid Range: 16-512
+:Default Value: 128
+
+This parameter sets the Rx NAPI budget.
+
+edma_tx_napi_budget
+-------------------
+:Valid Range: 16-512
+:Default Value: 512
+
+This parameter sets the Tx NAPI budget.
+
+edma_rx_mitigation_pkt_cnt
+--------------------------
+:Valid Range: 0-256
+:Default Value: 16
+
+This parameter is Rx mitigation packet count value.
+
+edma_rx_mitigation_timer
+------------------------
+:Valid Range: 0-1000
+:Default Value: 25
+
+This parameter is Rx mitigation timer value in microseconds.
+
+edma_tx_mitigation_timer
+------------------------
+:Valid Range: 0-1000
+:Default Value: 25
+
+This parameter is Tx mitigation timer value in microseconds.
+
+edma_tx_mitigation_pkt_cnt
+--------------------------
+:Valid Range: 0-256
+:Default Value: 16
+
+This parameter is Tx mitigation packet count value.
+
+tx_requeue_stop
+---------------
+:Valid Range: 0-1
+:Default Value: 1
+
+This parameter stops requeueing of Tx packets.
+It is recommended to disable this if a Qdisc is enabled on the ethernet netdevice.
+
+Procfs Support
+=======================
+
+EDMA driver leverages the existing Linux kernel procfs file system support
+to configure some of the features. The procfs interface creates the
+/proc/sys/net/edma directory.
+
+RPS (Receive Packet Steering): configure bitmap of cores for RPS.
+The network packets can be distributed among multiple CPUs by setting
+respective bitmap core value in this procfs entry
+``/proc/sys/net/edma/rps_bitmap_cores``
+
+PPE Port MAC Interface
+======================
+
+PPE driver sets up the PPE port instance for each PPE MAC port by parsing the "ethernet-ports"
+DTS node. The port MAC interface provides PHYLINK functions which interact with Linux PHYLINK
+framework and port MAC functions which are used by ethtool and netdev ops.
+
+PHYLINK support
+---------------
+PPE Port PHYLINK supports various interface mode including QUSGMII, USXGMII, 2500BASEX,
+10GBASER and so on. Various link mode including external PHY mode, fixed link mode and SFP
+mode are also supported.
+
+PPE driver uses below two functions to setup and destroy PHYLINK for the dedicated PPE port::
+
+	- int ppe_port_phylink_setup(struct ppe_port *ppe_port, struct net_device *netdev);
+	- void ppe_port_phylink_destroy(struct ppe_port *ppe_port)
+
+PPE driver will create PHYLINK instance and PCS instance for the dedicated PPE MAC port
+in the phylink setup function. The PPE PHYLINK MAC operations are provided when creating
+the PHYLINK instance.
+
+PPE driver will destroy PHYLINK and PCS instance for the dedicated PPE MAC port in the
+phylink destroy function.
+
+PPE port DTS node reference the PCS node and use the "ipq_pcs_get()" and "ipq_pcs_put()"
+APIs exported by Qualcomm IPQ9574 PCS driver to get and put the PHYLINK PCS instance.
+
+The PHYLINK PCS operations are provided by the IPQ9574 PCS driver while the PHYLINK MAC
+operations are provided by PPE driver. Below is the PHYLINK MAC operations in PPE driver
+which are used by PHYLINK.
+
+ppe_port_mac_config::
+
+	ppe_port_mac_config(struct phylink_config *config, unsigned int mode,
+		const struct phylink_link_state *state);
+
+This function is to perform PPE port MAC configurations.
+
+ppe_port_mac_link_up::
+
+	ppe_port_mac_link_up(struct phylink_config *config, struct phy_device *phy,
+		unsigned int mode, phy_interface_t interface, int speed, int duplex,
+		bool tx_pause, bool rx_pause);
+
+ppe_port_mac_link_down::
+
+	ppe_port_mac_link_down(struct phylink_config *config, unsigned int mode,
+		phy_interface_t interface);
+
+Above two functions are to perform PPE port MAC link up and link down configurations.
+
+ppe_port_mac_select_pcs::
+
+	ppe_port_mac_select_pcs(struct phylink_config *config, phy_interface_t interface);
+
+This function is to select the corresponding PCS instance for the PPE port PHYLINK
+instance.
+
+PPE port MAC functions
+----------------------
+
+Below are the PPE port MAC functions which are used by ethtool and netdev ops.
+
+get_stats64::
+
+	void ppe_port_get_stats64(struct ppe_port *ppe_port, struct rtnl_link_stats64 *s);
+
+get_sset_count::
+
+	int ppe_port_get_sset_count(struct ppe_port *ppe_port, int sset);
+
+get_strings::
+
+	void ppe_port_get_strings(struct ppe_port *ppe_port, u32 stringset, u8 *data);
+
+
+get_ethtool_stats::
+
+	void ppe_port_get_ethtool_stats(struct ppe_port *ppe_port, u64 *data);
+
+Above functions are used to get the MAC MIB statistics for the dedicated PPE port.
+
+set_mac_address::
+
+	int ppe_port_set_mac_address(struct ppe_port *ppe_port, u8 *macaddr);
+
+This function is used to set the MAC address for the dedicated PPE port.
+
+set_maxframe::
+
+	int ppe_port_set_maxframe(struct ppe_port *ppe_port, int maxframe_size);
+
+This function is used to set the maximum frame size for the dedicated PPE port.
+
+PPE Driver Supported SoCs
+=========================
+
+The PPE driver supports the following IPQ SoC:
+
+- IPQ9574
+
+
+Enabling the Driver
+===================
+
+The driver is located in the menu structure at::
+
+  -> Device Drivers
+    -> Network device support (NETDEVICES [=y])
+      -> Ethernet driver support
+        -> Qualcomm devices
+          -> Qualcomm Technologies, Inc. PPE Ethernet support
+
+The PPE driver functionally depends on the CMN PLL and NSSCC clock controller drivers.
+It also depends on the Qualcomm IPQ9574 PCS driver which enables PHYLINK PCS instance
+for the PPE MAC port. Please make sure the dependent modules are installed before
+installing the PPE driver module.
+
+Ethernet DMA (EDMA)
+===================
+
+This is a common ethernet DMA block inside PPE, which is used to transmit and
+receive packets between Ethernet ports in the PPE switch, and the ARM cores.
+
+Transmit Process
+----------------
+
+Tx netdevice transmit ops is invoked when the kernel needs to transmit a packet;
+it sets the descriptors in the ring and informs the DMA engine that there is a
+packet ready to be transmitted.
+
+By default, the driver sets the ``NETIF_F_SG`` bit in the features field of
+the ``net_device`` structure, enabling the scatter-gather feature.
+
+Receive Process
+---------------
+
+When One or more packets are received by by NIC interface then an RX ring
+interrupt is generated to process the packet in host. EDMA driver will handle
+this interrupt and schedule a NAPI to process the RX descriptors.
+
+NAPI poll function then reaps the RX descriptors and then constructs SKB
+which is then given to Linux networking stack for further processing.
+
+Interrupt Mitigation
+--------------------
+
+EDMA HW supports interrupt mitigation configuration which dictates the packet
+count or speed at which interrupt will be generated.
+
+Jumbo and Segmentation Offloading
+---------------------------------
+
+Jumbo frames are supported. The netdevice feature flags support GSO.
+
+TSO Support
+-----------
+
+TSO (TCP Segmentation Offload) feature is supported. HW does not support TSO
+for packets with more than or equal to 32 segments. HW hangs up if it sees
+more than 32 segments. So the driver implements a HW WAR to fallback to SW
+GSO path in case it sees more than 32 TSO segments.
+
+Debugging
+=========
+
+The PPE hardware counters are available in the debugfs and can be checked by the command
+``cat /sys/kernel/debug/ppe/packet_counters``.
+
+The PPE port MAC statistics can be checked by ethtool command ``ethtool -S ethX``.
+
+The PPE port link state and PHY features can be checked by ethtool command ``ethtool ethX``.
+
+PPE EDMA supported features can be checked by ``ethtool -k ethX``
+
+The SFP module state can be checked by ``cat /sys/kernel/debug/@sfp/state``, where the
+``@sfp`` is the SFP DTS node name.
+
+PPE EDMA Tx ring, Rx ring and miscellaneous statistics can be checked by
+``cat /sys/kernel/debug/edma-dp/stats/tx_ring_stats`` or
+``cat /sys/kernel/debug/edma-dp/stats/rx_ring_stats`` or
+``cat /sys/kernel/debug/edma-dp/stats/misc_stats``
+
+Developer can also enable the CONFIG_DYNAMIC_DEBUG to get further debug
+information.
diff --git a/drivers/net/ethernet/qualcomm/Kconfig b/drivers/net/ethernet/qualcomm/Kconfig
index 364d7e8ea..b636ffa25 100644
--- a/drivers/net/ethernet/qualcomm/Kconfig
+++ b/drivers/net/ethernet/qualcomm/Kconfig
@@ -67,10 +67,10 @@ config QCOM_PPE
 	depends on COMMON_CLK
 	select REGMAP_MMIO
 	select PHYLINK
-	select PCS_QCOM_IPQ_UNIPHY
+	select PCS_QCOM_IPQ9574
 	help
 	  This driver supports the Qualcomm Technologies, Inc. packet
-	  process engine (PPE) available with IPQ SoC. The PPE houses
+	  process engine (PPE) available with IPQ SoC. The PPE includes
 	  the ethernet MACs, Ethernet DMA (EDMA) and switch core that
 	  supports L3 flow offload, L2 switch function, RSS and tunnel
 	  offload.
diff --git a/drivers/net/ethernet/qualcomm/ppe/Makefile b/drivers/net/ethernet/qualcomm/ppe/Makefile
index cb9d30889..03a9f17b1 100644
--- a/drivers/net/ethernet/qualcomm/ppe/Makefile
+++ b/drivers/net/ethernet/qualcomm/ppe/Makefile
@@ -4,7 +4,7 @@
 #
 
 obj-$(CONFIG_QCOM_PPE) += qcom-ppe.o
-qcom-ppe-objs := ppe.o ppe_config.o ppe_api.o ppe_debugfs.o ppe_port.o
+qcom-ppe-objs := ppe.o ppe_config.o ppe_debugfs.o ppe_port.o ppe_api.o
 
 #EDMA
 qcom-ppe-objs += edma.o edma_cfg_rx.o edma_cfg_tx.o edma_debugfs.o edma_port.o edma_rx.o edma_tx.o edma_ethtool.o
diff --git a/drivers/net/ethernet/qualcomm/ppe/edma.c b/drivers/net/ethernet/qualcomm/ppe/edma.c
index 428c7b134..faf6cd817 100644
--- a/drivers/net/ethernet/qualcomm/ppe/edma.c
+++ b/drivers/net/ethernet/qualcomm/ppe/edma.c
@@ -1,5 +1,5 @@
 // SPDX-License-Identifier: GPL-2.0-only
- /* Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+ /* Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
   */
 
  /* Qualcomm Ethernet DMA driver setup, HW configuration, clocks and
@@ -75,14 +75,14 @@ static u8 edma_pri_map[PPE_QUEUE_INTER_PRI_NUM] = {
 	0, 1, 2, 3, 4, 5, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7};
 
 enum edma_clk_id {
-	EDMA_CLK,
-	EDMA_CFG_CLK,
+	EDMA_SYS_CLK,
+	EDMA_APB_CLK,
 	EDMA_CLK_MAX
 };
 
 static const char * const clock_name[EDMA_CLK_MAX] = {
-	[EDMA_CLK] = "edma",
-	[EDMA_CFG_CLK] = "edma-cfg",
+	[EDMA_SYS_CLK] = "sys",
+	[EDMA_APB_CLK] = "apb",
 };
 
 /* Rx Fill ring info for IPQ9574. */
@@ -120,7 +120,7 @@ static struct edma_hw_info ipq9574_hw_info = {
 	.tx = &ipq9574_tx_ring_info,
 	.txcmpl = &ipq9574_txcmpl_ring_info,
 	.max_ports = 6,
-	.napi_budget_rx = 128,
+	.napi_budget_rx = 32,
 	.napi_budget_tx = 512,
 };
 
@@ -155,13 +155,12 @@ static int edma_clock_set_and_enable(struct device *dev,
 	}
 
 	of_node_put(edma_np);
-
 	dev_dbg(dev, "set %lu rate for %s\n", rate, id);
 
 	return 0;
 }
 
-static int edma_clock_init(void)
+static int edma_clock_configure(void)
 {
 	struct ppe_device *ppe_dev = edma_ctx->ppe_dev;
 	struct device *dev = ppe_dev->dev;
@@ -170,16 +169,18 @@ static int edma_clock_init(void)
 
 	ppe_rate = ppe_dev->clk_rate;
 
-	ret = edma_clock_set_and_enable(dev, clock_name[EDMA_CLK],
+	ret = edma_clock_set_and_enable(dev, clock_name[EDMA_SYS_CLK],
 					ppe_rate);
 	if (ret)
 		return ret;
 
-	ret = edma_clock_set_and_enable(dev, clock_name[EDMA_CFG_CLK],
+	ret = edma_clock_set_and_enable(dev, clock_name[EDMA_APB_CLK],
 					ppe_rate);
 	if (ret)
 		return ret;
 
+	dev_dbg(dev, "EDMA clocks are configured\n");
+
 	return 0;
 }
 
@@ -244,7 +245,7 @@ static int edma_configure_ucast_prio_map_tbl(void)
 		}
 
 		ret = ppe_edma_queue_offset_config(edma_ctx->ppe_dev,
-						   PPE_QUEUE_CLASS_PRIORITY, int_pri, pri_class);
+						   PPE_QUEUE_OFFSET_BY_PRIORITY, int_pri, pri_class);
 
 		if (ret) {
 			pr_err("Failed with error: %d to set queue priority class for int_pri: %d for profile_id: %d\n",
@@ -382,7 +383,7 @@ static int edma_irq_register(void)
 			goto txcmpl_ring_irq_name_alloc_fail;
 		}
 
-		snprintf(edma_txcmpl_irq_name[i], EDMA_IRQ_NAME_SIZE, "edma_txcmpl_%d",
+		snprintf(edma_txcmpl_irq_name[i], EDMA_IRQ_NAME_SIZE, "txcmpl_%d",
 			 txcmpl->ring_start + i);
 
 		irq_set_status_flags(edma_ctx->intr_info.intr_txcmpl[i], IRQ_DISABLE_UNLAZY);
@@ -419,7 +420,7 @@ static int edma_irq_register(void)
 			goto rxdesc_ring_irq_name_alloc_fail;
 		}
 
-		snprintf(edma_rxdesc_irq_name[i], 20, "edma_rxdesc_%d",
+		snprintf(edma_rxdesc_irq_name[i], 20, "rxdesc_%d",
 			 rx->ring_start + i);
 
 		irq_set_status_flags(edma_ctx->intr_info.intr_rx[i], IRQ_DISABLE_UNLAZY);
@@ -442,7 +443,7 @@ static int edma_irq_register(void)
 
 	/* Request Misc IRQ */
 	ret = request_irq(edma_ctx->intr_info.intr_misc, edma_misc_handle_irq,
-			  IRQF_SHARED, "edma_misc",
+			  IRQF_SHARED, "misc",
 			  (void *)dev);
 	if (ret) {
 		pr_err("MISC IRQ:%d request failed\n",
@@ -479,7 +480,7 @@ static int edma_irq_register(void)
 	return ret;
 }
 
-static int edma_irq_init(void)
+static int edma_irq_configure(void)
 {
 	struct edma_hw_info *hw_info = edma_ctx->hw_info;
 	struct edma_ring_info *txcmpl = hw_info->txcmpl;
@@ -502,7 +503,7 @@ static int edma_irq_init(void)
 
 	/* Get TXCMPL rings IRQ numbers. */
 	for (i = 0; i < txcmpl->num_rings; i++) {
-		snprintf(edma_irq_name, sizeof(edma_irq_name), "edma_txcmpl_%d",
+		snprintf(edma_irq_name, sizeof(edma_irq_name), "txcmpl_%d",
 			 txcmpl->ring_start + i);
 		edma_ctx->intr_info.intr_txcmpl[i] = of_irq_get_byname(edma_np, edma_irq_name);
 		if (edma_ctx->intr_info.intr_txcmpl[i] < 0) {
@@ -527,7 +528,7 @@ static int edma_irq_init(void)
 
 	/* Get RXDESC rings IRQ numbers. */
 	for (i = 0; i < rx->num_rings; i++) {
-		snprintf(edma_irq_name, sizeof(edma_irq_name), "edma_rxdesc_%d",
+		snprintf(edma_irq_name, sizeof(edma_irq_name), "rxdesc_%d",
 			 rx->ring_start + i);
 		edma_ctx->intr_info.intr_rx[i] = of_irq_get_byname(edma_np, edma_irq_name);
 		if (edma_ctx->intr_info.intr_rx[i] < 0) {
@@ -544,7 +545,7 @@ static int edma_irq_init(void)
 	}
 
 	/* Get misc IRQ number. */
-	edma_ctx->intr_info.intr_misc = of_irq_get_byname(edma_np, "edma_misc");
+	edma_ctx->intr_info.intr_misc = of_irq_get_byname(edma_np, "misc");
 	if (edma_ctx->intr_info.intr_misc < 0) {
 		dev_err(dev, "%s: misc_intr irq get failed\n", edma_np->name);
 		of_node_put(edma_np);
@@ -587,44 +588,23 @@ static int edma_hw_reset(void)
 	struct device *dev = ppe_dev->dev;
 	struct reset_control *edma_hw_rst;
 	struct device_node *edma_np;
-	const char *reset_string;
-	u32 count, i;
-	int ret;
 
-	/* Count and parse reset names from DTSI. */
 	edma_np = of_get_child_by_name(dev->of_node, "edma");
-	count = of_property_count_strings(edma_np, "reset-names");
-	if (count < 0) {
-		dev_err(dev, "EDMA reset entry not found\n");
+	edma_hw_rst = of_reset_control_get_exclusive(edma_np, NULL);
+	if (IS_ERR(edma_hw_rst)) {
 		of_node_put(edma_np);
-		return -EINVAL;
+		return PTR_ERR(edma_hw_rst);
 	}
 
-	for (i = 0; i < count; i++) {
-		ret = of_property_read_string_index(edma_np, "reset-names",
-						    i, &reset_string);
-		if (ret) {
-			dev_err(dev, "Error reading reset-names");
-			of_node_put(edma_np);
-			return -EINVAL;
-		}
-
-		edma_hw_rst = of_reset_control_get_exclusive(edma_np, reset_string);
-		if (IS_ERR(edma_hw_rst)) {
-			of_node_put(edma_np);
-			return PTR_ERR(edma_hw_rst);
-		}
-
-		/* 100ms delay is required by hardware to reset EDMA. */
-		reset_control_assert(edma_hw_rst);
-		fsleep(100);
+	/* 100ms delay is required by hardware to reset EDMA. */
+	reset_control_assert(edma_hw_rst);
+	fsleep(100);
 
-		reset_control_deassert(edma_hw_rst);
-		fsleep(100);
+	reset_control_deassert(edma_hw_rst);
+	fsleep(100);
 
-		reset_control_put(edma_hw_rst);
-		dev_dbg(dev, "EDMA HW reset, i:%d reset_string:%s\n", i, reset_string);
-	}
+	reset_control_put(edma_hw_rst);
+	dev_dbg(dev, "EDMA HW reset\n");
 
 	of_node_put(edma_np);
 
@@ -646,14 +626,12 @@ static int edma_hw_configure(void)
 
 	pr_debug("EDMA ver %d hw init\n", data);
 
-	/* Setup private data structure. */
 	edma_ctx->intr_info.intr_mask_rx = EDMA_RXDESC_INT_MASK_PKT_INT;
 	edma_ctx->intr_info.intr_mask_txcmpl = EDMA_TX_INT_MASK_PKT_INT;
 
-	/* Reset EDMA. */
 	ret = edma_hw_reset();
 	if (ret) {
-		pr_err("Error in resetting the hardware. ret: %d\n", ret);
+		pr_err("Error in resetting the hardware, ret: %d\n", ret);
 		return ret;
 	}
 
@@ -667,7 +645,7 @@ static int edma_hw_configure(void)
 	edma_ctx->dummy_dev = alloc_netdev_dummy(0);
 	if (!edma_ctx->dummy_dev) {
 		ret = -ENOMEM;
-		pr_err("Failed to allocate dummy device. ret: %d\n", ret);
+		pr_err("Failed to allocate dummy device, ret: %d\n", ret);
 		goto dummy_dev_alloc_failed;
 	}
 
@@ -681,7 +659,7 @@ static int edma_hw_configure(void)
 
 	ret = edma_alloc_rings();
 	if (ret) {
-		pr_err("Error in initializaing the rings. ret: %d\n", ret);
+		pr_err("Error in initializing the rings, ret: %d\n", ret);
 		goto edma_alloc_rings_failed;
 	}
 
@@ -701,7 +679,7 @@ static int edma_hw_configure(void)
 
 	ret = edma_cfg_rx_rings();
 	if (ret) {
-		pr_err("Error in configuring Rx rings. ret: %d\n", ret);
+		pr_err("Error in configuring Rx rings, ret: %d\n", ret);
 		goto edma_cfg_rx_rings_failed;
 	}
 
@@ -719,15 +697,13 @@ static int edma_hw_configure(void)
 	if (ret)
 		return ret;
 
-	/* Configure Tx Timeout Threshold. */
 	data = EDMA_TX_TIMEOUT_THRESH_VAL;
-
 	reg = EDMA_BASE_OFFSET + EDMA_REG_TX_TIMEOUT_THRESH_ADDR;
 	ret = regmap_write(regmap, reg, data);
 	if (ret)
 		return ret;
 
-	/* Set Miscellaneous error mask. */
+	/* Set Miscellaneous error interrupt mask. */
 	data = EDMA_MISC_AXI_RD_ERR_MASK |
 		EDMA_MISC_AXI_WR_ERR_MASK |
 		EDMA_MISC_RX_DESC_FIFO_FULL_MASK |
@@ -742,15 +718,15 @@ static int edma_hw_configure(void)
 	edma_cfg_rx_napi_add();
 	edma_cfg_rx_napi_enable();
 
-	/* Global EDMA enable and padding enable. */
+	/* Enable whole edma to work and padding if packet length less than 60
+	 * byte in EDMA port interface control register.
+	 */
 	data = EDMA_PORT_PAD_EN | EDMA_PORT_EDMA_EN;
-
 	reg = EDMA_BASE_OFFSET + EDMA_REG_PORT_CTRL_ADDR;
 	ret = regmap_write(regmap, reg, data);
 	if (ret)
 		return ret;
 
-	/* Initialize unicast priority map table. */
 	ret = (int)edma_configure_ucast_prio_map_tbl();
 	if (ret) {
 		pr_err("Failed to initialize unicast priority map table: %d\n",
@@ -758,7 +734,6 @@ static int edma_hw_configure(void)
 		goto configure_ucast_prio_map_tbl_failed;
 	}
 
-	/* Initialize RPS hash map table. */
 	ret = edma_cfg_rx_rps_hash_map();
 	if (ret) {
 		pr_err("Failed to configure rps hash table: %d\n",
@@ -766,6 +741,8 @@ static int edma_hw_configure(void)
 		goto edma_cfg_rx_rps_hash_map_failed;
 	}
 
+	pr_info("EDMA Hardware Configured\n");
+
 	return 0;
 
 edma_cfg_rx_rps_hash_map_failed:
@@ -802,14 +779,13 @@ void edma_destroy(struct ppe_device *ppe_dev)
 		edma_ctx->rx_rps_ctl_table_hdr = NULL;
 	}
 
-	/* Disable interrupts. */
 	for (i = 1; i <= hw_info->max_ports; i++)
 		edma_cfg_tx_disable_interrupts(i);
 
 	edma_cfg_rx_disable_interrupts();
 	edma_disable_misc_interrupt();
 
-	/* Free IRQ for TXCMPL rings. */
+	/* Free IRQ for Tx cmpl rings. */
 	for (i = 0; i < txcmpl->num_rings; i++) {
 		synchronize_irq(edma_ctx->intr_info.intr_txcmpl[i]);
 
@@ -819,7 +795,7 @@ void edma_destroy(struct ppe_device *ppe_dev)
 	}
 	kfree(edma_txcmpl_irq_name);
 
-	/* Free IRQ for RXDESC rings */
+	/* Free IRQ for Rx DESC rings */
 	for (i = 0; i < rx->num_rings; i++) {
 		synchronize_irq(edma_ctx->intr_info.intr_rx[i]);
 		free_irq(edma_ctx->intr_info.intr_rx[i],
@@ -860,7 +836,7 @@ static struct ctl_table edma_rx_rps_core_table[] = {
  * edma_setup - EDMA Setup.
  * @ppe_dev: PPE device
  *
- * Configure Ethernet global ctx, clocks, hardware and interrupts.
+ * Configure EDMA global context, clocks, hardware and interrupts.
  *
  * Return 0 on success, negative error code on failure.
  */
@@ -888,22 +864,19 @@ int edma_setup(struct ppe_device *ppe_dev)
 		return -EINVAL;
 	}
 
-	/* Configure the EDMA common clocks. */
-	ret = edma_clock_init();
+	ret = edma_clock_configure();
 	if (ret) {
 		dev_err(dev, "Error in configuring the EDMA clocks\n");
 		return ret;
 	}
 
-	dev_dbg(dev, "QCOM EDMA common clocks are configured\n");
-
 	ret = edma_hw_configure();
 	if (ret) {
 		dev_err(dev, "Error in edma configuration\n");
 		return ret;
 	}
 
-	ret = edma_irq_init();
+	ret = edma_irq_configure();
 	if (ret) {
 		dev_err(dev, "Error in irq initialization\n");
 		return ret;
diff --git a/drivers/net/ethernet/qualcomm/ppe/edma.h b/drivers/net/ethernet/qualcomm/ppe/edma.h
index 3f3d25347..9bb0bb1b2 100644
--- a/drivers/net/ethernet/qualcomm/ppe/edma.h
+++ b/drivers/net/ethernet/qualcomm/ppe/edma.h
@@ -1,13 +1,13 @@
 /* SPDX-License-Identifier: GPL-2.0-only
- * Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+ * Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
  */
 
 #ifndef __EDMA_MAIN__
 #define __EDMA_MAIN__
 
-#include "ppe_api.h"
 #include "edma_rx.h"
 #include "edma_tx.h"
+#include "ppe_api.h"
 
 /* One clock cycle = 1/(EDMA clock frequency in Mhz) micro seconds.
  *
@@ -113,7 +113,7 @@ struct edma_intr_info {
 /**
  * struct edma_context - EDMA context.
  * @netdev_arr: Net device for each EDMA port
- * @dummy_dev: Dummy netdevice for RX DMA
+ * @dummy_dev: Dummy netdevice for Rx DMA
  * @ppe_dev: PPE device
  * @hw_info: EDMA Hardware info
  * @intr_info: EDMA Interrupt info
@@ -144,7 +144,7 @@ struct edma_context {
 	bool tx_requeue_stop;
 };
 
-/* Global EDMA context */
+/* Global EDMA context. */
 extern struct edma_context *edma_ctx;
 
 int edma_err_stats_alloc(void);
diff --git a/drivers/net/ethernet/qualcomm/ppe/edma_cfg_rx.c b/drivers/net/ethernet/qualcomm/ppe/edma_cfg_rx.c
index 716b19e96..08f7a80a7 100644
--- a/drivers/net/ethernet/qualcomm/ppe/edma_cfg_rx.c
+++ b/drivers/net/ethernet/qualcomm/ppe/edma_cfg_rx.c
@@ -1,5 +1,5 @@
 // SPDX-License-Identifier: GPL-2.0-only
-/* Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+/* Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
  */
 
 /* Configure rings, Buffers and NAPI for receive path along with
@@ -19,9 +19,6 @@
 #include "ppe.h"
 #include "ppe_regs.h"
 
-/* EDMA Queue ID to Ring ID Table. */
-#define EDMA_QID2RID_TABLE_MEM(q)	(0xb9000 + (0x4 * (q)))
-
 /* Rx ring queue offset. */
 #define EDMA_QUEUE_OFFSET(q_id)	((q_id) / EDMA_MAX_PRI_PER_CORE)
 
@@ -32,6 +29,9 @@
 /* EDMA Queue ID to Ring ID configuration. */
 #define EDMA_QID2RID_NUM_PER_REG	4
 
+/* EDMA Queue ID to Ring ID Table. */
+#define EDMA_QID2RID_TABLE_MEM(q)	(0xb9000 + (0x4 * (q)))
+
 int rx_queues[] = {0, 8, 16, 24};
 
 static u32 edma_rx_ring_queue_map[][EDMA_MAX_CORE] = {{ 0, 8, 16, 24 },
@@ -75,7 +75,7 @@ static int edma_cfg_rx_desc_ring_reset_queue_priority(u32 rxdesc_ring_idx)
 	for (i = 0; i < EDMA_MAX_PRI_PER_CORE; i++) {
 		queue_id = edma_rx_ring_queue_map[i][rxdesc_ring_idx];
 
-		ret = ppe_queue_priority_set(edma_ctx->ppe_dev, queue_id, i);
+		ret = ppe_queue_node_priority_set(edma_ctx->ppe_dev, queue_id, i);
 		if (ret) {
 			pr_err("Error in resetting %u queue's priority\n",
 			       queue_id);
@@ -138,7 +138,7 @@ static int edma_cfg_rx_desc_ring_to_queue_mapping(void)
 			pr_err("Error in configuring Rx ring to PPE queue mapping, ret: %d, id: %d\n",
 			       ret, rxdesc_ring->ring_id);
 			if (!edma_cfg_rx_desc_rings_reset_queue_mapping())
-				pr_err("Error in resetting Rx desc ringbackpressure configurations\n");
+				pr_err("Error in resetting Rx desc ring configurations\n");
 
 			return ret;
 		}
@@ -219,7 +219,9 @@ static void edma_cfg_rx_qid_to_rx_desc_ring_mapping(void)
 
 	desc_index = (rx->ring_start & EDMA_RX_RING_ID_MASK);
 
-	/* Here map all the queues to ring. */
+	/* There are 4 Rx desc rings, one for each core.
+	 * Map the unicast queues to Rx desc rings.
+	 */
 	for (q_id = EDMA_RX_QUEUE_START;
 		q_id <= EDMA_CPU_PORT_QUEUE_MAX(EDMA_RX_QUEUE_START);
 			q_id += EDMA_QID2RID_NUM_PER_REG) {
@@ -271,6 +273,11 @@ static void edma_cfg_rx_rings_to_rx_fill_mapping(void)
 	struct edma_ring_info *rx = hw_info->rx;
 	u32 i, data, reg;
 
+        /* Set RXDESC2FILL_MAP_xx reg.
+         * 3 registers hold the Rxfill mapping for all Rx desc rings.
+         * 3 bits holds the Rx fill ring mapping for each of the
+         * Rx descriptor ring.
+         */
 	regmap_write(regmap, EDMA_BASE_OFFSET + EDMA_REG_RXDESC2FILL_MAP_0_ADDR, 0);
 	regmap_write(regmap, EDMA_BASE_OFFSET + EDMA_REG_RXDESC2FILL_MAP_1_ADDR, 0);
 	regmap_write(regmap, EDMA_BASE_OFFSET + EDMA_REG_RXDESC2FILL_MAP_2_ADDR, 0);
@@ -326,7 +333,6 @@ void edma_cfg_rx_rings_enable(void)
 	struct edma_ring_info *rx = hw_info->rx;
 	u32 i, reg;
 
-	/* Enable Rx rings */
 	for (i = rx->ring_start; i < rx->ring_start + rx->num_rings; i++) {
 		u32 data;
 
@@ -445,7 +451,6 @@ static int edma_cfg_rx_fill_ring_dma_alloc(struct edma_rxfill_ring *rxfill_ring)
 	struct ppe_device *ppe_dev = edma_ctx->ppe_dev;
 	struct device *dev = ppe_dev->dev;
 
-	/* Allocate RxFill ring descriptors */
 	rxfill_ring->desc = dma_alloc_coherent(dev, (sizeof(struct edma_rxfill_desc)
 					       * rxfill_ring->count),
 					       &rxfill_ring->dma,
@@ -633,6 +638,62 @@ static int edma_cfg_rx_rings_setup(void)
 	return -ENOMEM;
 }
 
+static void edma_cfg_rx_fill_ring_configure(struct edma_rxfill_ring *rxfill_ring)
+{
+	struct ppe_device *ppe_dev = edma_ctx->ppe_dev;
+	struct regmap *regmap = ppe_dev->regmap;
+	u32 ring_sz, reg;
+
+	reg = EDMA_BASE_OFFSET + EDMA_REG_RXFILL_BA(rxfill_ring->ring_id);
+	regmap_write(regmap, reg, (u32)(rxfill_ring->dma & EDMA_RING_DMA_MASK));
+
+	ring_sz = rxfill_ring->count & EDMA_RXFILL_RING_SIZE_MASK;
+	reg = EDMA_BASE_OFFSET + EDMA_REG_RXFILL_RING_SIZE(rxfill_ring->ring_id);
+	regmap_write(regmap, reg, ring_sz);
+
+	edma_rx_alloc_buffer(rxfill_ring, rxfill_ring->count - 1);
+}
+
+static void edma_cfg_rx_desc_ring_flow_control(u32 threshold_xoff, u32 threshold_xon)
+{
+	struct edma_hw_info *hw_info = edma_ctx->hw_info;
+	struct ppe_device *ppe_dev = edma_ctx->ppe_dev;
+	struct regmap *regmap = ppe_dev->regmap;
+	struct edma_ring_info *rx = hw_info->rx;
+	u32 data, i, reg;
+
+	data = (threshold_xoff & EDMA_RXDESC_FC_XOFF_THRE_MASK) << EDMA_RXDESC_FC_XOFF_THRE_SHIFT;
+	data |= ((threshold_xon & EDMA_RXDESC_FC_XON_THRE_MASK) << EDMA_RXDESC_FC_XON_THRE_SHIFT);
+
+	for (i = 0; i < rx->num_rings; i++) {
+		struct edma_rxdesc_ring *rxdesc_ring;
+
+		rxdesc_ring = &edma_ctx->rx_rings[i];
+		reg = EDMA_BASE_OFFSET + EDMA_REG_RXDESC_FC_THRE(rxdesc_ring->ring_id);
+		regmap_write(regmap, reg, data);
+	}
+}
+
+static void edma_cfg_rx_fill_ring_flow_control(int threshold_xoff, int threshold_xon)
+{
+	struct edma_hw_info *hw_info = edma_ctx->hw_info;
+	struct edma_ring_info *rxfill = hw_info->rxfill;
+	struct ppe_device *ppe_dev = edma_ctx->ppe_dev;
+	struct regmap *regmap = ppe_dev->regmap;
+	u32 data, i, reg;
+
+	data = (threshold_xoff & EDMA_RXFILL_FC_XOFF_THRE_MASK) << EDMA_RXFILL_FC_XOFF_THRE_SHIFT;
+	data |= ((threshold_xon & EDMA_RXFILL_FC_XON_THRE_MASK) << EDMA_RXFILL_FC_XON_THRE_SHIFT);
+
+	for (i = 0; i < rxfill->num_rings; i++) {
+		struct edma_rxfill_ring *rxfill_ring;
+
+		rxfill_ring = &edma_ctx->rxfill_rings[i];
+		reg = EDMA_BASE_OFFSET + EDMA_REG_RXFILL_FC_THRE(rxfill_ring->ring_id);
+		regmap_write(regmap, reg, data);
+	}
+}
+
 /**
  * edma_cfg_rx_buff_size_setup - Configure EDMA Rx jumbo buffer
  *
@@ -729,62 +790,6 @@ void edma_cfg_rx_rings_cleanup(void)
 	edma_ctx->rx_rings = NULL;
 }
 
-static void edma_cfg_rx_fill_ring_configure(struct edma_rxfill_ring *rxfill_ring)
-{
-	struct ppe_device *ppe_dev = edma_ctx->ppe_dev;
-	struct regmap *regmap = ppe_dev->regmap;
-	u32 ring_sz, reg;
-
-	reg = EDMA_BASE_OFFSET + EDMA_REG_RXFILL_BA(rxfill_ring->ring_id);
-	regmap_write(regmap, reg, (u32)(rxfill_ring->dma & EDMA_RING_DMA_MASK));
-
-	ring_sz = rxfill_ring->count & EDMA_RXFILL_RING_SIZE_MASK;
-	reg = EDMA_BASE_OFFSET + EDMA_REG_RXFILL_RING_SIZE(rxfill_ring->ring_id);
-	regmap_write(regmap, reg, ring_sz);
-
-	edma_rx_alloc_buffer(rxfill_ring, rxfill_ring->count - 1);
-}
-
-static void edma_cfg_rx_desc_ring_flow_control(u32 threshold_xoff, u32 threshold_xon)
-{
-	struct edma_hw_info *hw_info = edma_ctx->hw_info;
-	struct ppe_device *ppe_dev = edma_ctx->ppe_dev;
-	struct regmap *regmap = ppe_dev->regmap;
-	struct edma_ring_info *rx = hw_info->rx;
-	u32 data, i, reg;
-
-	data = (threshold_xoff & EDMA_RXDESC_FC_XOFF_THRE_MASK) << EDMA_RXDESC_FC_XOFF_THRE_SHIFT;
-	data |= ((threshold_xon & EDMA_RXDESC_FC_XON_THRE_MASK) << EDMA_RXDESC_FC_XON_THRE_SHIFT);
-
-	for (i = 0; i < rx->num_rings; i++) {
-		struct edma_rxdesc_ring *rxdesc_ring;
-
-		rxdesc_ring = &edma_ctx->rx_rings[i];
-		reg = EDMA_BASE_OFFSET + EDMA_REG_RXDESC_FC_THRE(rxdesc_ring->ring_id);
-		regmap_write(regmap, reg, data);
-	}
-}
-
-static void edma_cfg_rx_fill_ring_flow_control(int threshold_xoff, int threshold_xon)
-{
-	struct edma_hw_info *hw_info = edma_ctx->hw_info;
-	struct edma_ring_info *rxfill = hw_info->rxfill;
-	struct ppe_device *ppe_dev = edma_ctx->ppe_dev;
-	struct regmap *regmap = ppe_dev->regmap;
-	u32 data, i, reg;
-
-	data = (threshold_xoff & EDMA_RXFILL_FC_XOFF_THRE_MASK) << EDMA_RXFILL_FC_XOFF_THRE_SHIFT;
-	data |= ((threshold_xon & EDMA_RXFILL_FC_XON_THRE_MASK) << EDMA_RXFILL_FC_XON_THRE_SHIFT);
-
-	for (i = 0; i < rxfill->num_rings; i++) {
-		struct edma_rxfill_ring *rxfill_ring;
-
-		rxfill_ring = &edma_ctx->rxfill_rings[i];
-		reg = EDMA_BASE_OFFSET + EDMA_REG_RXFILL_FC_THRE(rxfill_ring->ring_id);
-		regmap_write(regmap, reg, data);
-	}
-}
-
 /**
  * edma_cfg_rx_rings - Configure EDMA Rx rings.
  *
@@ -978,7 +983,7 @@ int edma_cfg_rx_rps_hash_map(void)
 
 	for (hash = 0; hash < PPE_QUEUE_HASH_NUM; hash++) {
 		ret = ppe_edma_queue_offset_config(edma_ctx->ppe_dev,
-						   PPE_QUEUE_CLASS_HASH, hash, q_map[idx]);
+						   PPE_QUEUE_OFFSET_BY_HASH, hash, q_map[idx]);
 		if (ret)
 			return ret;
 
diff --git a/drivers/net/ethernet/qualcomm/ppe/edma_cfg_rx.h b/drivers/net/ethernet/qualcomm/ppe/edma_cfg_rx.h
index 234fe94ed..6c6d8f063 100644
--- a/drivers/net/ethernet/qualcomm/ppe/edma_cfg_rx.h
+++ b/drivers/net/ethernet/qualcomm/ppe/edma_cfg_rx.h
@@ -1,5 +1,5 @@
 /* SPDX-License-Identifier: GPL-2.0-only
- * Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+ * Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
  */
 
 #ifndef __EDMA_CFG_RX__
@@ -28,6 +28,7 @@
 
 /* Rx AC flow control default threshold */
 #define EDMA_RX_AC_FC_THRES_DEF		0x104
+
 /* Rx mitigation timer's default value in microseconds */
 #define EDMA_RX_MITIGATION_TIMER_DEF	25
 
diff --git a/drivers/net/ethernet/qualcomm/ppe/edma_cfg_tx.c b/drivers/net/ethernet/qualcomm/ppe/edma_cfg_tx.c
index 771acebda..5ca0a077e 100644
--- a/drivers/net/ethernet/qualcomm/ppe/edma_cfg_tx.c
+++ b/drivers/net/ethernet/qualcomm/ppe/edma_cfg_tx.c
@@ -1,5 +1,5 @@
 // SPDX-License-Identifier: GPL-2.0-only
-/* Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+/* Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
  */
 
 /* Configure rings, Buffers and NAPI for transmit path along with
@@ -27,7 +27,7 @@ static void edma_cfg_txcmpl_ring_cleanup(struct edma_txcmpl_ring *txcmpl_ring)
 	/* Free any buffers assigned to any descriptors. */
 	edma_tx_complete(EDMA_TX_RING_SIZE - 1, txcmpl_ring);
 
-	/* Free TxCmpl ring descriptors. */
+	/* Free Tx cmpl ring descriptors. */
 	dma_free_coherent(dev, sizeof(struct edma_txcmpl_desc)
 			  * txcmpl_ring->count, txcmpl_ring->desc,
 			  txcmpl_ring->dma);
@@ -101,7 +101,6 @@ static int edma_cfg_tx_desc_ring_setup(struct edma_txdesc_ring *txdesc_ring)
 	struct ppe_device *ppe_dev = edma_ctx->ppe_dev;
 	struct device *dev = ppe_dev->dev;
 
-	/* Allocate RxFill ring descriptors. */
 	txdesc_ring->pdesc = dma_alloc_coherent(dev, sizeof(struct edma_txdesc_pri)
 						* txdesc_ring->count,
 						&txdesc_ring->pdma,
@@ -159,14 +158,14 @@ static void edma_cfg_txcmpl_ring_configure(struct edma_txcmpl_ring *txcmpl_ring)
 	struct regmap *regmap = ppe_dev->regmap;
 	u32 data, reg;
 
-	/* Configure TxCmpl ring base address. */
+	/* Configure Tx cmpl ring base address. */
 	reg = EDMA_BASE_OFFSET + EDMA_REG_TXCMPL_BA(txcmpl_ring->id);
 	regmap_write(regmap, reg, (u32)(txcmpl_ring->dma & EDMA_RING_DMA_MASK));
 
 	reg = EDMA_BASE_OFFSET + EDMA_REG_TXCMPL_RING_SIZE(txcmpl_ring->id);
 	regmap_write(regmap, reg, (u32)(txcmpl_ring->count & EDMA_TXDESC_RING_SIZE_MASK));
 
-	/* Set TxCmpl ret mode to opaque. */
+	/* Set Tx cmpl ret mode to opaque. */
 	reg = EDMA_BASE_OFFSET + EDMA_REG_TXCMPL_CTRL(txcmpl_ring->id);
 	regmap_write(regmap, reg, EDMA_TXCMPL_RETMODE_OPAQUE);
 
@@ -327,10 +326,10 @@ void edma_cfg_tx_ring_mappings(void)
 		else
 			reg = EDMA_BASE_OFFSET + EDMA_REG_TXDESC2CMPL_MAP_5_ADDR;
 
-		pr_debug("Configure Tx desc:%u to use TxCmpl:%u\n", i, desc_index);
+		pr_debug("Configure Tx desc:%u to use Tx cmpl:%u\n", i, desc_index);
 
 		/* Set the Tx complete descriptor ring number in the mapping register.
-		 * E.g. If (txcmpl ring)desc_index = 31, (txdesc ring)i = 28.
+		 * E.g. If (Tx cmpl ring)desc_index = 31, (txdesc ring)i = 28.
 		 *	reg = EDMA_REG_TXDESC2CMPL_MAP_4_ADDR
 		 *	data |= (desc_index & 0x1F) << ((i % 6) * 5);
 		 *	data |= (0x1F << 20); -
@@ -408,7 +407,7 @@ static int edma_cfg_tx_rings_setup(void)
 		}
 	}
 
-	/* Allocate TxCmpl ring descriptors. */
+	/* Allocate Tx cmpl ring descriptors. */
 	for (i = 0; i < txcmpl->num_rings; i++) {
 		struct edma_txcmpl_ring *txcmpl_ring = NULL;
 		int ret;
@@ -419,7 +418,7 @@ static int edma_cfg_tx_rings_setup(void)
 
 		ret = edma_cfg_txcmpl_ring_setup(txcmpl_ring);
 		if (ret != 0) {
-			pr_err("Error in setting up %d TxCmpl ring. ret: %d",
+			pr_err("Error in setting up %d Tx cmpl ring. ret: %d",
 			       txcmpl_ring->id, ret);
 			while (i-- >= 0)
 				edma_cfg_txcmpl_ring_cleanup(&edma_ctx->txcmpl_rings[i]);
@@ -527,7 +526,7 @@ void edma_cfg_tx_rings(void)
 	for (i = 0; i < tx->num_rings; i++)
 		edma_cfg_tx_desc_ring_configure(&edma_ctx->tx_rings[i]);
 
-	/* Configure TxCmpl ring. */
+	/* Configure Tx cmpl ring. */
 	for (i = 0; i < txcmpl->num_rings; i++)
 		edma_cfg_txcmpl_ring_configure(&edma_ctx->txcmpl_rings[i]);
 }
@@ -666,7 +665,7 @@ void edma_cfg_tx_napi_add(struct net_device *netdev, u32 port_id)
 		netif_napi_add_weight(netdev, &txcmpl_ring->napi,
 				      edma_tx_napi_poll, hw_info->napi_budget_tx);
 		txcmpl_ring->napi_added = true;
-		netdev_dbg(netdev, "Napi added for txcmpl ring: %u\n", txcmpl_ring->id);
+		netdev_dbg(netdev, "Napi added for Tx cmpl ring: %u\n", txcmpl_ring->id);
 	}
 
 	netdev_dbg(netdev, "Tx NAPI budget: %d\n", edma_tx_napi_budget);
diff --git a/drivers/net/ethernet/qualcomm/ppe/edma_cfg_tx.h b/drivers/net/ethernet/qualcomm/ppe/edma_cfg_tx.h
index 608bbc5f9..5702518a1 100644
--- a/drivers/net/ethernet/qualcomm/ppe/edma_cfg_tx.h
+++ b/drivers/net/ethernet/qualcomm/ppe/edma_cfg_tx.h
@@ -1,5 +1,5 @@
 /* SPDX-License-Identifier: GPL-2.0-only
- * Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+ * Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
  */
 
 #ifndef __EDMA_CFG_TX__
diff --git a/drivers/net/ethernet/qualcomm/ppe/edma_debugfs.c b/drivers/net/ethernet/qualcomm/ppe/edma_debugfs.c
index 671062d4e..baaa32816 100644
--- a/drivers/net/ethernet/qualcomm/ppe/edma_debugfs.c
+++ b/drivers/net/ethernet/qualcomm/ppe/edma_debugfs.c
@@ -1,5 +1,5 @@
 // SPDX-License-Identifier: GPL-2.0-only
-/* Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+/* Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
  */
 
 /* EDMA debugfs routines for display of Tx/Rx counters. */
diff --git a/drivers/net/ethernet/qualcomm/ppe/edma_ethtool.c b/drivers/net/ethernet/qualcomm/ppe/edma_ethtool.c
index 119d6ba1f..96438da02 100644
--- a/drivers/net/ethernet/qualcomm/ppe/edma_ethtool.c
+++ b/drivers/net/ethernet/qualcomm/ppe/edma_ethtool.c
@@ -1,5 +1,5 @@
 // SPDX-License-Identifier: GPL-2.0-only
-/* Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+/* Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
  */
 
 /* ethtool support for EDMA */
@@ -242,33 +242,6 @@ static int edma_set_pauseparam(struct net_device *netdev,
 	return phylink_ethtool_set_pauseparam(port->phylink, pause);
 }
 
-static int edma_get_eee(struct net_device *netdev, struct ethtool_eee *eee)
-{
-	struct edma_port_priv *port_priv = (struct edma_port_priv *)netdev_priv(netdev);
-	struct ppe_port *port =  port_priv->ppe_port;
-
-	if (!port_priv)
-		return -EINVAL;
-
-	return phylink_ethtool_get_eee(port->phylink, eee);
-}
-
-static int edma_set_eee(struct net_device *netdev, struct ethtool_eee *eee)
-{
-	struct edma_port_priv *port_priv = (struct edma_port_priv *)netdev_priv(netdev);
-	struct ppe_port *port =  port_priv->ppe_port;
-	int ret;
-
-	if (!port_priv)
-		return -EINVAL;
-
-	ret = ppe_port_set_mac_eee(port_priv->ppe_port, eee);
-	if (ret)
-		return ret;
-
-	return phylink_ethtool_set_eee(port->phylink, eee);
-}
-
 static const struct ethtool_ops edma_ethtool_ops = {
 	.get_strings = &edma_get_strings,
 	.get_sset_count = &edma_get_strset_count,
@@ -278,8 +251,6 @@ static const struct ethtool_ops edma_ethtool_ops = {
 	.set_link_ksettings = edma_set_link_ksettings,
 	.get_pauseparam = &edma_get_pauseparam,
 	.set_pauseparam = &edma_set_pauseparam,
-	.get_eee = &edma_get_eee,
-	.set_eee = &edma_set_eee,
 };
 
 /**
diff --git a/drivers/net/ethernet/qualcomm/ppe/edma_port.c b/drivers/net/ethernet/qualcomm/ppe/edma_port.c
index 6730cee5d..0de540aa2 100644
--- a/drivers/net/ethernet/qualcomm/ppe/edma_port.c
+++ b/drivers/net/ethernet/qualcomm/ppe/edma_port.c
@@ -1,5 +1,5 @@
 // SPDX-License-Identifier: GPL-2.0-only
- /* Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+ /* Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
   */
 
 /* EDMA port initialization, configuration and netdevice ops handling */
@@ -59,9 +59,12 @@ static void edma_port_stats_free(struct net_device *netdev)
 static void edma_port_configure(struct net_device *netdev)
 {
 	struct edma_port_priv *port_priv = (struct edma_port_priv *)netdev_priv(netdev);
-	struct ppe_port *port =  port_priv->ppe_port;
+	struct ppe_port *port = port_priv->ppe_port;
 	int port_id = port->port_id;
 
+	netdev_dbg(netdev, "Configuring the port %s(qcom-id:%d)\n",
+		   netdev->name, port_id);
+
 	edma_cfg_tx_fill_per_port_tx_map(netdev, port_id);
 	edma_cfg_tx_rings_enable(port_id);
 	edma_cfg_tx_napi_add(netdev, port_id);
@@ -70,7 +73,7 @@ static void edma_port_configure(struct net_device *netdev)
 static void edma_port_deconfigure(struct net_device *netdev)
 {
 	struct edma_port_priv *port_priv = (struct edma_port_priv *)netdev_priv(netdev);
-	struct ppe_port *port =  port_priv->ppe_port;
+	struct ppe_port *port = port_priv->ppe_port;
 	int port_id = port->port_id;
 
 	edma_cfg_tx_napi_delete(port_id);
@@ -140,7 +143,6 @@ static int edma_port_close(struct net_device *netdev)
 	edma_cfg_tx_disable_interrupts(port_id);
 	edma_cfg_tx_napi_disable(port_id);
 
-	/* Phylink close. */
 	if (ppe_port->phylink)
 		phylink_stop(ppe_port->phylink);
 
@@ -219,7 +221,7 @@ static netdev_tx_t edma_port_xmit(struct sk_buff *skb,
 		if (unlikely(ret == EDMA_TX_FAIL_NO_DESC)) {
 			if (likely(!edma_ctx->tx_requeue_stop)) {
 				cpu_id = smp_processor_id();
-				netdev_dbg(dev, "Stopping tx queue due to lack oftx descriptors\n");
+				netdev_dbg(dev, "Stopping tx queue due to lack of tx descriptors\n");
 				u64_stats_update_begin(&stats->syncp);
 				++stats->tx_queue_stopped[cpu_id];
 				u64_stats_update_end(&stats->syncp);
@@ -406,16 +408,12 @@ int edma_port_setup(struct ppe_port *port)
 			    port_id, netdev->dev_addr);
 	}
 
-	/* Allocate memory for EDMA port statistics. */
 	ret = edma_port_stats_alloc(netdev);
 	if (ret) {
 		netdev_dbg(netdev, "EDMA port stats alloc failed\n");
 		goto stats_alloc_fail;
 	}
 
-	netdev_dbg(netdev, "Configuring the port %s(qcom-id:%d)\n",
-		   netdev->name, port_id);
-
 	/* We expect 'port_id' to correspond to ports numbers on SoC.
 	 * These begin from '1' and hence we subtract
 	 * one when using it as an array index.
@@ -424,7 +422,6 @@ int edma_port_setup(struct ppe_port *port)
 
 	edma_port_configure(netdev);
 
-	/* Setup phylink. */
 	ret = ppe_port_phylink_setup(port, netdev);
 	if (ret) {
 		netdev_dbg(netdev, "EDMA port phylink setup for netdevice %s\n",
@@ -432,7 +429,6 @@ int edma_port_setup(struct ppe_port *port)
 		goto port_phylink_setup_fail;
 	}
 
-	/* Register the network interface. */
 	ret = register_netdev(netdev);
 	if (ret) {
 		netdev_dbg(netdev, "Error registering netdevice %s\n",
@@ -440,7 +436,7 @@ int edma_port_setup(struct ppe_port *port)
 		goto register_netdev_fail;
 	}
 
-	netdev_dbg(netdev, "Setup EDMA port GMAC%d done\n", port_id);
+	netdev_dbg(netdev, "Setup EDMA port%d done\n", port_id);
 	return ret;
 
 register_netdev_fail:
diff --git a/drivers/net/ethernet/qualcomm/ppe/edma_port.h b/drivers/net/ethernet/qualcomm/ppe/edma_port.h
index b67eddabd..911d2d138 100644
--- a/drivers/net/ethernet/qualcomm/ppe/edma_port.h
+++ b/drivers/net/ethernet/qualcomm/ppe/edma_port.h
@@ -1,5 +1,5 @@
 /* SPDX-License-Identifier: GPL-2.0-only
- * Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+ * Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
  */
 
 #ifndef __EDMA_PORTS__
@@ -17,7 +17,7 @@
 					| NETIF_F_TSO6)
 
 /**
- * struct edma_port_rx_stats - EDMA RX per CPU stats for the port.
+ * struct edma_port_rx_stats - EDMA Rx per CPU stats for the port.
  * @rx_pkts: Number of Rx packets
  * @rx_bytes: Number of Rx bytes
  * @rx_drops: Number of Rx drops
diff --git a/drivers/net/ethernet/qualcomm/ppe/edma_rx.c b/drivers/net/ethernet/qualcomm/ppe/edma_rx.c
index a1eb53341..df5a5497d 100644
--- a/drivers/net/ethernet/qualcomm/ppe/edma_rx.c
+++ b/drivers/net/ethernet/qualcomm/ppe/edma_rx.c
@@ -1,5 +1,5 @@
 // SPDX-License-Identifier: GPL-2.0-only
-/* Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+/* Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
  */
 
 /* Provides APIs to alloc Rx Buffers, reap the buffers, receive and
@@ -33,10 +33,12 @@ static int edma_rx_alloc_buffer_list(struct edma_rxfill_ring *rxfill_ring, int a
 	struct device *dev = ppe_dev->dev;
 	u16 prod_idx, start_idx;
 	u16 num_alloc = 0;
+	u32 dma_map_size;
 	u32 reg;
 
 	prod_idx = rxfill_ring->prod_idx;
 	start_idx = prod_idx;
+	dma_map_size = rx_alloc_size - EDMA_RX_SKB_HEADROOM - NET_IP_ALIGN;
 
 	while (likely(alloc_count--)) {
 		dma_addr_t buff_addr;
@@ -56,9 +58,9 @@ static int edma_rx_alloc_buffer_list(struct edma_rxfill_ring *rxfill_ring, int a
 		skb_reserve(skb, EDMA_RX_SKB_HEADROOM + NET_IP_ALIGN);
 
 		if (likely(!page_mode)) {
-			buff_addr = dma_map_single(dev, skb->data, rx_alloc_size, DMA_FROM_DEVICE);
+			buff_addr = dma_map_single(dev, skb->data, dma_map_size, DMA_TO_DEVICE);
 			if (dma_mapping_error(dev, buff_addr)) {
-				dev_dbg(dev, "edma_context:%p Unable to dma for non page mode",
+				dev_dbg(dev, "edma_context:%pK Unable to dma for non page mode",
 					edma_ctx);
 				dev_kfree_skb_any(skb);
 				break;
@@ -70,14 +72,14 @@ static int edma_rx_alloc_buffer_list(struct edma_rxfill_ring *rxfill_ring, int a
 				++rxfill_stats->page_alloc_failed;
 				u64_stats_update_end(&rxfill_stats->syncp);
 				dev_kfree_skb_any(skb);
-				dev_dbg(dev, "edma_context:%p Unable to allocate page",
+				dev_dbg(dev, "edma_context:%pK Unable to allocate page",
 					edma_ctx);
 				break;
 			}
 
-			buff_addr = dma_map_page(dev, pg, 0, PAGE_SIZE, DMA_FROM_DEVICE);
+			buff_addr = dma_map_page(dev, pg, 0, PAGE_SIZE, DMA_TO_DEVICE);
 			if (dma_mapping_error(dev, buff_addr)) {
-				dev_dbg(dev, "edma_context:%p Mapping error for page mode",
+				dev_dbg(dev, "edma_context:%pK Mapping error for page mode",
 					edma_ctx);
 				__free_page(pg);
 				dev_kfree_skb_any(skb);
@@ -97,9 +99,12 @@ static int edma_rx_alloc_buffer_list(struct edma_rxfill_ring *rxfill_ring, int a
 					   (u32)(buf_len) & EDMA_RXFILL_BUF_SIZE_MASK);
 		prod_idx = (prod_idx + 1) & EDMA_RX_RING_SIZE_MASK;
 		num_alloc++;
+
+		EDMA_RXFILL_ENDIAN_SET(rxfill_desc);
 	}
 
 	if (likely(num_alloc)) {
+		dsb(st);
 		reg = EDMA_BASE_OFFSET + EDMA_REG_RXFILL_PROD_IDX(rxfill_ring->ring_id);
 		regmap_write(regmap, reg, prod_idx);
 		rxfill_ring->prod_idx = prod_idx;
@@ -122,24 +127,26 @@ int edma_rx_alloc_buffer(struct edma_rxfill_ring *rxfill_ring, int alloc_count)
 	return edma_rx_alloc_buffer_list(rxfill_ring, alloc_count);
 }
 
-/* Mark ip_summed appropriately in the skb as per the L3/L4 checksum
- * status in descriptor.
- */
-static void edma_rx_checksum_verify(struct edma_rxdesc_pri *rxdesc_pri,
+static inline uint8_t edma_rx_checksum_verify(struct edma_rxdesc_pri *rxdesc_pri,
 				    struct sk_buff *skb)
 {
 	u8 pid = EDMA_RXDESC_PID_GET(rxdesc_pri);
 
 	skb_checksum_none_assert(skb);
 
+	/* Mark ip_summed appropriately in the skb as per the L3/L4 checksum
+	 * status in descriptor.
+	 */
 	if (likely(EDMA_RX_PID_IS_IPV4(pid))) {
 		if (likely(EDMA_RXDESC_L3CSUM_STATUS_GET(rxdesc_pri)) &&
 		    likely(EDMA_RXDESC_L4CSUM_STATUS_GET(rxdesc_pri)))
-			skb->ip_summed = CHECKSUM_UNNECESSARY;
+			return CHECKSUM_UNNECESSARY;
 	} else if (likely(EDMA_RX_PID_IS_IPV6(pid))) {
 		if (likely(EDMA_RXDESC_L4CSUM_STATUS_GET(rxdesc_pri)))
-			skb->ip_summed = CHECKSUM_UNNECESSARY;
+			return CHECKSUM_UNNECESSARY;
 	}
+
+	return skb->ip_summed;
 }
 
 static void edma_rx_process_last_segment(struct edma_rxdesc_ring *rxdesc_ring,
@@ -154,7 +161,6 @@ static void edma_rx_process_last_segment(struct edma_rxdesc_ring *rxdesc_ring,
 	struct net_device *dev;
 	u32 pkt_length;
 
-	/* Get packet length. */
 	pkt_length = EDMA_RXDESC_PACKET_LEN_GET(rxdesc_pri);
 
 	skb_head = rxdesc_ring->head;
@@ -162,7 +168,7 @@ static void edma_rx_process_last_segment(struct edma_rxdesc_ring *rxdesc_ring,
 
 	/* Check Rx checksum offload status. */
 	if (likely(dev->features & NETIF_F_RXCSUM))
-		edma_rx_checksum_verify(rxdesc_pri, skb_head);
+		skb->ip_summed = edma_rx_checksum_verify(rxdesc_pri, skb_head);
 
 	/* Get stats for the netdevice. */
 	port_dev = netdev_priv(dev);
@@ -207,7 +213,7 @@ static void edma_rx_process_last_segment(struct edma_rxdesc_ring *rxdesc_ring,
 	rx_stats->rx_fraglist_pkts += (u64)(!page_mode);
 	u64_stats_update_end(&rx_stats->syncp);
 
-	pr_debug("edma_context:%p skb:%p Jumbo pkt_length:%u\n",
+	pr_debug("edma_context:%pK skb:%pK Jumbo pkt_length:%u\n",
 		 edma_ctx, skb_head, skb_head->len);
 
 	skb_head->protocol = eth_type_trans(skb_head, dev);
@@ -229,9 +235,8 @@ static void edma_rx_handle_frag_list(struct edma_rxdesc_ring *rxdesc_ring,
 {
 	u32 pkt_length;
 
-	/* Get packet length. */
 	pkt_length = EDMA_RXDESC_PACKET_LEN_GET(rxdesc_pri);
-	pr_debug("edma_context:%p skb:%p fragment pkt_length:%u\n",
+	pr_debug("edma_context:%pK skb:%pK fragment pkt_length:%u\n",
 		 edma_ctx, skb, pkt_length);
 
 	if (!(rxdesc_ring->head)) {
@@ -275,9 +280,8 @@ static void edma_rx_handle_nr_frags(struct edma_rxdesc_ring *rxdesc_ring,
 	skb_frag_t *frag = NULL;
 	u32 pkt_length;
 
-	/* Get packet length. */
 	pkt_length = EDMA_RXDESC_PACKET_LEN_GET(rxdesc_pri);
-	pr_debug("edma_context:%p skb:%p fragment pkt_length:%u\n",
+	pr_debug("edma_context:%pK skb:%pK fragment pkt_length:%u\n",
 		 edma_ctx, skb, pkt_length);
 
 	if (!(rxdesc_ring->head)) {
@@ -327,7 +331,6 @@ static bool edma_rx_handle_linear_packets(struct edma_rxdesc_ring *rxdesc_ring,
 	pcpu_stats = &port_dev->pcpu_stats;
 	rx_stats = this_cpu_ptr(pcpu_stats->rx_stats);
 
-	/* Get packet length. */
 	pkt_length = EDMA_RXDESC_PACKET_LEN_GET(rxdesc_pri);
 
 	if (likely(!page_mode)) {
@@ -355,7 +358,7 @@ static bool edma_rx_handle_linear_packets(struct edma_rxdesc_ring *rxdesc_ring,
 
 	/* Check Rx checksum offload status. */
 	if (likely(skb->dev->features & NETIF_F_RXCSUM))
-		edma_rx_checksum_verify(rxdesc_pri, skb);
+		skb->ip_summed = edma_rx_checksum_verify(rxdesc_pri, skb);
 
 	u64_stats_update_begin(&rx_stats->syncp);
 	rx_stats->rx_pkts++;
@@ -363,15 +366,15 @@ static bool edma_rx_handle_linear_packets(struct edma_rxdesc_ring *rxdesc_ring,
 	rx_stats->rx_nr_frag_pkts += (u64)page_mode;
 	u64_stats_update_end(&rx_stats->syncp);
 
+	netdev_dbg(skb->dev, "edma_context:%pK, skb:%pK pkt_length:%u\n",
+		   edma_ctx, skb, skb->len);
+
 	skb->protocol = eth_type_trans(skb, skb->dev);
 	if (skb->dev->features & NETIF_F_GRO)
 		napi_gro_receive(&rxdesc_ring->napi, skb);
 	else
 		netif_receive_skb(skb);
 
-	netdev_dbg(skb->dev, "edma_context:%p, skb:%p pkt_length:%u\n",
-		   edma_ctx, skb, skb->len);
-
 	return true;
 }
 
@@ -390,7 +393,7 @@ static struct net_device *edma_rx_get_src_dev(struct edma_rxdesc_stats *rxdesc_s
 		src_port_num = src_info & EDMA_RXDESC_PORTNUM_BITS;
 	} else {
 		if (net_ratelimit()) {
-			pr_warn("Invalid src info_type:0x%x. Drop skb:%p\n",
+			pr_warn("Invalid src info_type:0x%x. Drop skb:%pK\n",
 				(src_info & EDMA_RXDESC_SRCINFO_TYPE_MASK), skb);
 		}
 
@@ -401,11 +404,10 @@ static struct net_device *edma_rx_get_src_dev(struct edma_rxdesc_stats *rxdesc_s
 		return NULL;
 	}
 
-	/* Packet with PP source. */
 	if (likely(src_port_num <= hw_info->max_ports)) {
 		if (unlikely(src_port_num < EDMA_START_IFNUM)) {
 			if (net_ratelimit())
-				pr_warn("Port number error :%d. Drop skb:%p\n",
+				pr_warn("Port number error :%d. Drop skb:%pK\n",
 					src_port_num, skb);
 
 			u64_stats_update_begin(&rxdesc_stats->syncp);
@@ -427,7 +429,7 @@ static struct net_device *edma_rx_get_src_dev(struct edma_rxdesc_stats *rxdesc_s
 		return ndev;
 
 	if (net_ratelimit())
-		pr_warn("Netdev Null src_info_type:0x%x src port num:%d Drop skb:%p\n",
+		pr_warn("Netdev Null src_info_type:0x%x src port num:%d Drop skb:%pK\n",
 			(src_info & EDMA_RXDESC_SRCINFO_TYPE_MASK),
 			src_port_num, skb);
 
@@ -482,16 +484,17 @@ static int edma_rx_reap(struct edma_rxdesc_ring *rxdesc_ring, int budget)
 		struct net_device *ndev;
 		struct sk_buff *skb;
 		dma_addr_t dma_addr;
+		u32 dma_map_size;
 
 		skb = next_skb;
 		rxdesc_pri = next_rxdesc_pri;
 		dma_addr = EDMA_RXDESC_BUFFER_ADDR_GET(rxdesc_pri);
+		dma_map_size = alloc_size - EDMA_RX_SKB_HEADROOM - NET_IP_ALIGN;
 
 		if (!page_mode)
-			dma_unmap_single(dev, dma_addr, alloc_size,
-					 DMA_TO_DEVICE);
+			dma_unmap_single(dev, dma_addr, dma_map_size, DMA_FROM_DEVICE);
 		else
-			dma_unmap_page(dev, dma_addr, PAGE_SIZE, DMA_TO_DEVICE);
+			dma_unmap_page(dev, dma_addr, PAGE_SIZE, DMA_FROM_DEVICE);
 
 		/* Update consumer index. */
 		cons_idx = (cons_idx + 1) & EDMA_RX_RING_SIZE_MASK;
diff --git a/drivers/net/ethernet/qualcomm/ppe/edma_rx.h b/drivers/net/ethernet/qualcomm/ppe/edma_rx.h
index 0ef8138b4..edead8085 100644
--- a/drivers/net/ethernet/qualcomm/ppe/edma_rx.h
+++ b/drivers/net/ethernet/qualcomm/ppe/edma_rx.h
@@ -1,5 +1,5 @@
 /* SPDX-License-Identifier: GPL-2.0-only
- * Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+ * Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
  */
 
 #ifndef __EDMA_RX__
diff --git a/drivers/net/ethernet/qualcomm/ppe/edma_tx.c b/drivers/net/ethernet/qualcomm/ppe/edma_tx.c
index 47876c142..f98bcbba7 100644
--- a/drivers/net/ethernet/qualcomm/ppe/edma_tx.c
+++ b/drivers/net/ethernet/qualcomm/ppe/edma_tx.c
@@ -1,5 +1,5 @@
 // SPDX-License-Identifier: GPL-2.0-only
-/* Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+/* Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
  */
 
 /* Provide APIs to alloc Tx Buffers, fill the Tx descriptors and transmit
@@ -68,14 +68,9 @@ enum edma_tx_gso_status edma_tx_gso_segment(struct sk_buff *skb,
 	if (likely(!skb_is_nonlinear(skb)))
 		return EDMA_TX_GSO_NOT_NEEDED;
 
-	/* Check if TSO is enabled. If so, return as skb doesn't
-	 * need to be segmented by linux.
-	 */
-	if (netdev->features & (NETIF_F_TSO | NETIF_F_TSO6)) {
-		num_tx_desc_needed = edma_tx_num_descs_for_sg(skb);
-		if (likely(num_tx_desc_needed <= EDMA_TX_TSO_SEG_MAX))
-			return EDMA_TX_GSO_NOT_NEEDED;
-	}
+	num_tx_desc_needed = edma_tx_num_descs_for_sg(skb);
+	if (likely(num_tx_desc_needed <= EDMA_TX_TSO_SEG_MAX))
+		return EDMA_TX_GSO_NOT_NEEDED;
 
 	/* GSO segmentation of the skb into multiple segments. */
 	*segs = skb_gso_segment(skb, netdev->features
@@ -119,7 +114,7 @@ u32 edma_tx_complete(u32 work_to_do, struct edma_txcmpl_ring *txcmpl_ring)
 	if (likely(txcmpl_ring->avail_pkt >= work_to_do)) {
 		avail = work_to_do;
 	} else {
-		/* Get TXCMPL ring producer index. */
+		/* Get Tx cmpl ring producer index. */
 		reg = EDMA_BASE_OFFSET + EDMA_REG_TXCMPL_PROD_IDX(txcmpl_ring->id);
 		regmap_read(regmap, reg, &data);
 		prod_idx = data & EDMA_TXCMPL_PROD_IDX_MASK;
@@ -128,7 +123,7 @@ u32 edma_tx_complete(u32 work_to_do, struct edma_txcmpl_ring *txcmpl_ring)
 		txcmpl_ring->avail_pkt = avail;
 
 		if (unlikely(!avail)) {
-			dev_dbg(dev, "No available descriptors are pending for %d txcmpl ring\n",
+			dev_dbg(dev, "No available descriptors are pending for %d Tx cmpl ring\n",
 				txcmpl_ring->id);
 			u64_stats_update_begin(&txcmpl_stats->syncp);
 			++txcmpl_stats->no_pending_desc;
@@ -144,9 +139,6 @@ u32 edma_tx_complete(u32 work_to_do, struct edma_txcmpl_ring *txcmpl_ring)
 	end_idx = (cons_idx + avail) & EDMA_TX_RING_SIZE_MASK;
 	txcmpl = EDMA_TXCMPL_DESC(txcmpl_ring, cons_idx);
 
-	/* Instead of freeing the skb, it might be better to save and use
-	 * for Rxfill.
-	 */
 	while (likely(avail--)) {
 		/* The last descriptor holds the SKB pointer for scattered frames.
 		 * So skip the descriptors with more bit set.
@@ -172,27 +164,13 @@ u32 edma_tx_complete(u32 work_to_do, struct edma_txcmpl_ring *txcmpl_ring)
 			++txcmpl_stats->invalid_buffer;
 			u64_stats_update_end(&txcmpl_stats->syncp);
 		} else {
-			dev_dbg(dev, "TXCMPL: skb:%p, skb->len %d, skb->data_len %d, cons_idx:%d prod_idx:%d word2:0x%x word3:0x%x\n",
-				skb, skb->len, skb->data_len, cons_idx, prod_idx,
-				txcmpl->word2, txcmpl->word3);
-
-			txcmpl_errors = EDMA_TXCOMP_RING_ERROR_GET(txcmpl->word3);
-			if (unlikely(txcmpl_errors)) {
-				if (net_ratelimit())
-					dev_err(dev, "Error 0x%0x observed in tx complete %d ring\n",
-						txcmpl_errors, txcmpl_ring->id);
-
-				u64_stats_update_begin(&txcmpl_stats->syncp);
-				++txcmpl_stats->errors;
-				u64_stats_update_end(&txcmpl_stats->syncp);
-			}
 
 			/* Retrieve pool id for unmapping.
 			 * 0 for linear skb and (pool id - 1) represents nr_frag index.
 			 */
 			if (!EDMA_TXCOMP_POOL_ID_GET(txcmpl)) {
 				dma_unmap_single(dev, virt_to_phys(skb->data),
-						 skb->len, DMA_TO_DEVICE);
+						 skb_headlen(skb), DMA_TO_DEVICE);
 			} else {
 				u8 frag_index = (EDMA_TXCOMP_POOL_ID_GET(txcmpl) - 1);
 				skb_frag_t *frag = &skb_shinfo(skb)->frags[frag_index];
@@ -201,6 +179,21 @@ u32 edma_tx_complete(u32 work_to_do, struct edma_txcmpl_ring *txcmpl_ring)
 					       PAGE_SIZE, DMA_TO_DEVICE);
 			}
 
+			dev_dbg(dev, "TXCMPL: skb:%pK, skb->len %d, skb->data_len %d, cons_idx:%d prod_idx:%d word2:0x%x word3:0x%x\n",
+				skb, skb->len, skb->data_len, cons_idx, prod_idx,
+				txcmpl->word2, txcmpl->word3);
+
+			txcmpl_errors = EDMA_TXCOMP_RING_ERROR_GET(txcmpl->word3);
+			if (unlikely(txcmpl_errors)) {
+				if (net_ratelimit())
+					dev_err(dev, "Error 0x%0x observed in tx complete %d ring\n",
+						txcmpl_errors, txcmpl_ring->id);
+
+				u64_stats_update_begin(&txcmpl_stats->syncp);
+				++txcmpl_stats->errors;
+				u64_stats_update_end(&txcmpl_stats->syncp);
+			}
+
 			dev_kfree_skb(skb);
 		}
 
@@ -211,7 +204,7 @@ u32 edma_tx_complete(u32 work_to_do, struct edma_txcmpl_ring *txcmpl_ring)
 	txcmpl_ring->cons_idx = cons_idx;
 	txcmpl_ring->avail_pkt -= count;
 
-	dev_dbg(dev, "TXCMPL:%u count:%u prod_idx:%u cons_idx:%u\n",
+	dev_dbg(dev, "Tx cmpl:%u count:%u prod_idx:%u cons_idx:%u\n",
 		txcmpl_ring->id, count, prod_idx, cons_idx);
 	reg = EDMA_BASE_OFFSET + EDMA_REG_TXCMPL_CONS_IDX(txcmpl_ring->id);
 	regmap_write(regmap, reg, cons_idx);
@@ -265,7 +258,7 @@ int edma_tx_napi_poll(struct napi_struct *napi, int budget)
 	/* No more packets to process. Finish NAPI processing. */
 	napi_complete(napi);
 
-	/* Set TXCMPL ring interrupt mask. */
+	/* Set Tx cmpl ring interrupt mask. */
 	reg = EDMA_BASE_OFFSET + EDMA_REG_TX_INT_MASK(txcmpl_ring->id);
 	regmap_write(regmap, reg, edma_ctx->intr_info.intr_mask_txcmpl);
 
@@ -392,7 +385,7 @@ static u32 edma_tx_skb_nr_frags(struct edma_txdesc_ring *txdesc_ring,
 	return num_descs;
 }
 
-static void edma_tx_fill_pp_desc(struct edma_port_priv *port_priv,
+static void edma_tx_fill_desc(struct edma_port_priv *port_priv,
 				 struct edma_txdesc_pri *txd, struct sk_buff *skb,
 	struct edma_port_tx_stats *stats)
 {
@@ -463,7 +456,7 @@ static struct edma_txdesc_pri *edma_tx_skb_first_desc(struct edma_port_priv *por
 
 	EDMA_TXDESC_BUFFER_ADDR_SET(txd, buff_addr);
 	EDMA_TXDESC_POOL_ID_SET(txd, 0);
-	edma_tx_fill_pp_desc(port_priv, txd, skb, stats);
+	edma_tx_fill_desc(port_priv, txd, skb, stats);
 
 	/* Set packet length in the descriptor. */
 	EDMA_TXDESC_DATA_LEN_SET(txd, buf_len);
@@ -528,6 +521,23 @@ static u32 edma_tx_skb_sg_fill_desc(struct edma_txdesc_ring *txdesc_ring,
 	/* Head skb processed already. */
 	num_descs++;
 
+	/* Process skb with nr_frags. */
+	if (unlikely(skb_shinfo(skb)->nr_frags)) {
+		num_descs += edma_tx_skb_nr_frags(txdesc_ring, &txd, skb,
+						  hw_next_to_use, &invalid_frag);
+		if (unlikely(!num_descs)) {
+			dev_dbg(dev, "No descriptor available for ring %d\n", txdesc_ring->id);
+			edma_tx_dma_unmap_frags(skb, invalid_frag);
+			*txdesc = NULL;
+			return num_descs;
+		}
+
+		u64_stats_update_begin(&stats->syncp);
+		stats->tx_nr_frag_pkts++;
+		u64_stats_update_end(&stats->syncp);
+	}
+
+	/* Process skb if it has frag_list */
 	if (unlikely(skb_has_frag_list(skb))) {
 		struct edma_txdesc_pri *start_desc = NULL;
 		u32 start_idx = 0, end_idx = 0;
@@ -608,23 +618,9 @@ static u32 edma_tx_skb_sg_fill_desc(struct edma_txdesc_ring *txdesc_ring,
 		u64_stats_update_begin(&stats->syncp);
 		stats->tx_fraglist_pkts++;
 		u64_stats_update_end(&stats->syncp);
-	} else {
-		/* Process skb with nr_frags. */
-		num_descs += edma_tx_skb_nr_frags(txdesc_ring, &txd, skb,
-						  hw_next_to_use, &invalid_frag);
-		if (unlikely(!num_descs)) {
-			dev_dbg(dev, "No descriptor available for ring %d\n", txdesc_ring->id);
-			edma_tx_dma_unmap_frags(skb, invalid_frag);
-			*txdesc = NULL;
-			return num_descs;
-		}
-
-		u64_stats_update_begin(&stats->syncp);
-		stats->tx_nr_frag_pkts++;
-		u64_stats_update_end(&stats->syncp);
 	}
 
-	dev_dbg(dev, "skb:%p num_descs_filled: %u, nr_frags %u, frag_list fragments %u\n",
+	dev_dbg(dev, "skb:%pK num_descs_filled: %u, nr_frags %u, frag_list fragments %u\n",
 		skb, num_descs, skb_shinfo(skb)->nr_frags, num_sg_frag_list);
 
 	*txdesc = txd;
@@ -775,7 +771,7 @@ enum edma_tx_status edma_tx_ring_xmit(struct net_device *netdev,
 			netdev_dbg(netdev, "No descriptor available for ring %d\n",
 				   txdesc_ring->id);
 			dma_unmap_single(dev, virt_to_phys(skb->data),
-					 skb->len, DMA_TO_DEVICE);
+					 skb_headlen(skb), DMA_TO_DEVICE);
 			u64_stats_update_begin(&txdesc_stats->syncp);
 			++txdesc_stats->no_desc_avail;
 			u64_stats_update_end(&txdesc_stats->syncp);
@@ -792,10 +788,12 @@ enum edma_tx_status edma_tx_ring_xmit(struct net_device *netdev,
 	txdesc_ring->prod_idx = hw_next_to_use & EDMA_TXDESC_PROD_IDX_MASK;
 	txdesc_ring->avail_desc -= num_desc_filled;
 
-	netdev_dbg(netdev, "%s: skb:%p tx_ring:%u proto:0x%x skb->len:%d\n port:%u prod_idx:%u ip_summed:0x%x\n",
+	netdev_dbg(netdev, "%s: skb:%pK tx_ring:%u proto:0x%x skb->len:%d\n port:%u prod_idx:%u ip_summed:0x%x\n",
 		   netdev->name, skb, txdesc_ring->id, ntohs(skb->protocol),
 		 skb->len, port_id, hw_next_to_use, skb->ip_summed);
 
+	dsb(st);
+
 	reg = EDMA_BASE_OFFSET + EDMA_REG_TXDESC_PROD_IDX(txdesc_ring->id);
 	regmap_write(regmap, reg, txdesc_ring->prod_idx);
 
diff --git a/drivers/net/ethernet/qualcomm/ppe/edma_tx.h b/drivers/net/ethernet/qualcomm/ppe/edma_tx.h
index c4fa63321..a71c641bc 100644
--- a/drivers/net/ethernet/qualcomm/ppe/edma_tx.h
+++ b/drivers/net/ethernet/qualcomm/ppe/edma_tx.h
@@ -1,5 +1,5 @@
 /* SPDX-License-Identifier: GPL-2.0-only
- * Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+ * Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
  */
 
 #ifndef __EDMA_TX__
@@ -66,7 +66,7 @@
 #define EDMA_TXDESC_L4_CSUM_SET(desc)  ((desc)->word5 |= \
 			       (FIELD_PREP(EDMA_TXDESC_L4_CSUM_SET_MASK, 1)))
 
-#define EDMA_TXDESC_POOL_ID_SET_MASK	GENMASK(24, 18)
+#define EDMA_TXDESC_POOL_ID_SET_MASK	GENMASK(23, 18)
 #define EDMA_TXDESC_POOL_ID_SET(desc, x)	((desc)->word5 |= \
 				(FIELD_PREP(EDMA_TXDESC_POOL_ID_SET_MASK, x)))
 
@@ -153,7 +153,7 @@ enum edma_tx_gso_status {
 };
 
 /**
- * struct edma_txcmpl_stats - EDMA TX complete ring statistics.
+ * struct edma_txcmpl_stats - EDMA Tx complete ring statistics.
  * @invalid_buffer: Invalid buffer address received.
  * @errors: Other Tx complete descriptor errors indicated by the hardware.
  * @desc_with_more_bit: Packet's segment transmit count.
@@ -181,7 +181,7 @@ struct edma_txdesc_stats {
 };
 
 /**
- * struct edma_txdesc_pri - EDMA primary TX descriptor.
+ * struct edma_txdesc_pri - EDMA primary Tx descriptor.
  * @word0: Low 32-bit of buffer address.
  * @word1: Buffer recycling, PTP tag flag, PRI valid flag.
  * @word2: Low 32-bit of opaque value.
@@ -203,7 +203,7 @@ struct edma_txdesc_pri {
 };
 
 /**
- * struct edma_txdesc_sec - EDMA secondary TX descriptor.
+ * struct edma_txdesc_sec - EDMA secondary Tx descriptor.
  * @word0: Reserved.
  * @word1: Custom csum offset, payload offset, TTL/NAT action.
  * @word2: NAPT translated port, DSCP value, TTL value.
@@ -225,7 +225,7 @@ struct edma_txdesc_sec {
 };
 
 /**
- * struct edma_txcmpl_desc - EDMA TX complete descriptor.
+ * struct edma_txcmpl_desc - EDMA Tx complete descriptor.
  * @word0: Low 32-bit opaque value.
  * @word1: High 32-bit opaque value.
  * @word2: More fragment, transmit ring id, pool id.
@@ -239,7 +239,7 @@ struct edma_txcmpl_desc {
 };
 
 /**
- * struct edma_txdesc_ring - EDMA TX descriptor ring
+ * struct edma_txdesc_ring - EDMA Tx descriptor ring
  * @prod_idx: Producer index
  * @id: Tx ring number
  * @avail_desc: Number of available descriptor to process
@@ -265,7 +265,7 @@ struct edma_txdesc_ring {
 };
 
 /**
- * struct edma_txcmpl_ring - EDMA TX complete ring
+ * struct edma_txcmpl_ring - EDMA Tx complete ring
  * @napi: NAPI
  * @cons_idx: Consumer index
  * @avail_pkt: Number of available packets to process
diff --git a/drivers/net/ethernet/qualcomm/ppe/ppe.c b/drivers/net/ethernet/qualcomm/ppe/ppe.c
index 6b64f934c..4d6ec79eb 100644
--- a/drivers/net/ethernet/qualcomm/ppe/ppe.c
+++ b/drivers/net/ethernet/qualcomm/ppe/ppe.c
@@ -1,6 +1,6 @@
 // SPDX-License-Identifier: GPL-2.0-only
 /*
- * Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+ * Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
  */
 
 /* PPE platform device probe, DTSI parser and PPE clock initializations. */
@@ -23,8 +23,8 @@
 #define PPE_PORT_MAX		8
 #define PPE_CLK_RATE		353000000
 
-/* ICC clocks for enabling PPE device. The avg and peak with value 0
- * will be decided by the clock rate of PPE.
+/* ICC clocks for enabling PPE device. The avg_bw and peak_bw with value 0
+ * will be updated by the clock rate of PPE.
  */
 static const struct icc_bulk_data ppe_icc_data[] = {
 	{
@@ -137,9 +137,9 @@ static int ppe_clock_init_and_reset(struct ppe_device *ppe_dev)
 	if (ret)
 		return ret;
 
-	/* PPE clocks take the same clock tree, which work on the same
-	 * clock rate. Setting the clock rate of "ppe" ensures the clock
-	 * rate of all PPE clocks configured as same.
+	/* The PPE clocks have a common parent clock. Setting the clock
+	 * rate of "ppe" ensures the clock rate of all PPE clocks is
+	 * configured to the same rate.
 	 */
 	clk = devm_clk_get(dev, "ppe");
 	if (IS_ERR(clk))
@@ -153,25 +153,19 @@ static int ppe_clock_init_and_reset(struct ppe_device *ppe_dev)
 	if (ret < 0)
 		return ret;
 
+	/* Reset the PPE. */
 	rstc = devm_reset_control_get_exclusive(dev, NULL);
 	if (IS_ERR(rstc))
 		return PTR_ERR(rstc);
 
-	/* Reset PPE, the delay 100ms of assert and deassert is necessary
-	 * for resetting PPE.
-	 */
 	ret = reset_control_assert(rstc);
 	if (ret)
 		return ret;
 
-	msleep(100);
-	ret = reset_control_deassert(rstc);
-	if (ret)
-		return ret;
-
-	msleep(100);
+	/* The delay 10 ms of assert is necessary for resetting PPE. */
+	usleep_range(10000, 11000);
 
-	return 0;
+	return reset_control_deassert(rstc);
 }
 
 static int qcom_ppe_probe(struct platform_device *pdev)
@@ -182,11 +176,10 @@ static int qcom_ppe_probe(struct platform_device *pdev)
 	int ret, num_icc;
 
 	num_icc = ARRAY_SIZE(ppe_icc_data);
-	ppe_dev = devm_kzalloc(dev,
-			       struct_size(ppe_dev, icc_paths, num_icc),
+	ppe_dev = devm_kzalloc(dev, struct_size(ppe_dev, icc_paths, num_icc),
 			       GFP_KERNEL);
 	if (!ppe_dev)
-		return dev_err_probe(dev, -ENOMEM, "PPE alloc memory failed\n");
+		return -ENOMEM;
 
 	base = devm_platform_ioremap_resource(pdev, 0);
 	if (IS_ERR(base))
@@ -231,16 +224,14 @@ static void qcom_ppe_remove(struct platform_device *pdev)
 	struct ppe_device *ppe_dev;
 
 	ppe_dev = platform_get_drvdata(pdev);
-	ppe_debugfs_teardown(ppe_dev);
 	ppe_port_mac_deinit(ppe_dev);
+	ppe_debugfs_teardown(ppe_dev);
 	edma_destroy(ppe_dev);
-
-	platform_set_drvdata(pdev, NULL);
 }
 
 static const struct of_device_id qcom_ppe_of_match[] = {
 	{ .compatible = "qcom,ipq9574-ppe" },
-	{},
+	{}
 };
 MODULE_DEVICE_TABLE(of, qcom_ppe_of_match);
 
@@ -255,4 +246,4 @@ static struct platform_driver qcom_ppe_driver = {
 module_platform_driver(qcom_ppe_driver);
 
 MODULE_LICENSE("GPL");
-MODULE_DESCRIPTION("Qualcomm IPQ PPE driver");
+MODULE_DESCRIPTION("Qualcomm Technologies, Inc. IPQ PPE driver");
diff --git a/drivers/net/ethernet/qualcomm/ppe/ppe.h b/drivers/net/ethernet/qualcomm/ppe/ppe.h
index 020d5df2c..c7fd4fc48 100644
--- a/drivers/net/ethernet/qualcomm/ppe/ppe.h
+++ b/drivers/net/ethernet/qualcomm/ppe/ppe.h
@@ -1,6 +1,6 @@
 /* SPDX-License-Identifier: GPL-2.0-only
  *
- * Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+ * Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
  */
 
 #ifndef __PPE_H__
@@ -13,13 +13,15 @@ struct device;
 struct regmap;
 struct dentry;
 
+struct ppe_ports;
+
 /**
  * struct ppe_device - PPE device private data.
  * @dev: PPE device structure.
  * @regmap: PPE register map.
  * @clk_rate: PPE clock rate.
  * @num_ports: Number of PPE ports.
- * @debugfs_root: PPE debug root entry.
+ * @debugfs_root: Debugfs root entry.
  * @ports: PPE MAC ports.
  * @num_icc_paths: Number of interconnect paths.
  * @icc_paths: Interconnect path array.
diff --git a/drivers/net/ethernet/qualcomm/ppe/ppe_api.c b/drivers/net/ethernet/qualcomm/ppe/ppe_api.c
index 6199c7025..4a26c278a 100644
--- a/drivers/net/ethernet/qualcomm/ppe/ppe_api.c
+++ b/drivers/net/ethernet/qualcomm/ppe/ppe_api.c
@@ -1,6 +1,6 @@
 // SPDX-License-Identifier: GPL-2.0-only
 /*
- * Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+ * Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
  */
 
 #include "ppe.h"
@@ -8,26 +8,29 @@
 #include "ppe_config.h"
 
 /**
- * ppe_queue_priority_set - set scheduler priority of PPE hardware queue
+ * ppe_queue_node_priority_set - set scheduler priority of PPE queue or flow
  * @ppe_dev: PPE device
- * @node_id: PPE hardware node ID, which is either queue ID or flow ID
- * @priority: Qos scheduler priority
+ * @node_id: PPE hardware node ID, which can be queue ID or flow ID.
+ * @priority: PPE discipline scheduler priority
  *
- * Configure scheduler priority of PPE hardware queque, the maximum node
- * ID supported is PPE_QUEUE_ID_NUM added by PPE_FLOW_ID_NUM, queue ID
- * belongs to level 0, flow ID belongs to level 1 in the packet pipeline.
+ * Configure scheduler priority for a given PPE node. Node may be of type
+ * PPE queue or flow. The packet is dispatched first by queue scheduler
+ * (level 0), then dispatched by flow scheduler (level 1).
  *
  * Return 0 on success, negative error code on failure.
  */
-int ppe_queue_priority_set(struct ppe_device *ppe_dev,
-			   int node_id, int priority)
+int ppe_queue_node_priority_set(struct ppe_device *ppe_dev,
+				int node_id, int priority)
 {
-	struct ppe_qos_scheduler_cfg sch_cfg;
+	struct ppe_scheduler_cfg sch_cfg;
 	int ret, port, level = 0;
 
-	if (node_id >= PPE_QUEUE_ID_NUM) {
+	if (node_id >= PPE_QUEUE_ID_MAX + PPE_FLOW_ID_MAX)
+		return -EINVAL;
+
+	if (node_id >= PPE_QUEUE_ID_MAX) {
 		level = 1;
-		node_id -= PPE_QUEUE_ID_NUM;
+		node_id -= PPE_QUEUE_ID_MAX;
 	}
 
 	ret = ppe_queue_scheduler_get(ppe_dev, node_id, level, &port, &sch_cfg);
@@ -42,25 +45,26 @@ int ppe_queue_priority_set(struct ppe_device *ppe_dev,
 /**
  * ppe_edma_queue_offset_config - Configure queue offset for EDMA interface
  * @ppe_dev: PPE device
- * @class: The class to configure queue offset
- * @index: Class index, internal priority or hash value
- * @queue_offset: Queue offset value
+ * @type: The type can be internal priority or PPE hash
+ * @index: Class index, which can be internal priority or hash value
+ * @queue_offset: Queue offset value which is added by the queue base to get
+ * 		  the egress queue ID.
  *
  * PPE EDMA queue offset is configured based on the PPE internal priority or
- * RSS hash value, the profile ID is fixed to 0 for EDMA interface.
+ * RSS hash value, the profile ID is fixed to 0 for the EDMA interface.
  *
  * Return 0 on success, negative error code on failure.
  */
 int ppe_edma_queue_offset_config(struct ppe_device *ppe_dev,
-				 enum ppe_queue_class_type class,
+				 enum ppe_queue_offset_type type,
 				 int index, int queue_offset)
 {
-	if (class == PPE_QUEUE_CLASS_PRIORITY)
-		return ppe_queue_ucast_pri_class_set(ppe_dev, 0,
-						     index, queue_offset);
+	if (type == PPE_QUEUE_OFFSET_BY_PRIORITY)
+		return ppe_queue_ucast_offset_pri_set(ppe_dev, 0,
+						      index, queue_offset);
 
-	return ppe_queue_ucast_hash_class_set(ppe_dev, 0,
-					      index, queue_offset);
+	return ppe_queue_ucast_offset_hash_set(ppe_dev, 0,
+					       index, queue_offset);
 }
 
 /**
@@ -74,7 +78,8 @@ int ppe_edma_queue_offset_config(struct ppe_device *ppe_dev,
  *
  * Return 0 on success, negative error code on failure.
  */
-int ppe_edma_queue_resource_get(struct ppe_device *ppe_dev, int type,
+int ppe_edma_queue_resource_get(struct ppe_device *ppe_dev,
+				enum ppe_resource_type type,
 				int *res_start, int *res_end)
 {
 	if (type != PPE_RES_UCAST && type != PPE_RES_MCAST)
@@ -84,20 +89,21 @@ int ppe_edma_queue_resource_get(struct ppe_device *ppe_dev, int type,
 };
 
 /**
- * ppe_edma_ring_to_queues_config - Map EDMA ring to PPE queues
+ * ppe_edma_ring_to_queues_config - Configure EDMA ring to queue mapping in PPE
  * @ppe_dev: PPE device
  * @ring_id: EDMA ring ID
  * @num: Number of queues mapped to EDMA ring
  * @queues: PPE queue IDs
  *
- * PPE queues are configured to map with the special EDMA ring ID.
+ * Enable EDMA ring to PPE queue mapping configuration for packet
+ * receive to an EDMA ring.
  *
  * Return 0 on success, negative error code on failure.
  */
 int ppe_edma_ring_to_queues_config(struct ppe_device *ppe_dev, int ring_id,
-				   int num, int queues[] __counted_by(num))
+				   int num, int queues[])
 {
-	u32 queue_bmap[PPE_RING_MAPPED_BP_QUEUE_WORD_COUNT] = {};
+	u32 queue_bmap[PPE_RING_TO_QUEUE_BITMAP_WORD_CNT] = {};
 	int index;
 
 	for (index = 0; index < num; index++)
diff --git a/drivers/net/ethernet/qualcomm/ppe/ppe_api.h b/drivers/net/ethernet/qualcomm/ppe/ppe_api.h
index 2135b5383..9361bf197 100644
--- a/drivers/net/ethernet/qualcomm/ppe/ppe_api.h
+++ b/drivers/net/ethernet/qualcomm/ppe/ppe_api.h
@@ -1,60 +1,43 @@
 /* SPDX-License-Identifier: GPL-2.0-only
  *
- * Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+ * Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
  */
 
-/* These may also be used by higher level network drivers such as ethernet or
- * QoS drivers.
+/* Functions for low level PPE configurations which are needed during ethernet
+ * driver initialization.
  */
 
 #ifndef __PPE_API_H__
 #define __PPE_API_H__
 
 #include "ppe.h"
+#include "ppe_config.h"
 
-#define PPE_QUEUE_ID_NUM			300
-#define PPE_FLOW_ID_NUM				64
-#define PPE_QUEUE_SCH_PRI_NUM			8
-#define PPE_QUEUE_INTER_PRI_NUM			16
+#define PPE_QUEUE_ID_MAX			300
+#define PPE_FLOW_ID_MAX				64
+#define PPE_QUEUE_INTERNAL_PRI_NUM		16
 #define PPE_QUEUE_HASH_NUM			256
 
-/* The service code is used by EDMA driver to transmit packet to PPE. */
-#define PPE_EDMA_SC_BYPASS_ID			1
-
-/**
- * enum ppe_queue_class_type - PPE queue class type
- * @PPE_QUEUE_CLASS_PRIORITY: Queue offset configured from internal priority
- * @PPE_QUEUE_CLASS_HASH: Queue offset configured from RSS hash.
- */
-enum ppe_queue_class_type {
-	PPE_QUEUE_CLASS_PRIORITY,
-	PPE_QUEUE_CLASS_HASH,
-};
-
 /**
- * enum ppe_resource_type - PPE resource type
- * @PPE_RES_UCAST: Unicast queue resource
- * @PPE_RES_MCAST: Multicast queue resource
- * @PPE_RES_FLOW_ID: Flow resource
- * @PPE_RES_L0_NODE: Level 0 QoS node resource
- * @PPE_RES_L1_NODE: Level 1 QoS node resource
+ * enum ppe_queue_offset_type - PPE queue offset type
+ * @PPE_QUEUE_CLASS_PRIORITY: Queue offset decided by PPE internal priority
+ * @PPE_QUEUE_CLASS_HASH: Queue offset decided by PPE RSS hash value.
  */
-enum ppe_resource_type {
-	PPE_RES_UCAST,
-	PPE_RES_MCAST,
-	PPE_RES_FLOW_ID,
-	PPE_RES_L0_NODE,
-	PPE_RES_L1_NODE,
+enum ppe_queue_offset_type {
+	PPE_QUEUE_OFFSET_BY_PRIORITY,
+	PPE_QUEUE_OFFSET_BY_HASH,
 };
 
-int ppe_queue_priority_set(struct ppe_device *ppe_dev,
-			   int queue_id, int priority);
+int ppe_queue_node_priority_set(struct ppe_device *ppe_dev,
+				int node_id, int priority);
 
 int ppe_edma_queue_offset_config(struct ppe_device *ppe_dev,
-				 enum ppe_queue_class_type class,
+				 enum ppe_queue_offset_type type,
 				 int index, int queue_offset);
-int ppe_edma_queue_resource_get(struct ppe_device *ppe_dev, int type,
+
+int ppe_edma_queue_resource_get(struct ppe_device *ppe_dev,
+				enum ppe_resource_type type,
 				int *res_start, int *res_end);
 int ppe_edma_ring_to_queues_config(struct ppe_device *ppe_dev, int ring_id,
-				   int num, int queues[] __counted_by(num));
+				   int num, int queues[]);
 #endif
diff --git a/drivers/net/ethernet/qualcomm/ppe/ppe_config.c b/drivers/net/ethernet/qualcomm/ppe/ppe_config.c
index 621f4f0ba..a89298a39 100644
--- a/drivers/net/ethernet/qualcomm/ppe/ppe_config.c
+++ b/drivers/net/ethernet/qualcomm/ppe/ppe_config.c
@@ -1,6 +1,6 @@
 // SPDX-License-Identifier: GPL-2.0-only
 /*
- * Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+ * Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
  */
 
 /* PPE HW initialization configs such as BM(buffer management),
@@ -14,10 +14,11 @@
 #include <linux/regmap.h>
 
 #include "ppe.h"
-#include "ppe_api.h"
 #include "ppe_config.h"
 #include "ppe_regs.h"
 
+#define PPE_QUEUE_SCH_PRI_NUM		8
+
 /**
  * struct ppe_bm_port_config - PPE BM port configuration.
  * @port_id_start: The fist BM port ID to configure.
@@ -54,6 +55,9 @@ struct ppe_bm_port_config {
  * @weight: Weight value.
  * @resume_offset: Resume offset from the threshold.
  * @dynamic: Threshold value is decided dynamically or statically.
+ *
+ * Queue configuration decides the threshold to drop packet from PPE
+ * hardware queue.
  */
 struct ppe_qm_queue_config {
 	unsigned int queue_start;
@@ -66,39 +70,54 @@ struct ppe_qm_queue_config {
 };
 
 /**
- * struct ppe_sch_bm_config - PPE arbitration for buffer config.
+ * enum ppe_scheduler_direction - PPE scheduler direction for packet.
+ * @PPE_SCH_INGRESS: Scheduler for the packet on ingress,
+ * @PPE_SCH_EGRESS: Scheduler for the packet on egress,
+ */
+enum ppe_scheduler_direction {
+	PPE_SCH_INGRESS = 0,
+	PPE_SCH_EGRESS = 1,
+};
+
+/**
+ * struct ppe_scheduler_bm_config - PPE arbitration for buffer config.
  * @valid: Arbitration entry valid or not.
- * @is_egress: Arbitration entry for egress or not.
+ * @dir: Arbitration entry for egress or ingress.
  * @port: Port ID to use arbitration entry.
- * @second_valid: Second port valid or not.
- * @second_port: Second port to use.
+ * @backup_port_valid: Backup port valid or not.
+ * @backup_port: Backup port ID to use.
+ *
+ * Configure the scheduler settings for accessing and releasing the PPE buffers.
  */
-struct ppe_sch_bm_config {
+struct ppe_scheduler_bm_config {
 	bool valid;
-	bool is_egress;
+	enum ppe_scheduler_direction dir;
 	unsigned int port;
-	bool second_valid;
-	unsigned int second_port;
+	bool backup_port_valid;
+	unsigned int backup_port;
 };
 
 /**
- * struct ppe_sch_schedule_config - PPE arbitration for scheduler config.
+ * struct ppe_scheduler_qm_config - PPE arbitration for scheduler config.
  * @ensch_port_bmp: Port bit map for enqueue scheduler.
  * @ensch_port: Port ID to enqueue scheduler.
  * @desch_port: Port ID to dequeue scheduler.
- * @desch_second_valid: Dequeue for the second port valid or not.
- * @desch_second_port: Second port ID to dequeue scheduler.
+ * @desch_backup_port_valid: Dequeue for the backup port valid or not.
+ * @desch_backup_port: Backup port ID to dequeue scheduler.
+ *
+ * Configure the scheduler settings for enqueuing and dequeuing packets on
+ * the PPE port.
  */
-struct ppe_sch_schedule_config {
+struct ppe_scheduler_qm_config {
 	unsigned int ensch_port_bmp;
 	unsigned int ensch_port;
 	unsigned int desch_port;
-	bool desch_second_valid;
-	unsigned int desch_second_port;
+	bool desch_backup_port_valid;
+	unsigned int desch_backup_port;
 };
 
 /**
- * struct ppe_port_schedule_config - PPE port scheduler config.
+ * struct ppe_scheduler_port_config - PPE port scheduler config.
  * @port: Port ID to be scheduled.
  * @flow_level: Scheduler flow level or not.
  * @node_id: Node ID, for level 0, queue ID is used.
@@ -107,10 +126,10 @@ struct ppe_sch_schedule_config {
  * @flow_id: Strict priority ID.
  * @drr_node_id: Node ID for scheduler.
  *
- * PPE scheduler config, which decides the packet scheduler priority
- * from egress port.
+ * PPE port scheduler configuration which decides the priority in the
+ * packet scheduler for the egress port.
  */
-struct ppe_port_schedule_config {
+struct ppe_scheduler_port_config {
 	unsigned int port;
 	bool flow_level;
 	unsigned int node_id;
@@ -148,9 +167,21 @@ struct ppe_port_schedule_resource {
 	unsigned int l1node_end;
 };
 
-static int ipq9574_ppe_bm_group_config = 1550;
-static struct ppe_bm_port_config ipq9574_ppe_bm_port_config[] = {
+/* There are total 2048 buffers available in PPE, out of which some
+ * buffers are reserved for some specific purposes. The rest of the
+ * pool of 1550 buffers are assigned to the general 'group0' which
+ * is shared among all ports of the PPE.
+ */
+static const int ipq9574_ppe_bm_group_config = 1550;
+
+/* The buffer configurations per PPE port. There are 15 BM ports and
+ * 4 BM groups supported by PPE. BM port (0-7) is for EDMA port 0,
+ * BM port (8-13) is for PPE physical port 1-6 and BM port 14 is for
+ * EIP port.
+ */
+static const struct ppe_bm_port_config ipq9574_ppe_bm_port_config[] = {
 	{
+		/* Buffer configuration for the BM port ID 0 of EDMA. */
 		.port_id_start	= 0,
 		.port_id_end	= 0,
 		.pre_alloc	= 0,
@@ -162,6 +193,7 @@ static struct ppe_bm_port_config ipq9574_ppe_bm_port_config[] = {
 		.dynamic	= true,
 	},
 	{
+		/* Buffer configuration for the BM port ID 1-7 of EDMA. */
 		.port_id_start	= 1,
 		.port_id_end	= 7,
 		.pre_alloc	= 0,
@@ -173,6 +205,7 @@ static struct ppe_bm_port_config ipq9574_ppe_bm_port_config[] = {
 		.dynamic	= true,
 	},
 	{
+		/* Buffer configuration for the BM port ID 8-13 of PPE ports. */
 		.port_id_start	= 8,
 		.port_id_end	= 13,
 		.pre_alloc	= 0,
@@ -184,6 +217,7 @@ static struct ppe_bm_port_config ipq9574_ppe_bm_port_config[] = {
 		.dynamic	= true,
 	},
 	{
+		/* Buffer configuration for the BM port ID 14 of EIP. */
 		.port_id_start	= 14,
 		.port_id_end	= 14,
 		.pre_alloc	= 0,
@@ -196,21 +230,29 @@ static struct ppe_bm_port_config ipq9574_ppe_bm_port_config[] = {
 	},
 };
 
-/* Default QM group settings for IPQ9754. */
-static int ipq9574_ppe_qm_group_config = 2000;
+/* QM receives the packet from PPE buffer management, there are total
+ * 2048 buffers available, out of which some buffers are dedicated to
+ * hardware exception handers, the rest of the pool of 2000 are assigned
+ * to the queue general 'group0' which all queues are assigned to by
+ * default. The QM group configuraton limits total buffers enqueued by
+ * all PPE hardware queues.
+ */
+static const int ipq9574_ppe_qm_group_config = 2000;
 
 /* Default QM settings for unicast and multicast queues for IPQ9754. */
-static struct ppe_qm_queue_config ipq9574_ppe_qm_queue_config[] = {
+static const struct ppe_qm_queue_config ipq9574_ppe_qm_queue_config[] = {
 	{
+		/* QM settings for unicast queues 0 to 255. */
 		.queue_start	= 0,
 		.queue_end	= 255,
 		.prealloc_buf	= 0,
-		.ceil		= 400,
-		.weight		= 4,
+		.ceil		= 1200,
+		.weight		= 7,
 		.resume_offset	= 36,
 		.dynamic	= true,
 	},
 	{
+		/* QM settings for multicast queues 256 to 299. */
 		.queue_start	= 256,
 		.queue_end	= 299,
 		.prealloc_buf	= 0,
@@ -221,172 +263,187 @@ static struct ppe_qm_queue_config ipq9574_ppe_qm_queue_config[] = {
 	},
 };
 
-static struct ppe_sch_bm_config ipq9574_ppe_sch_bm_config[] = {
-	{1, 0, 0, 0, 0},
-	{1, 1, 0, 0, 0},
-	{1, 0, 5, 0, 0},
-	{1, 1, 5, 0, 0},
-	{1, 0, 6, 0, 0},
-	{1, 1, 6, 0, 0},
-	{1, 0, 1, 0, 0},
-	{1, 1, 1, 0, 0},
-	{1, 0, 0, 0, 0},
-	{1, 1, 0, 0, 0},
-	{1, 0, 5, 0, 0},
-	{1, 1, 5, 0, 0},
-	{1, 0, 6, 0, 0},
-	{1, 1, 6, 0, 0},
-	{1, 0, 7, 0, 0},
-	{1, 1, 7, 0, 0},
-	{1, 0, 0, 0, 0},
-	{1, 1, 0, 0, 0},
-	{1, 0, 1, 0, 0},
-	{1, 1, 1, 0, 0},
-	{1, 0, 5, 0, 0},
-	{1, 1, 5, 0, 0},
-	{1, 0, 6, 0, 0},
-	{1, 1, 6, 0, 0},
-	{1, 0, 2, 0, 0},
-	{1, 1, 2, 0, 0},
-	{1, 0, 0, 0, 0},
-	{1, 1, 0, 0, 0},
-	{1, 0, 5, 0, 0},
-	{1, 1, 5, 0, 0},
-	{1, 0, 6, 0, 0},
-	{1, 1, 6, 0, 0},
-	{1, 0, 1, 0, 0},
-	{1, 1, 1, 0, 0},
-	{1, 0, 3, 0, 0},
-	{1, 1, 3, 0, 0},
-	{1, 0, 0, 0, 0},
-	{1, 1, 0, 0, 0},
-	{1, 0, 5, 0, 0},
-	{1, 1, 5, 0, 0},
-	{1, 0, 6, 0, 0},
-	{1, 1, 6, 0, 0},
-	{1, 0, 7, 0, 0},
-	{1, 1, 7, 0, 0},
-	{1, 0, 0, 0, 0},
-	{1, 1, 0, 0, 0},
-	{1, 0, 1, 0, 0},
-	{1, 1, 1, 0, 0},
-	{1, 0, 5, 0, 0},
-	{1, 1, 5, 0, 0},
-	{1, 0, 6, 0, 0},
-	{1, 1, 6, 0, 0},
-	{1, 0, 4, 0, 0},
-	{1, 1, 4, 0, 0},
-	{1, 0, 0, 0, 0},
-	{1, 1, 0, 0, 0},
-	{1, 0, 5, 0, 0},
-	{1, 1, 5, 0, 0},
-	{1, 0, 6, 0, 0},
-	{1, 1, 6, 0, 0},
-	{1, 0, 1, 0, 0},
-	{1, 1, 1, 0, 0},
-	{1, 0, 0, 0, 0},
-	{1, 1, 0, 0, 0},
-	{1, 0, 5, 0, 0},
-	{1, 1, 5, 0, 0},
-	{1, 0, 6, 0, 0},
-	{1, 1, 6, 0, 0},
-	{1, 0, 2, 0, 0},
-	{1, 1, 2, 0, 0},
-	{1, 0, 0, 0, 0},
-	{1, 1, 0, 0, 0},
-	{1, 0, 7, 0, 0},
-	{1, 1, 7, 0, 0},
-	{1, 0, 5, 0, 0},
-	{1, 1, 5, 0, 0},
-	{1, 0, 6, 0, 0},
-	{1, 1, 6, 0, 0},
-	{1, 0, 1, 0, 0},
-	{1, 1, 1, 0, 0},
-	{1, 0, 0, 0, 0},
-	{1, 1, 0, 0, 0},
-	{1, 0, 5, 0, 0},
-	{1, 1, 5, 0, 0},
-	{1, 0, 6, 0, 0},
-	{1, 1, 6, 0, 0},
-	{1, 0, 3, 0, 0},
-	{1, 1, 3, 0, 0},
-	{1, 0, 1, 0, 0},
-	{1, 1, 1, 0, 0},
-	{1, 0, 0, 0, 0},
-	{1, 1, 0, 0, 0},
-	{1, 0, 5, 0, 0},
-	{1, 1, 5, 0, 0},
-	{1, 0, 6, 0, 0},
-	{1, 1, 6, 0, 0},
-	{1, 0, 4, 0, 0},
-	{1, 1, 4, 0, 0},
-	{1, 0, 7, 0, 0},
-	{1, 1, 7, 0, 0},
+/* Scheduler configuration for the assigning and releasing buffers for the
+ * packet passing through PPE, which is different per SoC.
+ * PPE scheduler configuration for BM includes multiple entries, each entry
+ * indicates the primary port to be assigned the buffers for the ingress or
+ * release the buffers for the egress, And support the backup port ID that
+ * will be used when the primary port ID is down. Each entry can be optionally
+ * configured as invalid.
+ */
+static const struct ppe_scheduler_bm_config ipq9574_ppe_sch_bm_config[] = {
+	{true, PPE_SCH_INGRESS, 0, false, 0},
+	{true, PPE_SCH_EGRESS,  0, false, 0},
+	{true, PPE_SCH_INGRESS, 5, false, 0},
+	{true, PPE_SCH_EGRESS,  5, false, 0},
+	{true, PPE_SCH_INGRESS, 6, false, 0},
+	{true, PPE_SCH_EGRESS,  6, false, 0},
+	{true, PPE_SCH_INGRESS, 1, false, 0},
+	{true, PPE_SCH_EGRESS,  1, false, 0},
+	{true, PPE_SCH_INGRESS, 0, false, 0},
+	{true, PPE_SCH_EGRESS,  0, false, 0},
+	{true, PPE_SCH_INGRESS, 5, false, 0},
+	{true, PPE_SCH_EGRESS,  5, false, 0},
+	{true, PPE_SCH_INGRESS, 6, false, 0},
+	{true, PPE_SCH_EGRESS,  6, false, 0},
+	{true, PPE_SCH_INGRESS, 7, false, 0},
+	{true, PPE_SCH_EGRESS,  7, false, 0},
+	{true, PPE_SCH_INGRESS, 0, false, 0},
+	{true, PPE_SCH_EGRESS,  0, false, 0},
+	{true, PPE_SCH_INGRESS, 1, false, 0},
+	{true, PPE_SCH_EGRESS,  1, false, 0},
+	{true, PPE_SCH_INGRESS, 5, false, 0},
+	{true, PPE_SCH_EGRESS,  5, false, 0},
+	{true, PPE_SCH_INGRESS, 6, false, 0},
+	{true, PPE_SCH_EGRESS,  6, false, 0},
+	{true, PPE_SCH_INGRESS, 2, false, 0},
+	{true, PPE_SCH_EGRESS,  2, false, 0},
+	{true, PPE_SCH_INGRESS, 0, false, 0},
+	{true, PPE_SCH_EGRESS,  0, false, 0},
+	{true, PPE_SCH_INGRESS, 5, false, 0},
+	{true, PPE_SCH_EGRESS,  5, false, 0},
+	{true, PPE_SCH_INGRESS, 6, false, 0},
+	{true, PPE_SCH_EGRESS,  6, false, 0},
+	{true, PPE_SCH_INGRESS, 1, false, 0},
+	{true, PPE_SCH_EGRESS,  1, false, 0},
+	{true, PPE_SCH_INGRESS, 3, false, 0},
+	{true, PPE_SCH_EGRESS,  3, false, 0},
+	{true, PPE_SCH_INGRESS, 0, false, 0},
+	{true, PPE_SCH_EGRESS,  0, false, 0},
+	{true, PPE_SCH_INGRESS, 5, false, 0},
+	{true, PPE_SCH_EGRESS,  5, false, 0},
+	{true, PPE_SCH_INGRESS, 6, false, 0},
+	{true, PPE_SCH_EGRESS,  6, false, 0},
+	{true, PPE_SCH_INGRESS, 7, false, 0},
+	{true, PPE_SCH_EGRESS,  7, false, 0},
+	{true, PPE_SCH_INGRESS, 0, false, 0},
+	{true, PPE_SCH_EGRESS,  0, false, 0},
+	{true, PPE_SCH_INGRESS, 1, false, 0},
+	{true, PPE_SCH_EGRESS,  1, false, 0},
+	{true, PPE_SCH_INGRESS, 5, false, 0},
+	{true, PPE_SCH_EGRESS,  5, false, 0},
+	{true, PPE_SCH_INGRESS, 6, false, 0},
+	{true, PPE_SCH_EGRESS,  6, false, 0},
+	{true, PPE_SCH_INGRESS, 4, false, 0},
+	{true, PPE_SCH_EGRESS,  4, false, 0},
+	{true, PPE_SCH_INGRESS, 0, false, 0},
+	{true, PPE_SCH_EGRESS,  0, false, 0},
+	{true, PPE_SCH_INGRESS, 5, false, 0},
+	{true, PPE_SCH_EGRESS,  5, false, 0},
+	{true, PPE_SCH_INGRESS, 6, false, 0},
+	{true, PPE_SCH_EGRESS,  6, false, 0},
+	{true, PPE_SCH_INGRESS, 1, false, 0},
+	{true, PPE_SCH_EGRESS,  1, false, 0},
+	{true, PPE_SCH_INGRESS, 0, false, 0},
+	{true, PPE_SCH_EGRESS,  0, false, 0},
+	{true, PPE_SCH_INGRESS, 5, false, 0},
+	{true, PPE_SCH_EGRESS,  5, false, 0},
+	{true, PPE_SCH_INGRESS, 6, false, 0},
+	{true, PPE_SCH_EGRESS,  6, false, 0},
+	{true, PPE_SCH_INGRESS, 2, false, 0},
+	{true, PPE_SCH_EGRESS,  2, false, 0},
+	{true, PPE_SCH_INGRESS, 0, false, 0},
+	{true, PPE_SCH_EGRESS,  0, false, 0},
+	{true, PPE_SCH_INGRESS, 7, false, 0},
+	{true, PPE_SCH_EGRESS,  7, false, 0},
+	{true, PPE_SCH_INGRESS, 5, false, 0},
+	{true, PPE_SCH_EGRESS,  5, false, 0},
+	{true, PPE_SCH_INGRESS, 6, false, 0},
+	{true, PPE_SCH_EGRESS,  6, false, 0},
+	{true, PPE_SCH_INGRESS, 1, false, 0},
+	{true, PPE_SCH_EGRESS,  1, false, 0},
+	{true, PPE_SCH_INGRESS, 0, false, 0},
+	{true, PPE_SCH_EGRESS,  0, false, 0},
+	{true, PPE_SCH_INGRESS, 5, false, 0},
+	{true, PPE_SCH_EGRESS,  5, false, 0},
+	{true, PPE_SCH_INGRESS, 6, false, 0},
+	{true, PPE_SCH_EGRESS,  6, false, 0},
+	{true, PPE_SCH_INGRESS, 3, false, 0},
+	{true, PPE_SCH_EGRESS,  3, false, 0},
+	{true, PPE_SCH_INGRESS, 1, false, 0},
+	{true, PPE_SCH_EGRESS,  1, false, 0},
+	{true, PPE_SCH_INGRESS, 0, false, 0},
+	{true, PPE_SCH_EGRESS,  0, false, 0},
+	{true, PPE_SCH_INGRESS, 5, false, 0},
+	{true, PPE_SCH_EGRESS,  5, false, 0},
+	{true, PPE_SCH_INGRESS, 6, false, 0},
+	{true, PPE_SCH_EGRESS,  6, false, 0},
+	{true, PPE_SCH_INGRESS, 4, false, 0},
+	{true, PPE_SCH_EGRESS,  4, false, 0},
+	{true, PPE_SCH_INGRESS, 7, false, 0},
+	{true, PPE_SCH_EGRESS,  7, false, 0},
 };
 
-static struct ppe_sch_schedule_config ipq9574_ppe_sch_schedule_config[] = {
-	{0x98, 6, 0, 1, 1},
-	{0x94, 5, 6, 1, 3},
-	{0x86, 0, 5, 1, 4},
-	{0x8C, 1, 6, 1, 0},
-	{0x1C, 7, 5, 1, 1},
-	{0x98, 2, 6, 1, 0},
-	{0x1C, 5, 7, 1, 1},
-	{0x34, 3, 6, 1, 0},
-	{0x8C, 4, 5, 1, 1},
-	{0x98, 2, 6, 1, 0},
-	{0x8C, 5, 4, 1, 1},
-	{0xA8, 0, 6, 1, 2},
-	{0x98, 5, 1, 1, 0},
-	{0x98, 6, 5, 1, 2},
-	{0x89, 1, 6, 1, 4},
-	{0xA4, 3, 0, 1, 1},
-	{0x8C, 5, 6, 1, 4},
-	{0xA8, 0, 2, 1, 1},
-	{0x98, 6, 5, 1, 0},
-	{0xC4, 4, 3, 1, 1},
-	{0x94, 6, 5, 1, 0},
-	{0x1C, 7, 6, 1, 1},
-	{0x98, 2, 5, 1, 0},
-	{0x1C, 6, 7, 1, 1},
-	{0x1C, 5, 6, 1, 0},
-	{0x94, 3, 5, 1, 1},
-	{0x8C, 4, 6, 1, 0},
-	{0x94, 1, 5, 1, 3},
-	{0x94, 6, 1, 1, 0},
-	{0xD0, 3, 5, 1, 2},
-	{0x98, 6, 0, 1, 1},
-	{0x94, 5, 6, 1, 3},
-	{0x94, 1, 5, 1, 0},
-	{0x98, 2, 6, 1, 1},
-	{0x8C, 4, 5, 1, 0},
-	{0x1C, 7, 6, 1, 1},
-	{0x8C, 0, 5, 1, 4},
-	{0x89, 1, 6, 1, 2},
-	{0x98, 5, 0, 1, 1},
-	{0x94, 6, 5, 1, 3},
-	{0x92, 0, 6, 1, 2},
-	{0x98, 1, 5, 1, 0},
-	{0x98, 6, 2, 1, 1},
-	{0xD0, 0, 5, 1, 3},
-	{0x94, 6, 0, 1, 1},
-	{0x8C, 5, 6, 1, 4},
-	{0x8C, 1, 5, 1, 0},
-	{0x1C, 6, 7, 1, 1},
-	{0x1C, 5, 6, 1, 0},
-	{0xB0, 2, 3, 1, 1},
-	{0xC4, 4, 5, 1, 0},
-	{0x8C, 6, 4, 1, 1},
-	{0xA4, 3, 6, 1, 0},
-	{0x1C, 5, 7, 1, 1},
-	{0x4C, 0, 5, 1, 4},
-	{0x8C, 6, 0, 1, 1},
-	{0x34, 7, 6, 1, 3},
-	{0x94, 5, 0, 1, 1},
-	{0x98, 6, 5, 1, 2},
+/* Scheduler configuration for dispatching packet on PPE queues, which
+ * is different per SoC.
+ * PPE scheduler configuration for QM includes multiple entries, each entry
+ * contains ports to be dispatched for enqueueing and dequeueing, and the
+ * backup port for dequeueding is supported to be used when the primary port
+ * for dequeueing is down.
+ */
+static const struct ppe_scheduler_qm_config ipq9574_ppe_sch_qm_config[] = {
+	{0x98, 6, 0, true, 1},
+	{0x94, 5, 6, true, 3},
+	{0x86, 0, 5, true, 4},
+	{0x8C, 1, 6, true, 0},
+	{0x1C, 7, 5, true, 1},
+	{0x98, 2, 6, true, 0},
+	{0x1C, 5, 7, true, 1},
+	{0x34, 3, 6, true, 0},
+	{0x8C, 4, 5, true, 1},
+	{0x98, 2, 6, true, 0},
+	{0x8C, 5, 4, true, 1},
+	{0xA8, 0, 6, true, 2},
+	{0x98, 5, 1, true, 0},
+	{0x98, 6, 5, true, 2},
+	{0x89, 1, 6, true, 4},
+	{0xA4, 3, 0, true, 1},
+	{0x8C, 5, 6, true, 4},
+	{0xA8, 0, 2, true, 1},
+	{0x98, 6, 5, true, 0},
+	{0xC4, 4, 3, true, 1},
+	{0x94, 6, 5, true, 0},
+	{0x1C, 7, 6, true, 1},
+	{0x98, 2, 5, true, 0},
+	{0x1C, 6, 7, true, 1},
+	{0x1C, 5, 6, true, 0},
+	{0x94, 3, 5, true, 1},
+	{0x8C, 4, 6, true, 0},
+	{0x94, 1, 5, true, 3},
+	{0x94, 6, 1, true, 0},
+	{0xD0, 3, 5, true, 2},
+	{0x98, 6, 0, true, 1},
+	{0x94, 5, 6, true, 3},
+	{0x94, 1, 5, true, 0},
+	{0x98, 2, 6, true, 1},
+	{0x8C, 4, 5, true, 0},
+	{0x1C, 7, 6, true, 1},
+	{0x8C, 0, 5, true, 4},
+	{0x89, 1, 6, true, 2},
+	{0x98, 5, 0, true, 1},
+	{0x94, 6, 5, true, 3},
+	{0x92, 0, 6, true, 2},
+	{0x98, 1, 5, true, 0},
+	{0x98, 6, 2, true, 1},
+	{0xD0, 0, 5, true, 3},
+	{0x94, 6, 0, true, 1},
+	{0x8C, 5, 6, true, 4},
+	{0x8C, 1, 5, true, 0},
+	{0x1C, 6, 7, true, 1},
+	{0x1C, 5, 6, true, 0},
+	{0xB0, 2, 3, true, 1},
+	{0xC4, 4, 5, true, 0},
+	{0x8C, 6, 4, true, 1},
+	{0xA4, 3, 6, true, 0},
+	{0x1C, 5, 7, true, 1},
+	{0x4C, 0, 5, true, 4},
+	{0x8C, 6, 0, true, 1},
+	{0x34, 7, 6, true, 3},
+	{0x94, 5, 0, true, 1},
+	{0x98, 6, 5, true, 2},
 };
 
-static struct ppe_port_schedule_config ppe_qos_schedule_config[] = {
+static const struct ppe_scheduler_port_config ppe_port_sch_config[] = {
 	{
 		.port		= 0,
 		.flow_level	= true,
@@ -677,14 +734,14 @@ static struct ppe_port_schedule_config ppe_qos_schedule_config[] = {
 	},
 };
 
-/* The QoS resource is applied to each PPE port, there are some
- * resource reserved as the last one.
+/* The scheduler resource is applied to each PPE port, The resource
+ * includes the unicast & multicast queues, flow nodes and DRR nodes.
  */
-static struct ppe_port_schedule_resource ppe_scheduler_res[] = {
+static const struct ppe_port_schedule_resource ppe_scheduler_res[] = {
 	{	.ucastq_start	= 0,
 		.ucastq_end	= 63,
 		.mcastq_start	= 256,
-		.ucastq_end	= 271,
+		.mcastq_end	= 271,
 		.flow_id_start	= 0,
 		.flow_id_end	= 0,
 		.l0node_start	= 0,
@@ -695,7 +752,7 @@ static struct ppe_port_schedule_resource ppe_scheduler_res[] = {
 	{	.ucastq_start	= 144,
 		.ucastq_end	= 159,
 		.mcastq_start	= 272,
-		.ucastq_end	= 275,
+		.mcastq_end	= 275,
 		.flow_id_start	= 36,
 		.flow_id_end	= 39,
 		.l0node_start	= 48,
@@ -706,7 +763,7 @@ static struct ppe_port_schedule_resource ppe_scheduler_res[] = {
 	{	.ucastq_start	= 160,
 		.ucastq_end	= 175,
 		.mcastq_start	= 276,
-		.ucastq_end	= 279,
+		.mcastq_end	= 279,
 		.flow_id_start	= 40,
 		.flow_id_end	= 43,
 		.l0node_start	= 64,
@@ -717,7 +774,7 @@ static struct ppe_port_schedule_resource ppe_scheduler_res[] = {
 	{	.ucastq_start	= 176,
 		.ucastq_end	= 191,
 		.mcastq_start	= 280,
-		.ucastq_end	= 283,
+		.mcastq_end	= 283,
 		.flow_id_start	= 44,
 		.flow_id_end	= 47,
 		.l0node_start	= 80,
@@ -728,7 +785,7 @@ static struct ppe_port_schedule_resource ppe_scheduler_res[] = {
 	{	.ucastq_start	= 192,
 		.ucastq_end	= 207,
 		.mcastq_start	= 284,
-		.ucastq_end	= 287,
+		.mcastq_end	= 287,
 		.flow_id_start	= 48,
 		.flow_id_end	= 51,
 		.l0node_start	= 96,
@@ -739,7 +796,7 @@ static struct ppe_port_schedule_resource ppe_scheduler_res[] = {
 	{	.ucastq_start	= 208,
 		.ucastq_end	= 223,
 		.mcastq_start	= 288,
-		.ucastq_end	= 291,
+		.mcastq_end	= 291,
 		.flow_id_start	= 52,
 		.flow_id_end	= 55,
 		.l0node_start	= 112,
@@ -750,7 +807,7 @@ static struct ppe_port_schedule_resource ppe_scheduler_res[] = {
 	{	.ucastq_start	= 224,
 		.ucastq_end	= 239,
 		.mcastq_start	= 292,
-		.ucastq_end	= 295,
+		.mcastq_end	= 295,
 		.flow_id_start	= 56,
 		.flow_id_end	= 59,
 		.l0node_start	= 128,
@@ -761,7 +818,7 @@ static struct ppe_port_schedule_resource ppe_scheduler_res[] = {
 	{	.ucastq_start	= 240,
 		.ucastq_end	= 255,
 		.mcastq_start	= 296,
-		.ucastq_end	= 299,
+		.mcastq_end	= 299,
 		.flow_id_start	= 60,
 		.flow_id_end	= 63,
 		.l0node_start	= 144,
@@ -772,7 +829,7 @@ static struct ppe_port_schedule_resource ppe_scheduler_res[] = {
 	{	.ucastq_start	= 64,
 		.ucastq_end	= 143,
 		.mcastq_start	= 0,
-		.ucastq_end	= 0,
+		.mcastq_end	= 0,
 		.flow_id_start	= 1,
 		.flow_id_end	= 35,
 		.l0node_start	= 8,
@@ -782,10 +839,10 @@ static struct ppe_port_schedule_resource ppe_scheduler_res[] = {
 	},
 };
 
-/* Set the first level scheduler configuration. */
+/* Set the PPE queue level scheduler configuration. */
 static int ppe_scheduler_l0_queue_map_set(struct ppe_device *ppe_dev,
 					  int node_id, int port,
-					  struct ppe_qos_scheduler_cfg scheduler_cfg)
+					  struct ppe_scheduler_cfg scheduler_cfg)
 {
 	u32 val, reg;
 	int ret;
@@ -805,7 +862,7 @@ static int ppe_scheduler_l0_queue_map_set(struct ppe_device *ppe_dev,
 	      (scheduler_cfg.flow_id * PPE_QUEUE_SCH_PRI_NUM + scheduler_cfg.pri) *
 	      PPE_L0_C_FLOW_CFG_TBL_INC;
 	val = FIELD_PREP(PPE_L0_C_FLOW_CFG_TBL_NODE_ID, scheduler_cfg.drr_node_id);
-	val |= FIELD_PREP(PPE_L0_C_FLOW_CFG_TBL_NODE_CREDIT_UNIT, scheduler_cfg.node_unit);
+	val |= FIELD_PREP(PPE_L0_C_FLOW_CFG_TBL_NODE_CREDIT_UNIT, scheduler_cfg.unit_is_packet);
 
 	ret = regmap_write(ppe_dev->regmap, reg, val);
 	if (ret)
@@ -815,7 +872,7 @@ static int ppe_scheduler_l0_queue_map_set(struct ppe_device *ppe_dev,
 	      (scheduler_cfg.flow_id * PPE_QUEUE_SCH_PRI_NUM + scheduler_cfg.pri) *
 	      PPE_L0_E_FLOW_CFG_TBL_INC;
 	val = FIELD_PREP(PPE_L0_E_FLOW_CFG_TBL_NODE_ID, scheduler_cfg.drr_node_id);
-	val |= FIELD_PREP(PPE_L0_E_FLOW_CFG_TBL_NODE_CREDIT_UNIT, scheduler_cfg.node_unit);
+	val |= FIELD_PREP(PPE_L0_E_FLOW_CFG_TBL_NODE_CREDIT_UNIT, scheduler_cfg.unit_is_packet);
 
 	ret = regmap_write(ppe_dev->regmap, reg, val);
 	if (ret)
@@ -829,17 +886,17 @@ static int ppe_scheduler_l0_queue_map_set(struct ppe_device *ppe_dev,
 		return ret;
 
 	reg = PPE_L0_COMP_CFG_TBL_ADDR + node_id * PPE_L0_COMP_CFG_TBL_INC;
-	val = FIELD_PREP(PPE_L0_COMP_CFG_TBL_NODE_METER_LEN, scheduler_cfg.node_frame_mode);
+	val = FIELD_PREP(PPE_L0_COMP_CFG_TBL_NODE_METER_LEN, scheduler_cfg.frame_mode);
 
 	return regmap_update_bits(ppe_dev->regmap, reg,
 				  PPE_L0_COMP_CFG_TBL_NODE_METER_LEN,
 				  val);
 }
 
-/* Get the first level scheduler configuration. */
+/* Get the PPE queue level scheduler configuration. */
 static int ppe_scheduler_l0_queue_map_get(struct ppe_device *ppe_dev,
 					  int node_id, int *port,
-					  struct ppe_qos_scheduler_cfg *scheduler_cfg)
+					  struct ppe_scheduler_cfg *scheduler_cfg)
 {
 	u32 val, reg;
 	int ret;
@@ -862,7 +919,7 @@ static int ppe_scheduler_l0_queue_map_get(struct ppe_device *ppe_dev,
 		return ret;
 
 	scheduler_cfg->drr_node_id = FIELD_GET(PPE_L0_C_FLOW_CFG_TBL_NODE_ID, val);
-	scheduler_cfg->node_unit = FIELD_GET(PPE_L0_C_FLOW_CFG_TBL_NODE_CREDIT_UNIT, val);
+	scheduler_cfg->unit_is_packet = FIELD_GET(PPE_L0_C_FLOW_CFG_TBL_NODE_CREDIT_UNIT, val);
 
 	reg = PPE_L0_FLOW_PORT_MAP_TBL_ADDR + node_id * PPE_L0_FLOW_PORT_MAP_TBL_INC;
 	ret = regmap_read(ppe_dev->regmap, reg, &val);
@@ -876,15 +933,15 @@ static int ppe_scheduler_l0_queue_map_get(struct ppe_device *ppe_dev,
 	if (ret)
 		return ret;
 
-	scheduler_cfg->node_frame_mode = FIELD_GET(PPE_L0_COMP_CFG_TBL_NODE_METER_LEN, val);
+	scheduler_cfg->frame_mode = FIELD_GET(PPE_L0_COMP_CFG_TBL_NODE_METER_LEN, val);
 
 	return 0;
 }
 
-/* Set the second level scheduler configuration. */
+/* Set the PPE flow level scheduler configuration. */
 static int ppe_scheduler_l1_queue_map_set(struct ppe_device *ppe_dev,
 					  int node_id, int port,
-					  struct ppe_qos_scheduler_cfg scheduler_cfg)
+					  struct ppe_scheduler_cfg scheduler_cfg)
 {
 	u32 val, reg;
 	int ret;
@@ -901,7 +958,7 @@ static int ppe_scheduler_l1_queue_map_set(struct ppe_device *ppe_dev,
 		return ret;
 
 	val = FIELD_PREP(PPE_L1_C_FLOW_CFG_TBL_NODE_ID, scheduler_cfg.drr_node_id);
-	val |= FIELD_PREP(PPE_L1_C_FLOW_CFG_TBL_NODE_CREDIT_UNIT, scheduler_cfg.node_unit);
+	val |= FIELD_PREP(PPE_L1_C_FLOW_CFG_TBL_NODE_CREDIT_UNIT, scheduler_cfg.unit_is_packet);
 	reg = PPE_L1_C_FLOW_CFG_TBL_ADDR +
 	      (scheduler_cfg.flow_id * PPE_QUEUE_SCH_PRI_NUM + scheduler_cfg.pri) *
 	      PPE_L1_C_FLOW_CFG_TBL_INC;
@@ -911,7 +968,7 @@ static int ppe_scheduler_l1_queue_map_set(struct ppe_device *ppe_dev,
 		return ret;
 
 	val = FIELD_PREP(PPE_L1_E_FLOW_CFG_TBL_NODE_ID, scheduler_cfg.drr_node_id);
-	val |= FIELD_PREP(PPE_L1_E_FLOW_CFG_TBL_NODE_CREDIT_UNIT, scheduler_cfg.node_unit);
+	val |= FIELD_PREP(PPE_L1_E_FLOW_CFG_TBL_NODE_CREDIT_UNIT, scheduler_cfg.unit_is_packet);
 	reg = PPE_L1_E_FLOW_CFG_TBL_ADDR +
 		(scheduler_cfg.flow_id * PPE_QUEUE_SCH_PRI_NUM + scheduler_cfg.pri) *
 		PPE_L1_E_FLOW_CFG_TBL_INC;
@@ -928,15 +985,15 @@ static int ppe_scheduler_l1_queue_map_set(struct ppe_device *ppe_dev,
 		return ret;
 
 	reg = PPE_L1_COMP_CFG_TBL_ADDR + node_id * PPE_L1_COMP_CFG_TBL_INC;
-	val = FIELD_PREP(PPE_L1_COMP_CFG_TBL_NODE_METER_LEN, scheduler_cfg.node_frame_mode);
+	val = FIELD_PREP(PPE_L1_COMP_CFG_TBL_NODE_METER_LEN, scheduler_cfg.frame_mode);
 
 	return regmap_update_bits(ppe_dev->regmap, reg, PPE_L1_COMP_CFG_TBL_NODE_METER_LEN, val);
 }
 
-/* Get the second level scheduler configuration. */
+/* Get the PPE flow level scheduler configuration. */
 static int ppe_scheduler_l1_queue_map_get(struct ppe_device *ppe_dev,
 					  int node_id, int *port,
-					  struct ppe_qos_scheduler_cfg *scheduler_cfg)
+					  struct ppe_scheduler_cfg *scheduler_cfg)
 {
 	u32 val, reg;
 	int ret;
@@ -958,7 +1015,7 @@ static int ppe_scheduler_l1_queue_map_get(struct ppe_device *ppe_dev,
 		return ret;
 
 	scheduler_cfg->drr_node_id = FIELD_GET(PPE_L1_C_FLOW_CFG_TBL_NODE_ID, val);
-	scheduler_cfg->node_unit = FIELD_GET(PPE_L1_C_FLOW_CFG_TBL_NODE_CREDIT_UNIT, val);
+	scheduler_cfg->unit_is_packet = FIELD_GET(PPE_L1_C_FLOW_CFG_TBL_NODE_CREDIT_UNIT, val);
 
 	reg = PPE_L1_FLOW_PORT_MAP_TBL_ADDR + node_id * PPE_L1_FLOW_PORT_MAP_TBL_INC;
 	ret = regmap_read(ppe_dev->regmap, reg, &val);
@@ -972,27 +1029,27 @@ static int ppe_scheduler_l1_queue_map_get(struct ppe_device *ppe_dev,
 	if (ret)
 		return ret;
 
-	scheduler_cfg->node_frame_mode = FIELD_GET(PPE_L1_COMP_CFG_TBL_NODE_METER_LEN, val);
+	scheduler_cfg->frame_mode = FIELD_GET(PPE_L1_COMP_CFG_TBL_NODE_METER_LEN, val);
 
 	return 0;
 }
 
 /**
- * ppe_queue_scheduler_set - set QoS scheduler of PPE hardware queue
+ * ppe_queue_scheduler_set - Configure scheduler for PPE hardware queue
  * @ppe_dev: PPE device
- * @node_id: PPE node ID
+ * @node_id: PPE queue ID or flow ID
  * @flow_level: Flow level scheduler or queue level scheduler
  * @port: PPE port ID set scheduler configuration
- * @scheduler_cfg: QoS scheduler configuration
+ * @scheduler_cfg: PPE scheduler configuration
  *
- * The hardware QoS function is supported by PPE, which is based on
- * PPE hardware queue scheduler of PPE port.
+ * PPE scheduler configuration supports queue level and flow level on
+ * the PPE egress port.
  *
- * Return 0 on success, negative error code on failure.
+ * Return: 0 on success, negative error code on failure.
  */
 int ppe_queue_scheduler_set(struct ppe_device *ppe_dev,
 			    int node_id, bool flow_level, int port,
-			    struct ppe_qos_scheduler_cfg scheduler_cfg)
+			    struct ppe_scheduler_cfg scheduler_cfg)
 {
 	if (flow_level)
 		return ppe_scheduler_l1_queue_map_set(ppe_dev, node_id,
@@ -1003,41 +1060,17 @@ int ppe_queue_scheduler_set(struct ppe_device *ppe_dev,
 }
 
 /**
- * ppe_queue_scheduler_get - get QoS scheduler of PPE hardware queue
- * @ppe_dev: PPE device
- * @node_id: PPE node ID
- * @flow_level: Flow level scheduler or queue level scheduler
- * @port: PPE port ID to get scheduler config
- * @scheduler_cfg: QoS scheduler configuration
- *
- * The hardware QoS function is supported by PPE, the current scheduler
- * configuration can be acquired based on the queue ID of PPE port.
- *
- * Return 0 on success, negative error code on failure.
- */
-int ppe_queue_scheduler_get(struct ppe_device *ppe_dev,
-			    int node_id, bool flow_level, int *port,
-			    struct ppe_qos_scheduler_cfg *scheduler_cfg)
-{
-	if (flow_level)
-		return ppe_scheduler_l1_queue_map_get(ppe_dev, node_id,
-						      port, scheduler_cfg);
-
-	return ppe_scheduler_l0_queue_map_get(ppe_dev, node_id,
-					      port, scheduler_cfg);
-}
-
-/**
- * ppe_queue_ucast_base_set - Set PPE unicast queue base ID
+ * ppe_queue_ucast_base_set - Set PPE unicast queue base ID and profile ID
  * @ppe_dev: PPE device
  * @queue_dst: PPE queue destination configuration
  * @queue_base: PPE queue base ID
  * @profile_id: Profile ID
  *
- * The PPE unicast queue base ID is configured based on the destination
- * port information per profile ID.
+ * The PPE unicast queue base ID and profile ID are configured based on the
+ * destination port information that can be service code or CPU code or the
+ * destination port.
  *
- * Return 0 on success, negative error code on failure.
+ * Return: 0 on success, negative error code on failure.
  */
 int ppe_queue_ucast_base_set(struct ppe_device *ppe_dev,
 			     struct ppe_queue_ucast_dest queue_dst,
@@ -1064,53 +1097,53 @@ int ppe_queue_ucast_base_set(struct ppe_device *ppe_dev,
 }
 
 /**
- * ppe_queue_ucast_pri_class_set - Set PPE unicast queue class of priority
+ * ppe_queue_ucast_offset_pri_set - Set PPE unicast queue offset based on priority
  * @ppe_dev: PPE device
  * @profile_id: Profile ID
- * @priority: Priority to be used to set class
- * @class_offset: Class value for the destination queue ID
+ * @priority: PPE internal priority to be used to set queue offset
+ * @queue_offset: Queue offset used for calculating the destination queue ID
  *
- * The PPE unicast queue class is configured based on the PPE
+ * The PPE unicast queue offset is configured based on the PPE
  * internal priority.
  *
- * Return 0 on success, negative error code on failure.
+ * Return: 0 on success, negative error code on failure.
  */
-int ppe_queue_ucast_pri_class_set(struct ppe_device *ppe_dev,
-				  int profile_id,
-				  int priority,
-				  int class_offset)
+int ppe_queue_ucast_offset_pri_set(struct ppe_device *ppe_dev,
+				   int profile_id,
+				   int priority,
+				   int queue_offset)
 {
 	u32 val, reg;
 	int index;
 
 	index = (profile_id << 4) + priority;
-	val = FIELD_PREP(PPE_UCAST_PRIORITY_MAP_TBL_CLASS, class_offset);
+	val = FIELD_PREP(PPE_UCAST_PRIORITY_MAP_TBL_CLASS, queue_offset);
 	reg = PPE_UCAST_PRIORITY_MAP_TBL_ADDR + index * PPE_UCAST_PRIORITY_MAP_TBL_INC;
 
 	return regmap_write(ppe_dev->regmap, reg, val);
 }
 
 /**
- * ppe_queue_ucast_hash_class_set - Set PPE unicast queue class of hash value
+ * ppe_queue_ucast_offset_hash_set - Set PPE unicast queue offset based on hash
  * @ppe_dev: PPE device
  * @profile_id: Profile ID
- * @rss_hash: Hash value to be used to set clas
- * @class_offset: Class value for the destination queue ID
+ * @rss_hash: Packet hash value to be used to set queue offset
+ * @queue_offset: Queue offset used for calculating the destination queue ID
  *
- * The PPE unicast queue class is configured based on the RSS hash value.
+ * The PPE unicast queue offset is configured based on the RSS hash value.
  *
- * Return 0 on success, negative error code on failure.
+ * Return: 0 on success, negative error code on failure.
  */
-int ppe_queue_ucast_hash_class_set(struct ppe_device *ppe_dev,
-				   int profile_id,
-				   int rss_hash,
-				   int class_offset)
+int ppe_queue_ucast_offset_hash_set(struct ppe_device *ppe_dev,
+				    int profile_id,
+				    int rss_hash,
+				    int queue_offset)
 {
 	u32 val, reg;
 	int index;
 
 	index = (profile_id << 8) + rss_hash;
-	val = FIELD_PREP(PPE_UCAST_HASH_MAP_TBL_HASH, class_offset);
+	val = FIELD_PREP(PPE_UCAST_HASH_MAP_TBL_HASH, queue_offset);
 	reg = PPE_UCAST_HASH_MAP_TBL_ADDR + index * PPE_UCAST_HASH_MAP_TBL_INC;
 
 	return regmap_write(ppe_dev->regmap, reg, val);
@@ -1121,14 +1154,15 @@ int ppe_queue_ucast_hash_class_set(struct ppe_device *ppe_dev,
  * @ppe_dev: PPE device
  * @port: PPE port
  * @type: Resource type
- * @res_start: Resource start ID
- * @res_end: Resource end ID
+ * @res_start: Resource start ID returned
+ * @res_end: Resource end ID returned
  *
  * PPE resource is assigned per PPE port, which is acquired for QoS scheduler.
  *
- * Return 0 on success, negative error code on failure.
+ * Return: 0 on success, negative error code on failure.
  */
-int ppe_port_resource_get(struct ppe_device *ppe_dev, int port, int type,
+int ppe_port_resource_get(struct ppe_device *ppe_dev, int port,
+			  enum ppe_resource_type type,
 			  int *res_start, int *res_end)
 {
 	struct ppe_port_schedule_resource res;
@@ -1169,18 +1203,17 @@ int ppe_port_resource_get(struct ppe_device *ppe_dev, int port, int type,
 }
 
 /**
- * ppe_servcode_config_set - Set PPE service code configuration
+ * ppe_sc_config_set - Set PPE service code configuration
  * @ppe_dev: PPE device
- * @servcode: Service ID, 0-255 supported by PPE
+ * @sc: Service ID, 0-255 supported by PPE
  * @cfg: Service code configuration
  *
- * The service code configuration of PPE is used to handle the PPE
- * functions.
+ * PPE service code is used by the PPE during its packet processing stages,
+ * to perform or bypass certain selected packet operations on the packet.
  *
- * Return 0 on success, negative error code on failure.
+ * Return: 0 on success, negative error code on failure.
  */
-int ppe_servcode_config_set(struct ppe_device *ppe_dev, int servcode,
-			    struct ppe_servcode_cfg cfg)
+int ppe_sc_config_set(struct ppe_device *ppe_dev, int sc, struct ppe_sc_cfg cfg)
 {
 	u32 val, reg, servcode_val[2] = {};
 	unsigned long bitmap_value;
@@ -1196,7 +1229,7 @@ int ppe_servcode_config_set(struct ppe_device *ppe_dev, int servcode,
 			  test_bit(PPE_SC_BYPASS_COUNTER_RX, cfg.bitmaps.counter));
 	val |= FIELD_PREP(PPE_IN_L2_SERVICE_TBL_TX_CNT_EN,
 			  test_bit(PPE_SC_BYPASS_COUNTER_TX, cfg.bitmaps.counter));
-	reg = PPE_IN_L2_SERVICE_TBL_ADDR + PPE_IN_L2_SERVICE_TBL_INC * servcode;
+	reg = PPE_IN_L2_SERVICE_TBL_ADDR + PPE_IN_L2_SERVICE_TBL_INC * sc;
 
 	ret = regmap_write(ppe_dev->regmap, reg, val);
 	if (ret)
@@ -1206,23 +1239,23 @@ int ppe_servcode_config_set(struct ppe_device *ppe_dev, int servcode,
 	PPE_SERVICE_SET_BYPASS_BITMAP(servcode_val, bitmap_value);
 	PPE_SERVICE_SET_RX_CNT_EN(servcode_val,
 				  test_bit(PPE_SC_BYPASS_COUNTER_RX_VLAN, cfg.bitmaps.counter));
-	reg = PPE_SERVICE_TBL_ADDR + PPE_SERVICE_TBL_INC * servcode;
+	reg = PPE_SERVICE_TBL_ADDR + PPE_SERVICE_TBL_INC * sc;
 
 	ret = regmap_bulk_write(ppe_dev->regmap, reg,
 				servcode_val, ARRAY_SIZE(servcode_val));
 	if (ret)
 		return ret;
 
-	reg = PPE_EG_SERVICE_TBL_ADDR + PPE_EG_SERVICE_TBL_INC * servcode;
+	reg = PPE_EG_SERVICE_TBL_ADDR + PPE_EG_SERVICE_TBL_INC * sc;
 	ret = regmap_bulk_read(ppe_dev->regmap, reg,
 			       servcode_val, ARRAY_SIZE(servcode_val));
 	if (ret)
 		return ret;
 
-	PPE_EG_SERVICE_SET_UPDATE_ACTION(servcode_val, cfg.field_update_bitmap);
 	PPE_EG_SERVICE_SET_NEXT_SERVCODE(servcode_val, cfg.next_service_code);
-	PPE_EG_SERVICE_SET_HW_SERVICE(servcode_val, cfg.hw_service);
-	PPE_EG_SERVICE_SET_OFFSET_SEL(servcode_val, cfg.offset_sel);
+	PPE_EG_SERVICE_SET_UPDATE_ACTION(servcode_val, cfg.eip_field_update_bitmap);
+	PPE_EG_SERVICE_SET_HW_SERVICE(servcode_val, cfg.eip_hw_service);
+	PPE_EG_SERVICE_SET_OFFSET_SEL(servcode_val, cfg.eip_offset_sel);
 	PPE_EG_SERVICE_SET_TX_CNT_EN(servcode_val,
 				     test_bit(PPE_SC_BYPASS_COUNTER_TX_VLAN, cfg.bitmaps.counter));
 
@@ -1233,24 +1266,24 @@ int ppe_servcode_config_set(struct ppe_device *ppe_dev, int servcode,
 
 	bitmap_value = bitmap_read(cfg.bitmaps.tunnel, 0, PPE_SC_BYPASS_TUNNEL_SIZE);
 	val = FIELD_PREP(PPE_TL_SERVICE_TBL_BYPASS_BITMAP, bitmap_value);
-	reg = PPE_TL_SERVICE_TBL_ADDR + PPE_TL_SERVICE_TBL_INC * servcode;
+	reg = PPE_TL_SERVICE_TBL_ADDR + PPE_TL_SERVICE_TBL_INC * sc;
 
 	return regmap_write(ppe_dev->regmap, reg, val);
 }
 
 /**
- * ppe_counter_set - Set PPE port counter enabled or not
+ * ppe_counter_enable_set - Set PPE port counter enabled
  * @ppe_dev: PPE device
  * @port: PPE port ID
- * @enable: Counter status
  *
- * PPE port counter is optionally configured as enabled or not.
+ * Enable PPE counters on the given port for the unicast packet, multicast
+ * packet and VLAN packet received and transmitted by PPE.
  *
- * Return 0 on success, negative error code on failure.
+ * Return: 0 on success, negative error code on failure.
  */
-int ppe_counter_set(struct ppe_device *ppe_dev, int port, bool enable)
+int ppe_counter_enable_set(struct ppe_device *ppe_dev, int port)
 {
-	u32 reg, val, mru_mtu_val[3];
+	u32 reg, mru_mtu_val[3];
 	int ret;
 
 	reg = PPE_MRU_MTU_CTRL_TBL_ADDR + PPE_MRU_MTU_CTRL_TBL_INC * port;
@@ -1259,27 +1292,21 @@ int ppe_counter_set(struct ppe_device *ppe_dev, int port, bool enable)
 	if (ret)
 		return ret;
 
-	PPE_MRU_MTU_CTRL_SET_RX_CNT_EN(mru_mtu_val, enable);
-	PPE_MRU_MTU_CTRL_SET_TX_CNT_EN(mru_mtu_val, enable);
+	PPE_MRU_MTU_CTRL_SET_RX_CNT_EN(mru_mtu_val, true);
+	PPE_MRU_MTU_CTRL_SET_TX_CNT_EN(mru_mtu_val, true);
 	ret = regmap_bulk_write(ppe_dev->regmap, reg,
 				mru_mtu_val, ARRAY_SIZE(mru_mtu_val));
 	if (ret)
 		return ret;
 
 	reg = PPE_MC_MTU_CTRL_TBL_ADDR + PPE_MC_MTU_CTRL_TBL_INC * port;
-	val = FIELD_PREP(PPE_MC_MTU_CTRL_TBL_TX_CNT_EN, enable);
-	ret = regmap_update_bits(ppe_dev->regmap, reg,
-				 PPE_MC_MTU_CTRL_TBL_TX_CNT_EN,
-				 val);
+	ret = regmap_set_bits(ppe_dev->regmap, reg, PPE_MC_MTU_CTRL_TBL_TX_CNT_EN);
 	if (ret)
 		return ret;
 
-	reg = PPE_PORT_EG_VLAN_ADDR + PPE_PORT_EG_VLAN_INC * port;
-	val = FIELD_PREP(PPE_PORT_EG_VLAN_TX_COUNTING_EN, enable);
+	reg = PPE_PORT_EG_VLAN_TBL_ADDR + PPE_PORT_EG_VLAN_TBL_INC * port;
 
-	return regmap_update_bits(ppe_dev->regmap, reg,
-				  PPE_PORT_EG_VLAN_TX_COUNTING_EN,
-				  val);
+	return regmap_set_bits(ppe_dev->regmap, reg, PPE_PORT_EG_VLAN_TBL_TX_COUNTING_EN);
 }
 
 static int ppe_rss_hash_ipv4_config(struct ppe_device *ppe_dev, int index,
@@ -1343,14 +1370,14 @@ static int ppe_rss_hash_ipv6_config(struct ppe_device *ppe_dev, int index,
 }
 
 /**
- * ppe_rss_hash_config_set - Set PPE RSS hash seed
- * @ppe_dev: PPE device
- * @mode: Packet format mode
- * @hash_cfg: RSS hash configuration
+ * ppe_rss_hash_config_set - Configure the PPE hash settings for the packet received.
+ * @ppe_dev: PPE device.
+ * @mode: Configure RSS hash for the packet type IPv4 and IPv6.
+ * @cfg: RSS hash configuration.
  *
- * PPE RSS hash seed is configured based on the packet format.
+ * PPE RSS hash settings are configured for the packet type IPv4 and IPv6.
  *
- * Return 0 on success, negative error code on failure.
+ * Return: 0 on success, negative error code on failure.
  */
 int ppe_rss_hash_config_set(struct ppe_device *ppe_dev, int mode,
 			    struct ppe_rss_hash_cfg cfg)
@@ -1370,13 +1397,13 @@ int ppe_rss_hash_config_set(struct ppe_device *ppe_dev, int mode,
 		if (ret)
 			return ret;
 
-		for (i = 0; i < PPE_RSS_HASH_MIX_IPV4_NUM; i++) {
+		for (i = 0; i < PPE_RSS_HASH_MIX_IPV4_ENTRIES; i++) {
 			ret = ppe_rss_hash_ipv4_config(ppe_dev, i, cfg);
 			if (ret)
 				return ret;
 		}
 
-		for (i = 0; i < PPE_RSS_HASH_FIN_IPV4_NUM; i++) {
+		for (i = 0; i < PPE_RSS_HASH_FIN_IPV4_ENTRIES; i++) {
 			val = FIELD_PREP(PPE_RSS_HASH_FIN_IPV4_INNER, cfg.hash_fin_inner[i]);
 			val |= FIELD_PREP(PPE_RSS_HASH_FIN_IPV4_OUTER, cfg.hash_fin_outer[i]);
 			reg = PPE_RSS_HASH_FIN_IPV4_ADDR + i * PPE_RSS_HASH_FIN_IPV4_INC;
@@ -1399,13 +1426,13 @@ int ppe_rss_hash_config_set(struct ppe_device *ppe_dev, int mode,
 		if (ret)
 			return ret;
 
-		for (i = 0; i < PPE_RSS_HASH_MIX_NUM; i++) {
+		for (i = 0; i < PPE_RSS_HASH_MIX_ENTRIES; i++) {
 			ret = ppe_rss_hash_ipv6_config(ppe_dev, i, cfg);
 			if (ret)
 				return ret;
 		}
 
-		for (i = 0; i < PPE_RSS_HASH_FIN_NUM; i++) {
+		for (i = 0; i < PPE_RSS_HASH_FIN_ENTRIES; i++) {
 			val = FIELD_PREP(PPE_RSS_HASH_FIN_INNER, cfg.hash_fin_inner[i]);
 			val |= FIELD_PREP(PPE_RSS_HASH_FIN_OUTER, cfg.hash_fin_outer[i]);
 			reg = PPE_RSS_HASH_FIN_ADDR + i * PPE_RSS_HASH_FIN_INC;
@@ -1420,18 +1447,18 @@ int ppe_rss_hash_config_set(struct ppe_device *ppe_dev, int mode,
 }
 
 /**
- * ppe_ring_queue_map_set - Set PPE queue mapped with EDMA ring
+ * ppe_ring_queue_map_set - Set the PPE queue to Ethernet DMA ring mapping
  * @ppe_dev: PPE device
- * @ring_id: EDMA ring ID
- * @queue_map: Queue bit map
+ * @ring_id: Ethernet DMA ring ID
+ * @queue_map: Bit map of queue IDs to given Ethernet DMA ring
  *
- * PPE queue is configured to use the special Ring.
+ * Configure the mapping from a set of PPE queues to a given Ethernet DMA ring.
  *
- * Return 0 on success, negative error code on failure.
+ * Return: 0 on success, negative error code on failure.
  */
 int ppe_ring_queue_map_set(struct ppe_device *ppe_dev, int ring_id, u32 *queue_map)
 {
-	u32 reg, queue_bitmap_val[PPE_RING_MAPPED_BP_QUEUE_WORD_COUNT];
+	u32 reg, queue_bitmap_val[PPE_RING_TO_QUEUE_BITMAP_WORD_CNT];
 
 	memcpy(queue_bitmap_val, queue_map, sizeof(queue_bitmap_val));
 	reg = PPE_RING_Q_MAP_TBL_ADDR + PPE_RING_Q_MAP_TBL_INC * ring_id;
@@ -1441,13 +1468,44 @@ int ppe_ring_queue_map_set(struct ppe_device *ppe_dev, int ring_id, u32 *queue_m
 				 ARRAY_SIZE(queue_bitmap_val));
 }
 
+/**
+ * ppe_queue_scheduler_get - get scheduler of PPE hardware queue
+ * @ppe_dev: PPE device
+ * @node_id: PPE queue ID or flow ID
+ * @flow_level: Flow level scheduler or queue level scheduler
+ * @port: PPE port ID to get scheduler config
+ * @scheduler_cfg: PPE scheduler configuration
+ *
+ * The current scheduler configuration can be acquired based on the
+ * queue ID of PPE port.
+ *
+ * Return 0 on success, negative error code on failure.
+ */
+int ppe_queue_scheduler_get(struct ppe_device *ppe_dev,
+			    int node_id, bool flow_level, int *port,
+			    struct ppe_scheduler_cfg *scheduler_cfg)
+{
+	if (flow_level)
+		return ppe_scheduler_l1_queue_map_get(ppe_dev, node_id,
+						      port, scheduler_cfg);
+
+	return ppe_scheduler_l0_queue_map_get(ppe_dev, node_id,
+					      port, scheduler_cfg);
+}
+
 static int ppe_config_bm_threshold(struct ppe_device *ppe_dev, int bm_port_id,
-				   struct ppe_bm_port_config port_cfg)
+				   const struct ppe_bm_port_config port_cfg)
 {
 	u32 reg, val, bm_fc_val[2];
 	int ret;
 
-	/* Configure BM flow control related threshold */
+	reg = PPE_BM_PORT_FC_CFG_TBL_ADDR + PPE_BM_PORT_FC_CFG_TBL_INC * bm_port_id;
+	ret = regmap_bulk_read(ppe_dev->regmap, reg,
+			       bm_fc_val, ARRAY_SIZE(bm_fc_val));
+	if (ret)
+		return ret;
+
+	/* Configure BM flow control related threshold. */
 	PPE_BM_PORT_FC_SET_WEIGHT(bm_fc_val, port_cfg.weight);
 	PPE_BM_PORT_FC_SET_RESUME_OFFSET(bm_fc_val, port_cfg.resume_offset);
 	PPE_BM_PORT_FC_SET_RESUME_THRESHOLD(bm_fc_val, port_cfg.resume_ceil);
@@ -1455,19 +1513,18 @@ static int ppe_config_bm_threshold(struct ppe_device *ppe_dev, int bm_port_id,
 	PPE_BM_PORT_FC_SET_REACT_LIMIT(bm_fc_val, port_cfg.in_fly_buf);
 	PPE_BM_PORT_FC_SET_PRE_ALLOC(bm_fc_val, port_cfg.pre_alloc);
 
-	/* Ceiling is divided into the different register word. */
+	/* Configure low/high bits of the ceiling for the BM port. */
 	val = FIELD_GET(GENMASK(2, 0), port_cfg.ceil);
 	PPE_BM_PORT_FC_SET_CEILING_LOW(bm_fc_val, val);
 	val = FIELD_GET(GENMASK(10, 3), port_cfg.ceil);
 	PPE_BM_PORT_FC_SET_CEILING_HIGH(bm_fc_val, val);
 
-	reg = PPE_BM_PORT_FC_CFG_ADDR + PPE_BM_PORT_FC_CFG_INC * bm_port_id;
 	ret = regmap_bulk_write(ppe_dev->regmap, reg,
 				bm_fc_val, ARRAY_SIZE(bm_fc_val));
 	if (ret)
 		return ret;
 
-	/* Assign the default group ID 0 to the BM port */
+	/* Assign the default group ID 0 to the BM port. */
 	val = FIELD_PREP(PPE_BM_PORT_GROUP_ID_SHARED_GROUP_ID, 0);
 	reg = PPE_BM_PORT_GROUP_ID_ADDR + PPE_BM_PORT_GROUP_ID_INC * bm_port_id;
 	ret = regmap_update_bits(ppe_dev->regmap, reg,
@@ -1476,26 +1533,23 @@ static int ppe_config_bm_threshold(struct ppe_device *ppe_dev, int bm_port_id,
 	if (ret)
 		return ret;
 
-	/* Enable BM port flow control */
-	val = FIELD_PREP(PPE_BM_PORT_FC_MODE_EN, true);
+	/* Enable BM port flow control. */
 	reg = PPE_BM_PORT_FC_MODE_ADDR + PPE_BM_PORT_FC_MODE_INC * bm_port_id;
 
-	return regmap_update_bits(ppe_dev->regmap, reg,
-				  PPE_BM_PORT_FC_MODE_EN,
-				  val);
-};
+	return regmap_set_bits(ppe_dev->regmap, reg, PPE_BM_PORT_FC_MODE_EN);
+}
 
 /* Configure the buffer threshold for the port flow control function. */
 static int ppe_config_bm(struct ppe_device *ppe_dev)
 {
+	const struct ppe_bm_port_config *port_cfg;
 	unsigned int i, bm_port_id, port_cfg_cnt;
-	struct ppe_bm_port_config *port_cfg;
 	u32 reg, val;
 	int ret;
 
-	/* Configure the buffer number of group 0 by default. The buffer
-	 * number of group 1-3 is cleared to 0 after PPE reset on the probe
-	 * of PPE driver.
+	/* Configure the allocated buffer number only for group 0.
+	 * The buffer number of group 1-3 is already cleared to 0
+	 * after PPE reset during the probe of PPE driver.
 	 */
 	reg = PPE_BM_SHARED_GROUP_CFG_ADDR;
 	val = FIELD_PREP(PPE_BM_SHARED_GROUP_CFG_SHARED_LIMIT,
@@ -1506,6 +1560,7 @@ static int ppe_config_bm(struct ppe_device *ppe_dev)
 	if (ret)
 		goto bm_config_fail;
 
+	/* Configure buffer thresholds for the BM ports. */
 	port_cfg = ipq9574_ppe_bm_port_config;
 	port_cfg_cnt = ARRAY_SIZE(ipq9574_ppe_bm_port_config);
 	for (i = 0; i < port_cfg_cnt; i++) {
@@ -1530,7 +1585,7 @@ static int ppe_config_bm(struct ppe_device *ppe_dev)
  */
 static int ppe_config_qm(struct ppe_device *ppe_dev)
 {
-	struct ppe_qm_queue_config *queue_cfg;
+	const struct ppe_qm_queue_config *queue_cfg;
 	int ret, i, queue_id, queue_cfg_count;
 	u32 reg, multicast_queue_cfg[5];
 	u32 unicast_queue_cfg[4];
@@ -1555,13 +1610,13 @@ static int ppe_config_qm(struct ppe_device *ppe_dev)
 	for (i = 0; i < queue_cfg_count; i++) {
 		queue_id = queue_cfg[i].queue_start;
 
-		/* Configure threshold for dropping packet from unicast queue
-		 * and multicast queue, which belong to the different queue ID.
+		/* Configure threshold for dropping packets separately for
+		 * unicast and multicast PPE queues.
 		 */
 		while (queue_id <= queue_cfg[i].queue_end) {
-			if (queue_id < PPE_AC_UNI_QUEUE_CFG_TBL_NUM) {
-				reg = PPE_AC_UNI_QUEUE_CFG_TBL_ADDR +
-				      PPE_AC_UNI_QUEUE_CFG_TBL_INC * queue_id;
+			if (queue_id < PPE_AC_UNICAST_QUEUE_CFG_TBL_ENTRIES) {
+				reg = PPE_AC_UNICAST_QUEUE_CFG_TBL_ADDR +
+				      PPE_AC_UNICAST_QUEUE_CFG_TBL_INC * queue_id;
 
 				ret = regmap_bulk_read(ppe_dev->regmap, reg,
 						       unicast_queue_cfg,
@@ -1569,18 +1624,18 @@ static int ppe_config_qm(struct ppe_device *ppe_dev)
 				if (ret)
 					goto qm_config_fail;
 
-				PPE_AC_UNI_QUEUE_SET_EN(unicast_queue_cfg, true);
-				PPE_AC_UNI_QUEUE_SET_GRP_ID(unicast_queue_cfg, 0);
-				PPE_AC_UNI_QUEUE_SET_PRE_LIMIT(unicast_queue_cfg,
-							       queue_cfg[i].prealloc_buf);
-				PPE_AC_UNI_QUEUE_SET_DYNAMIC(unicast_queue_cfg,
-							     queue_cfg[i].dynamic);
-				PPE_AC_UNI_QUEUE_SET_WEIGHT(unicast_queue_cfg,
-							    queue_cfg[i].weight);
-				PPE_AC_UNI_QUEUE_SET_THRESHOLD(unicast_queue_cfg,
-							       queue_cfg[i].ceil);
-				PPE_AC_UNI_QUEUE_SET_GRN_RESUME(unicast_queue_cfg,
-								queue_cfg[i].resume_offset);
+				PPE_AC_UNICAST_QUEUE_SET_EN(unicast_queue_cfg, true);
+				PPE_AC_UNICAST_QUEUE_SET_GRP_ID(unicast_queue_cfg, 0);
+				PPE_AC_UNICAST_QUEUE_SET_PRE_LIMIT(unicast_queue_cfg,
+								   queue_cfg[i].prealloc_buf);
+				PPE_AC_UNICAST_QUEUE_SET_DYNAMIC(unicast_queue_cfg,
+								 queue_cfg[i].dynamic);
+				PPE_AC_UNICAST_QUEUE_SET_WEIGHT(unicast_queue_cfg,
+								queue_cfg[i].weight);
+				PPE_AC_UNICAST_QUEUE_SET_THRESHOLD(unicast_queue_cfg,
+								   queue_cfg[i].ceil);
+				PPE_AC_UNICAST_QUEUE_SET_GRN_RESUME(unicast_queue_cfg,
+								    queue_cfg[i].resume_offset);
 
 				ret = regmap_bulk_write(ppe_dev->regmap, reg,
 							unicast_queue_cfg,
@@ -1588,8 +1643,8 @@ static int ppe_config_qm(struct ppe_device *ppe_dev)
 				if (ret)
 					goto qm_config_fail;
 			} else {
-				reg = PPE_AC_MUL_QUEUE_CFG_TBL_ADDR +
-				      PPE_AC_MUL_QUEUE_CFG_TBL_INC * queue_id;
+				reg = PPE_AC_MULTICAST_QUEUE_CFG_TBL_ADDR +
+				      PPE_AC_MULTICAST_QUEUE_CFG_TBL_INC * queue_id;
 
 				ret = regmap_bulk_read(ppe_dev->regmap, reg,
 						       multicast_queue_cfg,
@@ -1597,14 +1652,14 @@ static int ppe_config_qm(struct ppe_device *ppe_dev)
 				if (ret)
 					goto qm_config_fail;
 
-				PPE_AC_MUL_QUEUE_SET_EN(multicast_queue_cfg, true);
-				PPE_AC_MUL_QUEUE_SET_GRN_GRP_ID(multicast_queue_cfg, 0);
-				PPE_AC_MUL_QUEUE_SET_GRN_PRE_LIMIT(multicast_queue_cfg,
-								   queue_cfg[i].prealloc_buf);
-				PPE_AC_MUL_QUEUE_SET_GRN_THRESHOLD(multicast_queue_cfg,
-								   queue_cfg[i].ceil);
-				PPE_AC_MUL_QUEUE_SET_GRN_RESUME(multicast_queue_cfg,
-								queue_cfg[i].resume_offset);
+				PPE_AC_MULTICAST_QUEUE_SET_EN(multicast_queue_cfg, true);
+				PPE_AC_MULTICAST_QUEUE_SET_GRN_GRP_ID(multicast_queue_cfg, 0);
+				PPE_AC_MULTICAST_QUEUE_SET_GRN_PRE_LIMIT(multicast_queue_cfg,
+									 queue_cfg[i].prealloc_buf);
+				PPE_AC_MULTICAST_QUEUE_SET_GRN_THRESHOLD(multicast_queue_cfg,
+									 queue_cfg[i].ceil);
+				PPE_AC_MULTICAST_QUEUE_SET_GRN_RESUME(multicast_queue_cfg,
+								      queue_cfg[i].resume_offset);
 
 				ret = regmap_bulk_write(ppe_dev->regmap, reg,
 							multicast_queue_cfg,
@@ -1613,19 +1668,17 @@ static int ppe_config_qm(struct ppe_device *ppe_dev)
 					goto qm_config_fail;
 			}
 
-			/* Enable enqueue */
+			/* Enable enqueue. */
 			reg = PPE_ENQ_OPR_TBL_ADDR + PPE_ENQ_OPR_TBL_INC * queue_id;
-			ret = regmap_update_bits(ppe_dev->regmap, reg,
-						 PPE_ENQ_OPR_TBL_ENQ_DISABLE,
-						 FIELD_PREP(PPE_ENQ_OPR_TBL_ENQ_DISABLE, false));
+			ret = regmap_clear_bits(ppe_dev->regmap, reg,
+						PPE_ENQ_OPR_TBL_ENQ_DISABLE);
 			if (ret)
 				goto qm_config_fail;
 
-			/* Enable dequeue */
+			/* Enable dequeue. */
 			reg = PPE_DEQ_OPR_TBL_ADDR + PPE_DEQ_OPR_TBL_INC * queue_id;
-			ret = regmap_update_bits(ppe_dev->regmap, reg,
-						 PPE_DEQ_OPR_TBL_DEQ_DISABLE,
-						 FIELD_PREP(PPE_ENQ_OPR_TBL_ENQ_DISABLE, false));
+			ret = regmap_clear_bits(ppe_dev->regmap, reg,
+						PPE_DEQ_OPR_TBL_DEQ_DISABLE);
 			if (ret)
 				goto qm_config_fail;
 
@@ -1634,9 +1687,8 @@ static int ppe_config_qm(struct ppe_device *ppe_dev)
 	}
 
 	/* Enable queue counter for all PPE hardware queues. */
-	ret = regmap_update_bits(ppe_dev->regmap, PPE_EG_BRIDGE_CONFIG_ADDR,
-				 PPE_EG_BRIDGE_CONFIG_QUEUE_CNT_EN,
-				 PPE_EG_BRIDGE_CONFIG_QUEUE_CNT_EN);
+	ret = regmap_set_bits(ppe_dev->regmap, PPE_EG_BRIDGE_CONFIG_ADDR,
+			      PPE_EG_BRIDGE_CONFIG_QUEUE_CNT_EN);
 	if (ret)
 		goto qm_config_fail;
 
@@ -1648,53 +1700,34 @@ static int ppe_config_qm(struct ppe_device *ppe_dev)
 }
 
 static int ppe_node_scheduler_config(struct ppe_device *ppe_dev,
-				     struct ppe_port_schedule_config config)
+				     const struct ppe_scheduler_port_config config)
 {
-	struct ppe_qos_scheduler_cfg qos_cfg;
+	struct ppe_scheduler_cfg sch_cfg;
 	int ret, i;
 
 	for (i = 0; i < config.loop_num; i++) {
 		if (!config.pri_max) {
 			/* Round robin scheduler without priority. */
-			qos_cfg.flow_id = config.flow_id;
-			qos_cfg.pri = 0;
-			qos_cfg.drr_node_id = config.drr_node_id;
+			sch_cfg.flow_id = config.flow_id;
+			sch_cfg.pri = 0;
+			sch_cfg.drr_node_id = config.drr_node_id;
 		} else {
-			qos_cfg.flow_id = config.flow_id + (i / config.pri_max);
-			qos_cfg.pri = i % config.pri_max;
-			qos_cfg.drr_node_id = config.drr_node_id + i;
+			sch_cfg.flow_id = config.flow_id + (i / config.pri_max);
+			sch_cfg.pri = i % config.pri_max;
+			sch_cfg.drr_node_id = config.drr_node_id + i;
 		}
 
 		/* Scheduler weight, must be more than 0. */
-		qos_cfg.drr_node_wt = 1;
-		/* Byte based to schedule. */
-		qos_cfg.node_unit = 0;
+		sch_cfg.drr_node_wt = 1;
+		/* Byte based to be scheduled. */
+		sch_cfg.unit_is_packet = false;
 		/* Frame + CRC calculated. */
-		qos_cfg.node_frame_mode = 1;
+		sch_cfg.frame_mode = PPE_SCH_WITH_FRAME_CRC;
 
 		ret = ppe_queue_scheduler_set(ppe_dev, config.node_id + i,
 					      config.flow_level,
 					      config.port,
-					      qos_cfg);
-		if (ret) {
-			dev_err(ppe_dev->dev, "PPE scheduler config error %d\n", ret);
-			return ret;
-		}
-	}
-
-	return 0;
-}
-
-/* Configure PPE offloaded QoS scheduler. */
-static int ppe_config_qos(struct ppe_device *ppe_dev)
-{
-	int ret, i;
-
-	for (i = 0; i < ARRAY_SIZE(ppe_qos_schedule_config); i++) {
-		if (ppe_qos_schedule_config[i].port >= ppe_dev->num_ports)
-			break;
-
-		ret = ppe_node_scheduler_config(ppe_dev, ppe_qos_schedule_config[i]);
+					      sch_cfg);
 		if (ret)
 			return ret;
 	}
@@ -1702,21 +1735,22 @@ static int ppe_config_qos(struct ppe_device *ppe_dev)
 	return 0;
 }
 
-/* Configure scheduling management of PPE ports. */
+/* Initialize scheduler settings for PPE buffer utilization and dispatching
+ * packet on PPE queue.
+ */
 static int ppe_config_scheduler(struct ppe_device *ppe_dev)
 {
-	struct ppe_sch_schedule_config *schedule_cfg;
-	int ret, i, bm_count, schedule_count;
-	struct ppe_sch_bm_config *bm_cfg;
+	const struct ppe_scheduler_port_config *port_cfg;
+	const struct ppe_scheduler_qm_config *qm_cfg;
+	const struct ppe_scheduler_bm_config *bm_cfg;
+	int ret, i, count;
 	u32 val, reg;
 
+	count = ARRAY_SIZE(ipq9574_ppe_sch_bm_config);
 	bm_cfg = ipq9574_ppe_sch_bm_config;
-	bm_count = ARRAY_SIZE(ipq9574_ppe_sch_bm_config);
 
-	schedule_cfg = ipq9574_ppe_sch_schedule_config;
-	schedule_count = ARRAY_SIZE(ipq9574_ppe_sch_schedule_config);
-
-	val = FIELD_PREP(PPE_BM_SCH_CTRL_SCH_DEPTH, bm_count);
+	/* Configure the depth of BM scheduler entries. */
+	val = FIELD_PREP(PPE_BM_SCH_CTRL_SCH_DEPTH, count);
 	val |= FIELD_PREP(PPE_BM_SCH_CTRL_SCH_OFFSET, 0);
 	val |= FIELD_PREP(PPE_BM_SCH_CTRL_SCH_EN, 1);
 
@@ -1724,12 +1758,16 @@ static int ppe_config_scheduler(struct ppe_device *ppe_dev)
 	if (ret)
 		goto sch_config_fail;
 
-	for (i = 0; i < bm_count; i++) {
+	/* Configure each BM scheduler entry with the valid ingress port and
+	 * egress port, the second port takes effect when the specified port
+	 * is in the inactive state.
+	 */
+	for (i = 0; i < count; i++) {
 		val = FIELD_PREP(PPE_BM_SCH_CFG_TBL_VALID, bm_cfg[i].valid);
-		val |= FIELD_PREP(PPE_BM_SCH_CFG_TBL_DIR, bm_cfg[i].is_egress);
+		val |= FIELD_PREP(PPE_BM_SCH_CFG_TBL_DIR, bm_cfg[i].dir);
 		val |= FIELD_PREP(PPE_BM_SCH_CFG_TBL_PORT_NUM, bm_cfg[i].port);
-		val |= FIELD_PREP(PPE_BM_SCH_CFG_TBL_SECOND_PORT_VALID, bm_cfg[i].second_valid);
-		val |= FIELD_PREP(PPE_BM_SCH_CFG_TBL_SECOND_PORT, bm_cfg[i].second_port);
+		val |= FIELD_PREP(PPE_BM_SCH_CFG_TBL_SECOND_PORT_VALID, bm_cfg[i].backup_port_valid);
+		val |= FIELD_PREP(PPE_BM_SCH_CFG_TBL_SECOND_PORT, bm_cfg[i].backup_port);
 
 		reg = PPE_BM_SCH_CFG_TBL_ADDR + i * PPE_BM_SCH_CFG_TBL_INC;
 		ret = regmap_write(ppe_dev->regmap, reg, val);
@@ -1737,30 +1775,51 @@ static int ppe_config_scheduler(struct ppe_device *ppe_dev)
 			goto sch_config_fail;
 	}
 
-	val = FIELD_PREP(PPE_PSCH_SCH_DEPTH_CFG_SCH_DEPTH, schedule_count);
+	count = ARRAY_SIZE(ipq9574_ppe_sch_qm_config);
+	qm_cfg = ipq9574_ppe_sch_qm_config;
+
+	/* Configure the depth of QM scheduler entries. */
+	val = FIELD_PREP(PPE_PSCH_SCH_DEPTH_CFG_SCH_DEPTH, count);
 	ret = regmap_write(ppe_dev->regmap, PPE_PSCH_SCH_DEPTH_CFG_ADDR, val);
 	if (ret)
 		goto sch_config_fail;
 
-	for (i = 0; i < schedule_count; i++) {
+	/* Configure each QM scheduler entry with enqueue port and dequeue
+	 * port, the second port takes effect when the specified dequeue
+	 * port is in the inactive port.
+	 */
+	for (i = 0; i < count; i++) {
 		val = FIELD_PREP(PPE_PSCH_SCH_CFG_TBL_ENS_PORT_BITMAP,
-				 schedule_cfg[i].ensch_port_bmp);
+				 qm_cfg[i].ensch_port_bmp);
 		val |= FIELD_PREP(PPE_PSCH_SCH_CFG_TBL_ENS_PORT,
-				  schedule_cfg[i].ensch_port);
+				  qm_cfg[i].ensch_port);
 		val |= FIELD_PREP(PPE_PSCH_SCH_CFG_TBL_DES_PORT,
-				  schedule_cfg[i].desch_port);
+				  qm_cfg[i].desch_port);
 		val |= FIELD_PREP(PPE_PSCH_SCH_CFG_TBL_DES_SECOND_PORT_EN,
-				  schedule_cfg[i].desch_second_valid);
+				  qm_cfg[i].desch_backup_port_valid);
 		val |= FIELD_PREP(PPE_PSCH_SCH_CFG_TBL_DES_SECOND_PORT,
-				  schedule_cfg[i].desch_second_port);
-		reg = PPE_PSCH_SCH_CFG_TBL_ADDR + i * PPE_PSCH_SCH_CFG_TBL_INC;
+				  qm_cfg[i].desch_backup_port);
 
+		reg = PPE_PSCH_SCH_CFG_TBL_ADDR + i * PPE_PSCH_SCH_CFG_TBL_INC;
 		ret = regmap_write(ppe_dev->regmap, reg, val);
 		if (ret)
 			goto sch_config_fail;
 	}
 
-	return ppe_config_qos(ppe_dev);
+	count = ARRAY_SIZE(ppe_port_sch_config);
+	port_cfg = ppe_port_sch_config;
+
+	/* Configure scheduler per PPE queue or flow. */
+	for (i = 0; i < count; i++) {
+		if (port_cfg[i].port >= ppe_dev->num_ports)
+			break;
+
+		ret = ppe_node_scheduler_config(ppe_dev, port_cfg[i]);
+		if (ret)
+			goto sch_config_fail;
+	}
+
+	return 0;
 
 sch_config_fail:
 	dev_err(ppe_dev->dev, "PPE scheduler arbitration config error %d\n", ret);
@@ -1770,7 +1829,7 @@ static int ppe_config_scheduler(struct ppe_device *ppe_dev)
 /* Configure PPE queue destination of each PPE port. */
 static int ppe_queue_dest_init(struct ppe_device *ppe_dev)
 {
-	int ret, port_id, index, class, res_start, res_end, queue_base, pri_max;
+	int ret, port_id, index, q_base, q_offset, res_start, res_end, pri_max;
 	struct ppe_queue_ucast_dest queue_dst;
 
 	for (port_id = 0; port_id < ppe_dev->num_ports; port_id++) {
@@ -1781,14 +1840,14 @@ static int ppe_queue_dest_init(struct ppe_device *ppe_dev)
 		if (ret)
 			return ret;
 
-		queue_base = res_start;
+		q_base = res_start;
 		queue_dst.dest_port = port_id;
 
 		/* Configure queue base ID and profile ID that is same as
 		 * physical port ID.
 		 */
 		ret = ppe_queue_ucast_base_set(ppe_dev, queue_dst,
-					       queue_base, port_id);
+					       q_base, port_id);
 		if (ret)
 			return ret;
 
@@ -1802,8 +1861,7 @@ static int ppe_queue_dest_init(struct ppe_device *ppe_dev)
 
 		/* Redirect ARP reply packet with the max priority on CPU port,
 		 * which keeps the ARP reply directed to CPU (CPU code is 101)
-		 * with highest priority received by EDMA when there is a heavy
-		 * traffic loaded.
+		 * with highest priority queue of EDMA.
 		 */
 		if (port_id == 0) {
 			memset(&queue_dst, 0, sizeof(queue_dst));
@@ -1811,30 +1869,29 @@ static int ppe_queue_dest_init(struct ppe_device *ppe_dev)
 			queue_dst.cpu_code_en = true;
 			queue_dst.cpu_code = 101;
 			ret = ppe_queue_ucast_base_set(ppe_dev, queue_dst,
-						       queue_base + pri_max,
+						       q_base + pri_max,
 						       0);
 			if (ret)
 				return ret;
 		}
 
-		/* Initialize the class offset of internal priority. */
+		/* Initialize the queue offset of internal priority. */
 		for (index = 0; index < PPE_QUEUE_INTER_PRI_NUM; index++) {
-			class = index > pri_max ? pri_max : index;
+			q_offset = index > pri_max ? pri_max : index;
 
-			ret = ppe_queue_ucast_pri_class_set(ppe_dev, port_id,
-							    index, class);
+			ret = ppe_queue_ucast_offset_pri_set(ppe_dev, port_id,
+							     index, q_offset);
 			if (ret)
 				return ret;
 		}
 
-		/* Initialize the class offset of RSS hash as 0 to avoid the
+		/* Initialize the queue offset of RSS hash as 0 to avoid the
 		 * random hardware value that will lead to the unexpected
 		 * destination queue generated.
 		 */
-		index = 0;
 		for (index = 0; index < PPE_QUEUE_HASH_NUM; index++) {
-			ret = ppe_queue_ucast_hash_class_set(ppe_dev, port_id,
-							     index, 0);
+			ret = ppe_queue_ucast_offset_hash_set(ppe_dev, port_id,
+							      index, 0);
 			if (ret)
 				return ret;
 		}
@@ -1846,31 +1903,32 @@ static int ppe_queue_dest_init(struct ppe_device *ppe_dev)
 /* Initialize the service code 1 used by CPU port. */
 static int ppe_servcode_init(struct ppe_device *ppe_dev)
 {
-	struct ppe_servcode_cfg servcode_cfg = {};
+	struct ppe_sc_cfg sc_cfg = {};
 
-	bitmap_zero(servcode_cfg.bitmaps.counter, PPE_SC_BYPASS_COUNTER_SIZE);
-	bitmap_zero(servcode_cfg.bitmaps.tunnel, PPE_SC_BYPASS_TUNNEL_SIZE);
+	bitmap_zero(sc_cfg.bitmaps.counter, PPE_SC_BYPASS_COUNTER_SIZE);
+	bitmap_zero(sc_cfg.bitmaps.tunnel, PPE_SC_BYPASS_TUNNEL_SIZE);
 
-	bitmap_fill(servcode_cfg.bitmaps.ingress, PPE_SC_BYPASS_INGRESS_SIZE);
-	clear_bit(PPE_SC_BYPASS_INGRESS_FAKE_MAC_HEADER, servcode_cfg.bitmaps.ingress);
-	clear_bit(PPE_SC_BYPASS_INGRESS_SERVICE_CODE, servcode_cfg.bitmaps.ingress);
-	clear_bit(PPE_SC_BYPASS_INGRESS_FAKE_L2_PROTO, servcode_cfg.bitmaps.ingress);
+	bitmap_fill(sc_cfg.bitmaps.ingress, PPE_SC_BYPASS_INGRESS_SIZE);
+	clear_bit(PPE_SC_BYPASS_INGRESS_FAKE_MAC_HEADER, sc_cfg.bitmaps.ingress);
+	clear_bit(PPE_SC_BYPASS_INGRESS_SERVICE_CODE, sc_cfg.bitmaps.ingress);
+	clear_bit(PPE_SC_BYPASS_INGRESS_FAKE_L2_PROTO, sc_cfg.bitmaps.ingress);
 
-	bitmap_fill(servcode_cfg.bitmaps.egress, PPE_SC_BYPASS_EGRESS_SIZE);
-	clear_bit(PPE_SC_BYPASS_EGRESS_ACL_POST_ROUTING_CHECK, servcode_cfg.bitmaps.egress);
+	bitmap_fill(sc_cfg.bitmaps.egress, PPE_SC_BYPASS_EGRESS_SIZE);
+	clear_bit(PPE_SC_BYPASS_EGRESS_ACL_POST_ROUTING_CHECK, sc_cfg.bitmaps.egress);
 
-	return ppe_servcode_config_set(ppe_dev, PPE_EDMA_SC_BYPASS_ID, servcode_cfg);
+	return ppe_sc_config_set(ppe_dev, PPE_EDMA_SC_BYPASS_ID, sc_cfg);
 }
 
 /* Initialize PPE port configurations. */
-static int ppe_port_ctrl_init(struct ppe_device *ppe_dev)
+static int ppe_port_config_init(struct ppe_device *ppe_dev)
 {
 	u32 reg, val, mru_mtu_val[3];
 	int i, ret;
 
+	/* MTU and MRU settings are not required for CPU port 0. */
 	for (i = 1; i < ppe_dev->num_ports; i++) {
-		/* Enable PPE port counter */
-		ret = ppe_counter_set(ppe_dev, i, true);
+		/* Enable Ethernet port counter */
+		ret = ppe_counter_enable_set(ppe_dev, i);
 		if (ret)
 			return ret;
 
@@ -1881,7 +1939,7 @@ static int ppe_port_ctrl_init(struct ppe_device *ppe_dev)
 			return ret;
 
 		/* Drop the packet when the packet size is more than
-		 * the MTU or MRU of the physical PPE port.
+		 * the MTU or MRU of the physical interface.
 		 */
 		PPE_MRU_MTU_CTRL_SET_MRU_CMD(mru_mtu_val, PPE_ACTION_DROP);
 		PPE_MRU_MTU_CTRL_SET_MTU_CMD(mru_mtu_val, PPE_ACTION_DROP);
@@ -1899,12 +1957,14 @@ static int ppe_port_ctrl_init(struct ppe_device *ppe_dev)
 			return ret;
 	}
 
-	/* Enable CPU port counter. */
-	return ppe_counter_set(ppe_dev, 0, true);
+	/* Enable CPU port counters. */
+	return ppe_counter_enable_set(ppe_dev, 0);
 }
 
-/* Initialize PPE RSS hash configuration, the RSS hash configs decides the
- * random hash value generated, which is used to generate the queue offset.
+/* Initialize the PPE RSS configuration for IPv4 and IPv6 packet receive.
+ * RSS settings are to calculate the random RSS hash value generated during
+ * packet receive. This hash is then used to generate the queue offset used
+ * to determine the queue used to transmit the packet.
  */
 static int ppe_rss_hash_init(struct ppe_device *ppe_dev)
 {
@@ -1915,19 +1975,30 @@ static int ppe_rss_hash_init(struct ppe_device *ppe_dev)
 
 	hash_cfg.hash_seed = get_random_u32();
 	hash_cfg.hash_mask = 0xfff;
+
+	/* Use 5 tuple as RSS hash key for the first fragment of TCP, UDP
+	 * and UDP-Lite packets.
+	 */
 	hash_cfg.hash_fragment_mode = false;
 
+	/* The final common seed configs used to calculate the RSS has value,
+	 * which is available for both IPv4 and IPv6 packet.
+	 */
 	for (i = 0; i < ARRAY_SIZE(fins); i++) {
 		hash_cfg.hash_fin_inner[i] = fins[i] & 0x1f;
 		hash_cfg.hash_fin_outer[i] = fins[i] >> 5;
 	}
 
+	/* RSS seeds for IP protocol, L4 destination & source port and
+	 * destination & source IP used to calculate the RSS hash value.
+	 */
 	hash_cfg.hash_protocol_mix = 0x13;
 	hash_cfg.hash_dport_mix = 0xb;
 	hash_cfg.hash_sport_mix = 0x13;
-	hash_cfg.hash_sip_mix[0] = 0x13;
 	hash_cfg.hash_dip_mix[0] = 0xb;
+	hash_cfg.hash_sip_mix[0] = 0x13;
 
+	/* Configure RSS seed configs for IPv4 packet. */
 	ret = ppe_rss_hash_config_set(ppe_dev, PPE_RSS_HASH_MODE_IPV4, hash_cfg);
 	if (ret)
 		return ret;
@@ -1937,13 +2008,16 @@ static int ppe_rss_hash_init(struct ppe_device *ppe_dev)
 		hash_cfg.hash_dip_mix[i] = ips[i];
 	}
 
+	/* Configure RSS seed configs for IPv6 packet. */
 	return ppe_rss_hash_config_set(ppe_dev, PPE_RSS_HASH_MODE_IPV6, hash_cfg);
 }
 
-/* Initialize queues of CPU port mapped with EDMA ring 0. */
+/* Initialize mapping between PPE queues assigned to CPU port 0
+ * to Ethernet DMA ring 0.
+ */
 static int ppe_queues_to_ring_init(struct ppe_device *ppe_dev)
 {
-	u32 queue_bmap[PPE_RING_MAPPED_BP_QUEUE_WORD_COUNT] = {};
+	u32 queue_bmap[PPE_RING_TO_QUEUE_BITMAP_WORD_CNT] = {};
 	int ret, queue_id, queue_max;
 
 	ret = ppe_port_resource_get(ppe_dev, 0, PPE_RES_UCAST,
@@ -1957,14 +2031,18 @@ static int ppe_queues_to_ring_init(struct ppe_device *ppe_dev)
 	return ppe_ring_queue_map_set(ppe_dev, 0, queue_bmap);
 }
 
-/* Initialize PPE bridge configuration. */
+/* Initialize PPE bridge settings to only enable L2 frame receive and
+ * transmit between CPU port and PPE Ethernet ports.
+ */
 static int ppe_bridge_init(struct ppe_device *ppe_dev)
 {
 	u32 reg, mask, port_cfg[4], vsi_cfg[2];
 	int ret, i;
 
-	/* CPU port0 enable bridge Tx and disable FDB new address
-	 * learning and station move address learning.
+	/* Configure the following settings for CPU port0:
+	 * a.) Enable Bridge TX
+	 * b.) Disable FDB new address learning
+	 * c.) Disable station move address learning
 	 */
 	mask = PPE_PORT_BRIDGE_TXMAC_EN;
 	mask |= PPE_PORT_BRIDGE_NEW_LRN_EN;
@@ -1977,8 +2055,9 @@ static int ppe_bridge_init(struct ppe_device *ppe_dev)
 		return ret;
 
 	for (i = 1; i < ppe_dev->num_ports; i++) {
-		/* Set Invalid VSI forwarding to CPU port0 if no VSI
-		 * is assigned to the port.
+		/* Enable invalid VSI forwarding for all the physical ports
+		 * to CPU port0, in case no VSI is assigned to the physical
+		 * port.
 		 */
 		reg = PPE_L2_VP_PORT_TBL_ADDR + PPE_L2_VP_PORT_TBL_INC * i;
 		ret = regmap_bulk_read(ppe_dev->regmap, reg,
@@ -1996,10 +2075,18 @@ static int ppe_bridge_init(struct ppe_device *ppe_dev)
 			return ret;
 	}
 
-	for (i = 0; i < PPE_VSI_TBL_NUM; i++) {
-		/* Enable VSI bridge forward address learning and set VSI
-		 * forward member includes CPU port0.
+	for (i = 0; i < PPE_VSI_TBL_ENTRIES; i++) {
+		/* Set the VSI forward membership to include only CPU port0.
+		 * FDB learning and forwarding take place only after switchdev
+		 * is supported later to create the VSI and join the physical
+		 * ports to the VSI port member.
 		 */
+		reg = PPE_VSI_TBL_ADDR + PPE_VSI_TBL_INC * i;
+		ret = regmap_bulk_read(ppe_dev->regmap, reg,
+				       vsi_cfg, ARRAY_SIZE(vsi_cfg));
+		if (ret)
+			return ret;
+
 		PPE_VSI_SET_MEMBER_PORT_BITMAP(vsi_cfg, BIT(0));
 		PPE_VSI_SET_UUC_BITMAP(vsi_cfg, BIT(0));
 		PPE_VSI_SET_UMC_BITMAP(vsi_cfg, BIT(0));
@@ -2009,7 +2096,6 @@ static int ppe_bridge_init(struct ppe_device *ppe_dev)
 		PPE_VSI_SET_STATION_MOVE_LRN_EN(vsi_cfg, true);
 		PPE_VSI_SET_STATION_MOVE_FWD_CMD(vsi_cfg, PPE_ACTION_FORWARD);
 
-		reg = PPE_VSI_TBL_ADDR + PPE_VSI_TBL_INC * i;
 		ret = regmap_bulk_write(ppe_dev->regmap, reg,
 					vsi_cfg, ARRAY_SIZE(vsi_cfg));
 		if (ret)
@@ -2019,49 +2105,41 @@ static int ppe_bridge_init(struct ppe_device *ppe_dev)
 	return 0;
 }
 
-/* Initialize PPE device to handle traffic correctly. */
-static int ppe_dev_hw_init(struct ppe_device *ppe_dev)
+int ppe_hw_config(struct ppe_device *ppe_dev)
 {
 	int ret;
 
-	ret = ppe_queue_dest_init(ppe_dev);
+	ret = ppe_config_bm(ppe_dev);
 	if (ret)
 		return ret;
 
-	ret = ppe_servcode_init(ppe_dev);
+	ret = ppe_config_qm(ppe_dev);
 	if (ret)
 		return ret;
 
-	ret = ppe_port_ctrl_init(ppe_dev);
+	ret = ppe_config_scheduler(ppe_dev);
 	if (ret)
 		return ret;
 
-	ret = ppe_rss_hash_init(ppe_dev);
+	ret = ppe_queue_dest_init(ppe_dev);
 	if (ret)
 		return ret;
 
-	ret = ppe_queues_to_ring_init(ppe_dev);
+	ret = ppe_servcode_init(ppe_dev);
 	if (ret)
 		return ret;
 
-	return ppe_bridge_init(ppe_dev);
-}
-
-int ppe_hw_config(struct ppe_device *ppe_dev)
-{
-	int ret;
-
-	ret = ppe_config_bm(ppe_dev);
+	ret = ppe_port_config_init(ppe_dev);
 	if (ret)
 		return ret;
 
-	ret = ppe_config_qm(ppe_dev);
+	ret = ppe_rss_hash_init(ppe_dev);
 	if (ret)
 		return ret;
 
-	ret = ppe_config_scheduler(ppe_dev);
+	ret = ppe_queues_to_ring_init(ppe_dev);
 	if (ret)
 		return ret;
 
-	return ppe_dev_hw_init(ppe_dev);
+	return ppe_bridge_init(ppe_dev);
 }
diff --git a/drivers/net/ethernet/qualcomm/ppe/ppe_config.h b/drivers/net/ethernet/qualcomm/ppe/ppe_config.h
index 9be749800..8cdb727ce 100644
--- a/drivers/net/ethernet/qualcomm/ppe/ppe_config.h
+++ b/drivers/net/ethernet/qualcomm/ppe/ppe_config.h
@@ -1,6 +1,6 @@
 /* SPDX-License-Identifier: GPL-2.0-only
  *
- * Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+ * Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
  */
 
 #ifndef __PPE_CONFIG_H__
@@ -8,50 +8,92 @@
 
 #include <linux/types.h>
 
-/* There are different queue config ranges for the destination port,
- * CPU code and service code.
+#include "ppe.h"
+
+/* There are different table index ranges for configuring queue base ID of
+ * the destination port, CPU code and service code.
  */
 #define PPE_QUEUE_BASE_DEST_PORT		0
 #define PPE_QUEUE_BASE_CPU_CODE			1024
 #define PPE_QUEUE_BASE_SERVICE_CODE		2048
 
+#define PPE_QUEUE_INTER_PRI_NUM			16
+#define PPE_QUEUE_HASH_NUM			256
+
+/* The service code is used by EDMA port to transmit packet to PPE. */
+#define PPE_EDMA_SC_BYPASS_ID			1
+
+/* The PPE RSS hash configured for IPv4 and IPv6 packet separately. */
 #define PPE_RSS_HASH_MODE_IPV4			BIT(0)
 #define PPE_RSS_HASH_MODE_IPV6			BIT(1)
 #define PPE_RSS_HASH_IP_LENGTH			4
 #define PPE_RSS_HASH_TUPLES			5
 
-#define PPE_RING_MAPPED_BP_QUEUE_WORD_COUNT	10
+/* PPE supports 300 queues, each bit presents as one queue. */
+#define PPE_RING_TO_QUEUE_BITMAP_WORD_CNT	10
+
+/**
+ * enum ppe_scheduler_frame_mode - PPE scheduler frame mode.
+ * @PPE_SCH_WITH_IPG_PREAMBLE_FRAME_CRC: The scheduled frame includes IPG,
+ * preamble, Ethernet packet and CRC.
+ * @PPE_SCH_WITH_FRAME_CRC: The scheduled frame includes Ethernet frame and CRC
+ * excluding IPG and preamble.
+ * @PPE_SCH_WITH_L3_PAYLOAD: The scheduled frame includes layer 3 packet data.
+ */
+enum ppe_scheduler_frame_mode {
+	PPE_SCH_WITH_IPG_PREAMBLE_FRAME_CRC = 0,
+	PPE_SCH_WITH_FRAME_CRC = 1,
+	PPE_SCH_WITH_L3_PAYLOAD = 2,
+};
 
 /**
- * struct ppe_qos_scheduler_cfg - PPE QoS scheduler configuration.
+ * struct ppe_scheduler_cfg - PPE scheduler configuration.
  * @flow_id: PPE flow ID.
  * @pri: Scheduler priority.
  * @drr_node_id: Node ID for scheduled traffic.
- * @drr_node_wt: weight for scheduled traffic.
- * @node_unit : Unit for scheduled traffic.
- * @node_frame_mode: Packet mode to be scheduled.
+ * @drr_node_wt: Weight for scheduled traffic.
+ * @unit_is_packet: Packet based or byte based unit for scheduled traffic.
+ * @frame_mode: Packet mode to be scheduled.
  *
- * PPE QoS feature supports the commit and exceed traffic.
+ * PPE scheduler supports commit rate and exceed rate configurations.
  */
-struct ppe_qos_scheduler_cfg {
+struct ppe_scheduler_cfg {
 	int flow_id;
 	int pri;
 	int drr_node_id;
 	int drr_node_wt;
-	int node_unit;
-	int node_frame_mode;
+	bool unit_is_packet;
+	enum ppe_scheduler_frame_mode frame_mode;
+};
+
+/**
+ * enum ppe_resource_type - PPE resource type.
+ * @PPE_RES_UCAST: Unicast queue resource.
+ * @PPE_RES_MCAST: Multicast queue resource.
+ * @PPE_RES_L0_NODE: Level 0 for queue based node resource.
+ * @PPE_RES_L1_NODE: Level 1 for flow based node resource.
+ * @PPE_RES_FLOW_ID: Flow based node resource.
+ */
+enum ppe_resource_type {
+	PPE_RES_UCAST,
+	PPE_RES_MCAST,
+	PPE_RES_L0_NODE,
+	PPE_RES_L1_NODE,
+	PPE_RES_FLOW_ID,
 };
 
 /**
  * struct ppe_queue_ucast_dest - PPE unicast queue destination.
  * @src_profile: Source profile.
- * @service_code_en: Enable service code.
+ * @service_code_en: Enable service code to map the queue base ID.
  * @service_code: Service code.
- * @cpu_code_en: Enable CPU code.
+ * @cpu_code_en: Enable CPU code to map the queue base ID.
  * @cpu_code: CPU code.
  * @dest_port: destination port.
  *
- * PPE egress queue ID is decided by the egress port ID.
+ * PPE egress queue ID is decided by the service code if enabled, otherwise
+ * by the CPU code if enabled, or by destination port if both service code
+ * and CPU code are disabled.
  */
 struct ppe_queue_ucast_dest {
 	int src_profile;
@@ -162,7 +204,7 @@ enum ppe_sc_tunnel_type {
 };
 
 /**
- * struct ppe_sc_bypss - PPE service bypass bitmaps
+ * struct ppe_sc_bypass - PPE service bypass bitmaps
  * @ingress: Bitmap of features that can be bypassed on the ingress packet.
  * @egress: Bitmap of features that can be bypassed on the egress packet.
  * @counter: Bitmap of features that can be bypassed on the counter type.
@@ -176,31 +218,36 @@ struct ppe_sc_bypass {
 };
 
 /**
- * struct ppe_servcode_cfg - PPE service code configuration.
+ * struct ppe_sc_cfg - PPE service code configuration.
  * @dest_port_valid: Generate destination port or not.
  * @dest_port: Destination port ID.
  * @bitmaps: Bitmap of bypass features.
  * @is_src: Destination port acts as source port, packet sent to CPU.
- * @field_update_bitmap: Fields updated to the EDMA preheader.
- * @next_service_code: New service code.
- * @hw_service: Hardware functions selected.
- * @offset_sel: Packet offset selection.
+ * @next_service_code: New service code generated.
+ * @eip_field_update_bitmap: Fields updated as actions taken for EIP.
+ * @eip_hw_service: Selected hardware functions for EIP.
+ * @eip_offset_sel: Packet offset selection, using packet's layer 4 offset
+ * or using packet's layer 3 offset for EIP.
  *
  * Service code is generated during the packet passing through PPE.
  */
-struct ppe_servcode_cfg {
+struct ppe_sc_cfg {
 	bool dest_port_valid;
 	int dest_port;
 	struct ppe_sc_bypass bitmaps;
 	bool is_src;
-	int field_update_bitmap;
 	int next_service_code;
-	int hw_service;
-	int offset_sel;
+	int eip_field_update_bitmap;
+	int eip_hw_service;
+	int eip_offset_sel;
 };
 
-/* The action of packet received by PPE can be forwarded, dropped, copied
- * to CPU (enter multicast queue), redirected to CPU (enter unicast queue).
+/**
+ * enum ppe_action_type - PPE action of the received packet.
+ * @PPE_ACTION_FORWARD: Packet forwarded per L2/L3 process.
+ * @PPE_ACTION_DROP: Packet dropped by PPE.
+ * @PPE_ACTION_COPY_TO_CPU: Packet copied to CPU port per multicast queue.
+ * @PPE_ACTION_REDIRECT_TO_CPU: Packet redirected to CPU port per unicast queue.
  */
 enum ppe_action_type {
 	PPE_ACTION_FORWARD = 0,
@@ -212,18 +259,20 @@ enum ppe_action_type {
 /**
  * struct ppe_rss_hash_cfg - PPE RSS hash configuration.
  * @hash_mask: Mask of the generated hash value.
- * @hash_fragment_mode: Mode of the fragment packet for 3 tuples.
+ * @hash_fragment_mode: Hash generation mode for the first fragment of TCP,
+ * UDP and UDP-Lite packets, to use either 3 tuple or 5 tuple for RSS hash
+ * key computation.
  * @hash_seed: Seed to generate RSS hash.
  * @hash_sip_mix: Source IP selection.
  * @hash_dip_mix: Destination IP selection.
  * @hash_protocol_mix: Protocol selection.
  * @hash_sport_mix: Source L4 port selection.
- * @hash_sport_mix: Destination L4 port selection.
+ * @hash_dport_mix: Destination L4 port selection.
  * @hash_fin_inner: RSS hash value first selection.
  * @hash_fin_outer: RSS hash value second selection.
  *
- * PPE RSS hash value is generated based on the RSS hash configuration
- * with the received packet.
+ * PPE RSS hash value is generated for the packet based on the RSS hash
+ * configured.
  */
 struct ppe_rss_hash_cfg {
 	u32 hash_mask;
@@ -241,28 +290,28 @@ struct ppe_rss_hash_cfg {
 int ppe_hw_config(struct ppe_device *ppe_dev);
 int ppe_queue_scheduler_set(struct ppe_device *ppe_dev,
 			    int node_id, bool flow_level, int port,
-			    struct ppe_qos_scheduler_cfg scheduler_cfg);
+			    struct ppe_scheduler_cfg scheduler_cfg);
 int ppe_queue_scheduler_get(struct ppe_device *ppe_dev,
 			    int node_id, bool flow_level, int *port,
-			    struct ppe_qos_scheduler_cfg *scheduler_cfg);
+			    struct ppe_scheduler_cfg *scheduler_cfg);
 int ppe_queue_ucast_base_set(struct ppe_device *ppe_dev,
 			     struct ppe_queue_ucast_dest queue_dst,
 			     int queue_base,
 			     int profile_id);
-int ppe_queue_ucast_pri_class_set(struct ppe_device *ppe_dev,
-				  int profile_id,
-				  int priority,
-				  int class_offset);
-int ppe_queue_ucast_hash_class_set(struct ppe_device *ppe_dev,
+int ppe_queue_ucast_offset_pri_set(struct ppe_device *ppe_dev,
 				   int profile_id,
-				   int rss_hash,
-				   int class_offset);
-int ppe_port_resource_get(struct ppe_device *ppe_dev, int port, int type,
+				   int priority,
+				   int queue_offset);
+int ppe_queue_ucast_offset_hash_set(struct ppe_device *ppe_dev,
+				    int profile_id,
+				    int rss_hash,
+				    int queue_offset);
+int ppe_port_resource_get(struct ppe_device *ppe_dev, int port,
+			  enum ppe_resource_type type,
 			  int *res_start, int *res_end);
-int ppe_servcode_config_set(struct ppe_device *ppe_dev,
-			    int servcode,
-			    struct ppe_servcode_cfg cfg);
-int ppe_counter_set(struct ppe_device *ppe_dev, int port, bool enable);
+int ppe_sc_config_set(struct ppe_device *ppe_dev, int sc,
+		      struct ppe_sc_cfg cfg);
+int ppe_counter_enable_set(struct ppe_device *ppe_dev, int port);
 int ppe_rss_hash_config_set(struct ppe_device *ppe_dev, int mode,
 			    struct ppe_rss_hash_cfg hash_cfg);
 int ppe_ring_queue_map_set(struct ppe_device *ppe_dev,
diff --git a/drivers/net/ethernet/qualcomm/ppe/ppe_debugfs.c b/drivers/net/ethernet/qualcomm/ppe/ppe_debugfs.c
index f325fcf1e..535da780c 100644
--- a/drivers/net/ethernet/qualcomm/ppe/ppe_debugfs.c
+++ b/drivers/net/ethernet/qualcomm/ppe/ppe_debugfs.c
@@ -1,11 +1,13 @@
 // SPDX-License-Identifier: GPL-2.0-only
 /*
- * Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+ * Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
  */
 
 /* PPE debugfs routines for display of PPE counters useful for debug. */
 
+#include <linux/bitfield.h>
 #include <linux/debugfs.h>
+#include <linux/dev_printk.h>
 #include <linux/netdevice.h>
 #include <linux/regmap.h>
 #include <linux/seq_file.h>
@@ -16,30 +18,19 @@
 #include "ppe_debugfs.h"
 #include "ppe_regs.h"
 
-#define PPE_PKT_CNT_TBL_SIZE		3
-#define PPE_DROP_PKT_CNT_TBL_SIZE	5
-
-#define PREFIX_S(desc, cnt_type) \
-	seq_printf(seq, "%-16s %16s", desc, cnt_type)
-#define CNT_ONE_TYPE(cnt, str, index) \
-	seq_printf(seq, "%10u(%s=%04d)", cnt, str, index)
-#define CNT_TWO_TYPE(cnt, cnt1, str, index) \
-	seq_printf(seq, "%10u/%u(%s=%04d)", cnt, cnt1, str, index)
-#define CNT_CPU_CODE(cnt, index) \
-	seq_printf(seq, "%10u(cpucode:%d)", cnt, index)
-#define CNT_DROP_CODE(cnt, port, index) \
-	seq_printf(seq, "%10u(port=%d),dropcode:%d", cnt, port, index)
-
-#define PPE_W0_PKT_CNT				GENMASK(31, 0)
-#define PPE_W2_DROP_PKT_CNT_LOW			GENMASK(31, 8)
-#define PPE_W3_DROP_PKT_CNT_HIGH		GENMASK(7, 0)
-
-#define PPE_GET_PKT_CNT(tbl_cfg)		\
-	u32_get_bits(*((u32 *)(tbl_cfg)), PPE_W0_PKT_CNT)
-#define PPE_GET_DROP_PKT_CNT_LOW(tbl_cfg)	\
-	u32_get_bits(*((u32 *)(tbl_cfg) + 0x2), PPE_W2_DROP_PKT_CNT_LOW)
-#define PPE_GET_DROP_PKT_CNT_HIGH(tbl_cfg)	\
-	u32_get_bits(*((u32 *)(tbl_cfg) + 0x3), PPE_W3_DROP_PKT_CNT_HIGH)
+#define PPE_PKT_CNT_TBL_SIZE				3
+#define PPE_DROP_PKT_CNT_TBL_SIZE			5
+
+#define PPE_W0_PKT_CNT					GENMASK(31, 0)
+#define PPE_W2_DROP_PKT_CNT_LOW				GENMASK(31, 8)
+#define PPE_W3_DROP_PKT_CNT_HIGH			GENMASK(7, 0)
+
+#define PPE_GET_PKT_CNT(tbl_cnt)			\
+	FIELD_GET(PPE_W0_PKT_CNT, *(tbl_cnt))
+#define PPE_GET_DROP_PKT_CNT_LOW(tbl_cnt)		\
+	FIELD_GET(PPE_W2_DROP_PKT_CNT_LOW, *(tbl_cnt + 0x2))
+#define PPE_GET_DROP_PKT_CNT_HIGH(tbl_cnt)		\
+	FIELD_GET(PPE_W3_DROP_PKT_CNT_HIGH, *(tbl_cnt + 0x3))
 
 /**
  * enum ppe_cnt_size_type - PPE counter size type
@@ -47,11 +38,11 @@
  * @PPE_PKT_CNT_SIZE_3WORD: Counter size with table of 3 words
  * @PPE_PKT_CNT_SIZE_5WORD: Counter size with table of 5 words
  *
- * PPE takes the different register size to record the packet counter,
- * which uses single register or register table with 3 words or 5 words.
+ * PPE takes the different register size to record the packet counters.
+ * It uses single register, or register table with 3 words or 5 words.
  * The counter with table size 5 words also records the drop counter.
- * There are also some other counters only occupying several bits less than
- * 32 bits, which is not covered by this enumeration type.
+ * There are also some other counter types occupying sizes less than 32
+ * bits, which is not covered by this enumeration type.
  */
 enum ppe_cnt_size_type {
 	PPE_PKT_CNT_SIZE_1WORD,
@@ -59,6 +50,84 @@ enum ppe_cnt_size_type {
 	PPE_PKT_CNT_SIZE_5WORD,
 };
 
+/**
+ * enum ppe_cnt_type - PPE counter type.
+ * @PPE_CNT_BM: Packet counter processed by BM.
+ * @PPE_CNT_PARSE: Packet counter parsed on ingress.
+ * @PPE_CNT_PORT_RX: Packet counter on the ingress port.
+ * @PPE_CNT_VLAN_RX: VLAN packet counter received.
+ * @PPE_CNT_L2_FWD: Packet counter processed by L2 forwarding.
+ * @PPE_CNT_CPU_CODE: Packet counter marked with various CPU codes.
+ * @PPE_CNT_VLAN_TX: VLAN packet counter transmitted.
+ * @PPE_CNT_PORT_TX: Packet counter on the egress port.
+ * @PPE_CNT_QM: Packet counter processed by QM.
+ */
+enum ppe_cnt_type {
+	PPE_CNT_BM,
+	PPE_CNT_PARSE,
+	PPE_CNT_PORT_RX,
+	PPE_CNT_VLAN_RX,
+	PPE_CNT_L2_FWD,
+	PPE_CNT_CPU_CODE,
+	PPE_CNT_VLAN_TX,
+	PPE_CNT_PORT_TX,
+	PPE_CNT_QM,
+};
+
+/**
+ * struct ppe_debugfs_entry - PPE debugfs entry.
+ * @name: Debugfs file name.
+ * @counter_type: PPE packet counter type.
+ * @ppe: PPE device.
+ *
+ * The PPE debugfs entry is used to create the debugfs file and passed
+ * to debugfs_create_file() as private data.
+ */
+struct ppe_debugfs_entry {
+	const char *name;
+	enum ppe_cnt_type counter_type;
+	struct ppe_device *ppe;
+};
+
+static const struct ppe_debugfs_entry debugfs_files[] = {
+	{
+		.name			= "bm",
+		.counter_type		= PPE_CNT_BM,
+	},
+	{
+		.name			= "parse",
+		.counter_type		= PPE_CNT_PARSE,
+	},
+	{
+		.name			= "port_rx",
+		.counter_type		= PPE_CNT_PORT_RX,
+	},
+	{
+		.name			= "vlan_rx",
+		.counter_type		= PPE_CNT_VLAN_RX,
+	},
+	{
+		.name			= "l2_forward",
+		.counter_type		= PPE_CNT_L2_FWD,
+	},
+	{
+		.name			= "cpu_code",
+		.counter_type		= PPE_CNT_CPU_CODE,
+	},
+	{
+		.name			= "vlan_tx",
+		.counter_type		= PPE_CNT_VLAN_TX,
+	},
+	{
+		.name			= "port_tx",
+		.counter_type		= PPE_CNT_PORT_TX,
+	},
+	{
+		.name			= "qm",
+		.counter_type		= PPE_CNT_QM,
+	},
+};
+
 static int ppe_pkt_cnt_get(struct ppe_device *ppe_dev, u32 reg,
 			   enum ppe_cnt_size_type cnt_type,
 			   u32 *cnt, u32 *drop_cnt)
@@ -126,600 +195,617 @@ static void ppe_tbl_pkt_cnt_clear(struct ppe_device *ppe_dev, u32 reg,
 	}
 }
 
-/* The number of packets dropped because of no buffer available. */
-static void ppe_prx_drop_counter_get(struct ppe_device *ppe_dev,
-				     struct seq_file *seq)
+static int ppe_bm_counter_get(struct ppe_device *ppe_dev, struct seq_file *seq)
 {
-	int ret, i, tag = 0;
-	u32 reg, drop_cnt;
+	u32 reg, val, pkt_cnt, pkt_cnt1;
+	int ret, i, tag;
 
-	PREFIX_S("PRX_DROP_CNT", "SILENT_DROP:");
-	for (i = 0; i < PPE_DROP_CNT_NUM; i++) {
-		reg = PPE_DROP_CNT_ADDR + i * PPE_DROP_CNT_INC;
+	seq_printf(seq, "%-24s", "BM SILENT_DROP:");
+	tag = 0;
+	for (i = 0; i < PPE_DROP_CNT_TBL_ENTRIES; i++) {
+		reg = PPE_DROP_CNT_TBL_ADDR + i * PPE_DROP_CNT_TBL_INC;
 		ret = ppe_pkt_cnt_get(ppe_dev, reg, PPE_PKT_CNT_SIZE_1WORD,
-				      &drop_cnt, NULL);
+				      &pkt_cnt, NULL);
 		if (ret) {
-			seq_printf(seq, "ERROR %d\n", ret);
-			return;
+			dev_err(ppe_dev->dev, "ERROR) %d\n", ret);
+			return ret;
 		}
 
-		if (drop_cnt > 0) {
-			tag++;
-			if (!(tag % 4)) {
-				seq_putc(seq, '\n');
-				PREFIX_S("", "");
-			}
+		if (pkt_cnt > 0) {
+			if (!((++tag) % 4))
+				seq_printf(seq, "\n%-24s", "");
 
-			CNT_ONE_TYPE(drop_cnt, "port", i);
+			seq_printf(seq, "%10u(%s=%04d)", pkt_cnt, "port", i);
 		}
 	}
 
 	seq_putc(seq, '\n');
-}
 
-/* The number of packet dropped because of no enough buffer to cache
- * packet, some buffer allocated for the part of packet.
- */
-static void ppe_prx_bm_drop_counter_get(struct ppe_device *ppe_dev,
-					struct seq_file *seq)
-{
-	u32 reg, pkt_cnt = 0;
-	int ret, i, tag = 0;
-
-	PREFIX_S("PRX_BM_DROP_CNT", "OVERFLOW_DROP:");
-	for (i = 0; i < PPE_DROP_STAT_NUM; i++) {
-		reg = PPE_DROP_STAT_ADDR + PPE_DROP_STAT_INC * i;
+	/* The number of packets dropped because hardware buffers were
+	 * available only partially for the packet.
+	 */
+	seq_printf(seq, "%-24s", "BM OVERFLOW_DROP:");
+	tag = 0;
+	for (i = 0; i < PPE_DROP_STAT_TBL_ENTRIES; i++) {
+		reg = PPE_DROP_STAT_TBL_ADDR + PPE_DROP_STAT_TBL_INC * i;
 
 		ret = ppe_pkt_cnt_get(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD,
 				      &pkt_cnt, NULL);
 		if (ret) {
-			seq_printf(seq, "ERROR %d\n", ret);
-			return;
+			dev_err(ppe_dev->dev, "ERROR) %d\n", ret);
+			return ret;
 		}
 
 		if (pkt_cnt > 0) {
-			tag++;
-			if (!(tag % 4)) {
-				seq_putc(seq, '\n');
-				PREFIX_S("", "");
-			}
+			if (!((++tag) % 4))
+				seq_printf(seq, "\n%-24s", "");
 
-			CNT_ONE_TYPE(pkt_cnt, "port", i);
+			seq_printf(seq, "%10u(%s=%04d)", pkt_cnt, "port", i);
 		}
 	}
 
 	seq_putc(seq, '\n');
-}
-
-/* The number of currently occupied buffers, that can't be flushed. */
-static void ppe_prx_bm_port_counter_get(struct ppe_device *ppe_dev,
-					struct seq_file *seq)
-{
-	int used_cnt, react_cnt;
-	int ret, i, tag = 0;
-	u32 reg, val;
 
-	PREFIX_S("PRX_BM_PORT_CNT", "USED/REACT:");
-	for (i = 0; i < PPE_BM_USED_CNT_NUM; i++) {
-		reg = PPE_BM_USED_CNT_ADDR + i * PPE_BM_USED_CNT_INC;
+	/* The number of currently occupied buffers, that can't be flushed. */
+	seq_printf(seq, "%-24s", "BM USED/REACT:");
+	tag = 0;
+	for (i = 0; i < PPE_BM_USED_CNT_TBL_ENTRIES; i++) {
+		reg = PPE_BM_USED_CNT_TBL_ADDR + i * PPE_BM_USED_CNT_TBL_INC;
 		ret = regmap_read(ppe_dev->regmap, reg, &val);
 		if (ret) {
-			seq_printf(seq, "ERROR %d\n", ret);
-			return;
+			dev_err(ppe_dev->dev, "ERROR) %d\n", ret);
+			return ret;
 		}
 
-		used_cnt = FIELD_GET(PPE_BM_USED_CNT_VAL, val);
+		/* The number of PPE buffers used for caching the received
+		 * packets before the pause frame sent.
+		 */
+		pkt_cnt = FIELD_GET(PPE_BM_USED_CNT_VAL, val);
 
-		reg = PPE_BM_REACT_CNT_ADDR + i * PPE_BM_REACT_CNT_INC;
+		reg = PPE_BM_REACT_CNT_TBL_ADDR + i * PPE_BM_REACT_CNT_TBL_INC;
 		ret = regmap_read(ppe_dev->regmap, reg, &val);
 		if (ret) {
-			seq_printf(seq, "ERROR %d\n", ret);
-			return;
+			dev_err(ppe_dev->dev, "ERROR) %d\n", ret);
+			return ret;
 		}
 
-		react_cnt = FIELD_GET(PPE_BM_REACT_CNT_VAL, val);
+		/* The number of PPE buffers used for caching the received
+		 * packets after pause frame sent out.
+		 */
+		pkt_cnt1 = FIELD_GET(PPE_BM_REACT_CNT_VAL, val);
 
-		if (used_cnt > 0 || react_cnt > 0) {
-			tag++;
-			if (!(tag % 4)) {
-				seq_putc(seq, '\n');
-				PREFIX_S("", "");
-			}
+		if (pkt_cnt > 0 || pkt_cnt1 > 0) {
+			if (!((++tag) % 4))
+				seq_printf(seq, "\n%-24s", "");
 
-			CNT_TWO_TYPE(used_cnt, react_cnt, "port", i);
+			seq_printf(seq, "%10u/%u(%s=%04d)", pkt_cnt, pkt_cnt1,
+				   "port", i);
 		}
 	}
 
 	seq_putc(seq, '\n');
+
+	return 0;
 }
 
-/* The number of ingress packets. */
-static void ppe_ipx_pkt_counter_get(struct ppe_device *ppe_dev,
-				    struct seq_file *seq)
+/* The number of packets processed by the ingress parser module of PPE. */
+static int ppe_parse_pkt_counter_get(struct ppe_device *ppe_dev,
+				     struct seq_file *seq)
 {
-	u32 reg, cnt, tunnel_cnt;
+	u32 reg, cnt = 0, tunnel_cnt = 0;
 	int i, ret, tag = 0;
 
-	PREFIX_S("IPR_PKT_CNT", "TPRX/IPRX:");
-	for (i = 0; i < PPE_IPR_PKT_CNT_NUM; i++) {
-		reg = PPE_TPR_PKT_CNT_ADDR + i * PPE_IPR_PKT_CNT_INC;
+	seq_printf(seq, "%-24s", "PARSE TPRX/IPRX:");
+	for (i = 0; i < PPE_IPR_PKT_CNT_TBL_ENTRIES; i++) {
+		reg = PPE_TPR_PKT_CNT_TBL_ADDR + i * PPE_TPR_PKT_CNT_TBL_INC;
 		ret = ppe_pkt_cnt_get(ppe_dev, reg, PPE_PKT_CNT_SIZE_1WORD,
 				      &tunnel_cnt, NULL);
 		if (ret) {
-			seq_printf(seq, "ERROR %d\n", ret);
-			return;
+			dev_err(ppe_dev->dev, "ERROR) %d\n", ret);
+			return ret;
 		}
 
-		reg = PPE_IPR_PKT_CNT_ADDR + i * PPE_IPR_PKT_CNT_INC;
+		reg = PPE_IPR_PKT_CNT_TBL_ADDR + i * PPE_IPR_PKT_CNT_TBL_INC;
 		ret = ppe_pkt_cnt_get(ppe_dev, reg, PPE_PKT_CNT_SIZE_1WORD,
 				      &cnt, NULL);
 		if (ret) {
-			seq_printf(seq, "ERROR %d\n", ret);
-			return;
+			dev_err(ppe_dev->dev, "ERROR) %d\n", ret);
+			return ret;
 		}
 
 		if (tunnel_cnt > 0 || cnt > 0) {
-			tag++;
-			if (!(tag % 4)) {
-				seq_putc(seq, '\n');
-				PREFIX_S("", "");
-			}
+			if (!((++tag) % 4))
+				seq_printf(seq, "\n%-24s", "");
 
-			CNT_TWO_TYPE(tunnel_cnt, cnt, "port", i);
+			seq_printf(seq, "%10u/%u(%s=%04d)", tunnel_cnt, cnt,
+				   "port", i);
 		}
 	}
 
 	seq_putc(seq, '\n');
+
+	return 0;
 }
 
-/* The number of packet received or dropped on the ingress direction. */
-static void ppe_port_rx_counter_get(struct ppe_device *ppe_dev,
-				    struct seq_file *seq)
+/* The number of packets received or dropped on the ingress port. */
+static int ppe_port_rx_counter_get(struct ppe_device *ppe_dev,
+				   struct seq_file *seq)
 {
-	u32 reg, pkt_cnt, drop_cnt;
-	int ret, i, tag = 0;
+	u32 reg, pkt_cnt = 0, drop_cnt = 0;
+	int ret, i, tag;
 
-	PREFIX_S("PORT_RX_CNT", "RX/RX_DROP:");
-	for (i = 0; i < PPE_PHY_PORT_RX_CNT_TBL_NUM; i++) {
+	seq_printf(seq, "%-24s", "PORT RX/RX_DROP:");
+	tag = 0;
+	for (i = 0; i < PPE_PHY_PORT_RX_CNT_TBL_ENTRIES; i++) {
 		reg = PPE_PHY_PORT_RX_CNT_TBL_ADDR + PPE_PHY_PORT_RX_CNT_TBL_INC * i;
 		ret = ppe_pkt_cnt_get(ppe_dev, reg, PPE_PKT_CNT_SIZE_5WORD,
 				      &pkt_cnt, &drop_cnt);
 		if (ret) {
-			seq_printf(seq, "ERROR %d\n", ret);
-			return;
+			dev_err(ppe_dev->dev, "ERROR) %d\n", ret);
+			return ret;
 		}
 
 		if (pkt_cnt > 0) {
-			tag++;
-			if (!(tag % 4)) {
-				seq_putc(seq, '\n');
-				PREFIX_S("", "");
-			}
+			if (!((++tag) % 4))
+				seq_printf(seq, "\n%-24s", "");
 
-			CNT_TWO_TYPE(pkt_cnt, drop_cnt, "port", i);
+			seq_printf(seq, "%10u/%u(%s=%04d)", pkt_cnt, drop_cnt,
+				   "port", i);
 		}
 	}
 
 	seq_putc(seq, '\n');
-}
-
-/* The number of packet received or dropped by the port. */
-static void ppe_vp_rx_counter_get(struct ppe_device *ppe_dev,
-				  struct seq_file *seq)
-{
-	u32 reg, pkt_cnt, drop_cnt;
-	int ret, i, tag = 0;
 
-	PREFIX_S("VPORT_RX_CNT", "RX/RX_DROP:");
-	for (i = 0; i < PPE_PORT_RX_CNT_TBL_NUM; i++) {
+	seq_printf(seq, "%-24s", "VPORT RX/RX_DROP:");
+	tag = 0;
+	for (i = 0; i < PPE_PORT_RX_CNT_TBL_ENTRIES; i++) {
 		reg = PPE_PORT_RX_CNT_TBL_ADDR + PPE_PORT_RX_CNT_TBL_INC * i;
 		ret = ppe_pkt_cnt_get(ppe_dev, reg, PPE_PKT_CNT_SIZE_5WORD,
 				      &pkt_cnt, &drop_cnt);
 		if (ret) {
-			seq_printf(seq, "ERROR %d\n", ret);
-			return;
+			dev_err(ppe_dev->dev, "ERROR) %d\n", ret);
+			return ret;
 		}
 
 		if (pkt_cnt > 0) {
-			tag++;
-			if (!(tag % 4)) {
-				seq_putc(seq, '\n');
-				PREFIX_S("", "");
-			}
+			if (!((++tag) % 4))
+				seq_printf(seq, "\n%-24s", "");
 
-			CNT_TWO_TYPE(pkt_cnt, drop_cnt, "port", i);
+			seq_printf(seq, "%10u/%u(%s=%04d)", pkt_cnt, drop_cnt,
+				   "port", i);
 		}
 	}
 
 	seq_putc(seq, '\n');
+
+	return 0;
 }
 
-/* The number of packet received or dropped by layer 2 processing. */
-static void ppe_pre_l2_counter_get(struct ppe_device *ppe_dev,
-				   struct seq_file *seq)
+/* The number of packets received or dropped by layer 2 processing. */
+static int ppe_l2_counter_get(struct ppe_device *ppe_dev,
+			      struct seq_file *seq)
 {
-	u32 reg, pkt_cnt, drop_cnt;
+	u32 reg, pkt_cnt = 0, drop_cnt = 0;
 	int ret, i, tag = 0;
 
-	PREFIX_S("PRE_L2_CNT", "RX/RX_DROP:");
-	for (i = 0; i < PPE_PRE_L2_CNT_TBL_NUM; i++) {
+	seq_printf(seq, "%-24s", "L2 RX/RX_DROP:");
+	for (i = 0; i < PPE_PRE_L2_CNT_TBL_ENTRIES; i++) {
 		reg = PPE_PRE_L2_CNT_TBL_ADDR + PPE_PRE_L2_CNT_TBL_INC * i;
 		ret = ppe_pkt_cnt_get(ppe_dev, reg, PPE_PKT_CNT_SIZE_5WORD,
 				      &pkt_cnt, &drop_cnt);
 		if (ret) {
-			seq_printf(seq, "ERROR %d\n", ret);
-			return;
+			dev_err(ppe_dev->dev, "ERROR) %d\n", ret);
+			return ret;
 		}
 
 		if (pkt_cnt > 0) {
-			tag++;
-			if (!(tag % 4)) {
-				seq_putc(seq, '\n');
-				PREFIX_S("", "");
-			}
+			if (!((++tag) % 4))
+				seq_printf(seq, "\n%-24s", "");
 
-			CNT_TWO_TYPE(pkt_cnt, drop_cnt, "vsi", i);
+			seq_printf(seq, "%10u/%u(%s=%04d)", pkt_cnt, drop_cnt,
+				   "vsi", i);
 		}
 	}
 
 	seq_putc(seq, '\n');
+
+	return 0;
 }
 
-/* The number of packet received for VLAN handler. */
-static void ppe_vlan_counter_get(struct ppe_device *ppe_dev,
-				 struct seq_file *seq)
+/* The number of VLAN packets received by PPE. */
+static int ppe_vlan_rx_counter_get(struct ppe_device *ppe_dev,
+				   struct seq_file *seq)
 {
 	u32 reg, pkt_cnt = 0;
 	int ret, i, tag = 0;
 
-	PREFIX_S("VLAN_CNT", "RX:");
-	for (i = 0; i < PPE_VLAN_CNT_TBL_NUM; i++) {
+	seq_printf(seq, "%-24s", "VLAN RX:");
+	for (i = 0; i < PPE_VLAN_CNT_TBL_ENTRIES; i++) {
 		reg = PPE_VLAN_CNT_TBL_ADDR + PPE_VLAN_CNT_TBL_INC * i;
 
 		ret = ppe_pkt_cnt_get(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD,
 				      &pkt_cnt, NULL);
 		if (ret) {
-			seq_printf(seq, "ERROR %d\n", ret);
-			return;
+			dev_err(ppe_dev->dev, "ERROR) %d\n", ret);
+			return ret;
 		}
 
 		if (pkt_cnt > 0) {
-			tag++;
-			if (!(tag % 4)) {
-				seq_putc(seq, '\n');
-				PREFIX_S("", "");
-			}
+			if (!((++tag) % 4))
+				seq_printf(seq, "\n%-24s", "");
 
-			CNT_ONE_TYPE(pkt_cnt, "vsi", i);
+			seq_printf(seq, "%10u(%s=%04d)", pkt_cnt, "vsi", i);
 		}
 	}
 
 	seq_putc(seq, '\n');
+
+	return 0;
 }
 
-/* The number of packet forwarded to CPU handler. */
-static void ppe_cpu_code_counter_get(struct ppe_device *ppe_dev,
-				     struct seq_file *seq)
+/* The number of packets handed to CPU by PPE. */
+static int ppe_cpu_code_counter_get(struct ppe_device *ppe_dev,
+				    struct seq_file *seq)
 {
 	u32 reg, pkt_cnt = 0;
 	int ret, i;
 
-	PREFIX_S("CPU_CODE_CNT", "CODE:");
-	for (i = 0; i < PPE_DROP_CPU_CNT_TBL_NUM; i++) {
+	seq_printf(seq, "%-24s", "CPU CODE:");
+	for (i = 0; i < PPE_DROP_CPU_CNT_TBL_ENTRIES; i++) {
 		reg = PPE_DROP_CPU_CNT_TBL_ADDR + PPE_DROP_CPU_CNT_TBL_INC * i;
 
 		ret = ppe_pkt_cnt_get(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD,
 				      &pkt_cnt, NULL);
 		if (ret) {
-			seq_printf(seq, "ERROR %d\n", ret);
-			return;
+			dev_err(ppe_dev->dev, "ERROR) %d\n", ret);
+			return ret;
 		}
 
 		if (!pkt_cnt)
 			continue;
 
+		/* There are 256 CPU codes saved in the first 256 entries
+		 * of register table, and 128 drop codes for each PPE port
+		 * (0-7), the total entries is 256 + 8 * 128.
+		 */
 		if (i < 256)
-			CNT_CPU_CODE(pkt_cnt, i);
+			seq_printf(seq, "%10u(cpucode:%d)", pkt_cnt, i);
 		else
-			CNT_DROP_CODE(pkt_cnt, (i - 256) % 8, (i - 256) / 8);
-
+			seq_printf(seq, "%10u(port=%d),dropcode:%d", pkt_cnt,
+				   (i - 256) % 8, (i - 256) / 8);
 		seq_putc(seq, '\n');
-		PREFIX_S("", "");
+		seq_printf(seq, "%-24s", "");
 	}
 
 	seq_putc(seq, '\n');
+
+	return 0;
 }
 
-/* The number of packet forwarded by VLAN on the egress direction. */
-static void ppe_eg_vsi_counter_get(struct ppe_device *ppe_dev,
+/* The number of packets forwarded by VLAN on the egress direction. */
+static int ppe_vlan_tx_counter_get(struct ppe_device *ppe_dev,
 				   struct seq_file *seq)
 {
 	u32 reg, pkt_cnt = 0;
 	int ret, i, tag = 0;
 
-	PREFIX_S("EG_VSI_CNT", "TX:");
-	for (i = 0; i < PPE_EG_VSI_COUNTER_TBL_NUM; i++) {
+	seq_printf(seq, "%-24s", "VLAN TX:");
+	for (i = 0; i < PPE_EG_VSI_COUNTER_TBL_ENTRIES; i++) {
 		reg = PPE_EG_VSI_COUNTER_TBL_ADDR + PPE_EG_VSI_COUNTER_TBL_INC * i;
 
 		ret = ppe_pkt_cnt_get(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD,
 				      &pkt_cnt, NULL);
 		if (ret) {
-			seq_printf(seq, "ERROR %d\n", ret);
-			return;
+			dev_err(ppe_dev->dev, "ERROR) %d\n", ret);
+			return ret;
 		}
 
 		if (pkt_cnt > 0) {
-			tag++;
-			if (!(tag % 4)) {
-				seq_putc(seq, '\n');
-				PREFIX_S("", "");
-			}
+			if (!((++tag) % 4))
+				seq_printf(seq, "\n%-24s", "");
 
-			CNT_ONE_TYPE(pkt_cnt, "vsi", i);
+			seq_printf(seq, "%10u(%s=%04d)", pkt_cnt, "vsi", i);
 		}
 	}
 
 	seq_putc(seq, '\n');
+
+	return 0;
 }
 
-/* The number of packet trasmitted or dropped by port. */
-static void ppe_vp_tx_counter_get(struct ppe_device *ppe_dev,
-				  struct seq_file *seq)
+/* The number of packets trasmitted or dropped on the egress port. */
+static int ppe_port_tx_counter_get(struct ppe_device *ppe_dev,
+				   struct seq_file *seq)
 {
 	u32 reg, pkt_cnt = 0, drop_cnt = 0;
-	int ret, i, tag = 0;
+	int ret, i, tag;
 
-	PREFIX_S("VPORT_TX_CNT", "TX/TX_DROP:");
-	for (i = 0; i < PPE_VPORT_TX_COUNTER_TBL_NUM; i++) {
+	seq_printf(seq, "%-24s", "VPORT TX/TX_DROP:");
+	tag = 0;
+	for (i = 0; i < PPE_VPORT_TX_COUNTER_TBL_ENTRIES; i++) {
 		reg = PPE_VPORT_TX_COUNTER_TBL_ADDR + PPE_VPORT_TX_COUNTER_TBL_INC * i;
 		ret = ppe_pkt_cnt_get(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD,
 				      &pkt_cnt, NULL);
 		if (ret) {
-			seq_printf(seq, "ERROR %d\n", ret);
-			return;
+			dev_err(ppe_dev->dev, "ERROR) %d\n", ret);
+			return ret;
 		}
 
 		reg = PPE_VPORT_TX_DROP_CNT_TBL_ADDR + PPE_VPORT_TX_DROP_CNT_TBL_INC * i;
 		ret = ppe_pkt_cnt_get(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD,
 				      &drop_cnt, NULL);
 		if (ret) {
-			seq_printf(seq, "ERROR %d\n", ret);
-			return;
+			dev_err(ppe_dev->dev, "ERROR) %d\n", ret);
+			return ret;
 		}
 
 		if (pkt_cnt > 0 || drop_cnt > 0) {
-			tag++;
-			if (!(tag % 4)) {
-				seq_putc(seq, '\n');
-				PREFIX_S("", "");
-			}
+			if (!((++tag) % 4))
+				seq_printf(seq, "\n%-24s", "");
 
-			CNT_TWO_TYPE(pkt_cnt, drop_cnt, "port", i);
+			seq_printf(seq, "%10u/%u(%s=%04d)", pkt_cnt, drop_cnt,
+				   "port", i);
 		}
 	}
 
 	seq_putc(seq, '\n');
-}
 
-/* The number of packet trasmitted or dropped on the egress direction. */
-static void ppe_port_tx_counter_get(struct ppe_device *ppe_dev,
-				    struct seq_file *seq)
-{
-	u32 reg, pkt_cnt = 0, drop_cnt = 0;
-	int ret, i, tag = 0;
-
-	PREFIX_S("PORT_TX_CNT", "TX/TX_DROP:");
-	for (i = 0; i < PPE_PORT_TX_COUNTER_TBL_NUM; i++) {
+	seq_printf(seq, "%-24s", "PORT TX/TX_DROP:");
+	tag = 0;
+	for (i = 0; i < PPE_PORT_TX_COUNTER_TBL_ENTRIES; i++) {
 		reg = PPE_PORT_TX_COUNTER_TBL_ADDR + PPE_PORT_TX_COUNTER_TBL_INC * i;
 		ret = ppe_pkt_cnt_get(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD,
 				      &pkt_cnt, NULL);
 		if (ret) {
-			seq_printf(seq, "ERROR %d\n", ret);
-			return;
+			dev_err(ppe_dev->dev, "ERROR) %d\n", ret);
+			return ret;
 		}
 
 		reg = PPE_PORT_TX_DROP_CNT_TBL_ADDR + PPE_PORT_TX_DROP_CNT_TBL_INC * i;
 		ret = ppe_pkt_cnt_get(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD,
 				      &drop_cnt, NULL);
 		if (ret) {
-			seq_printf(seq, "ERROR %d\n", ret);
-			return;
+			dev_err(ppe_dev->dev, "ERROR) %d\n", ret);
+			return ret;
 		}
 
 		if (pkt_cnt > 0 || drop_cnt > 0) {
-			tag++;
-			if (!(tag % 4)) {
-				seq_putc(seq, '\n');
-				PREFIX_S("", "");
-			}
+			if (!((++tag) % 4))
+				seq_printf(seq, "\n%-24s", "");
 
-			CNT_TWO_TYPE(pkt_cnt, drop_cnt, "port", i);
+			seq_printf(seq, "%10u/%u(%s=%04d)", pkt_cnt, drop_cnt,
+				   "port", i);
 		}
 	}
 
 	seq_putc(seq, '\n');
+
+	return 0;
 }
 
-/* The number of packet trasmitted or pended by the PPE queue. */
-static void ppe_queue_tx_counter_get(struct ppe_device *ppe_dev,
-				     struct seq_file *seq)
+/* The number of packets transmitted or pending by the PPE queue. */
+static int ppe_queue_counter_get(struct ppe_device *ppe_dev,
+				 struct seq_file *seq)
 {
 	u32 reg, val, pkt_cnt = 0, pend_cnt = 0;
 	int ret, i, tag = 0;
 
-	PREFIX_S("QUEUE_TX_CNT", "TX/PEND:");
-	for (i = 0; i < PPE_QUEUE_TX_COUNTER_TBL_NUM; i++) {
+	seq_printf(seq, "%-24s", "QUEUE TX/PEND:");
+	for (i = 0; i < PPE_QUEUE_TX_COUNTER_TBL_ENTRIES; i++) {
 		reg = PPE_QUEUE_TX_COUNTER_TBL_ADDR + PPE_QUEUE_TX_COUNTER_TBL_INC * i;
 		ret = ppe_pkt_cnt_get(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD,
 				      &pkt_cnt, NULL);
 		if (ret) {
-			seq_printf(seq, "ERROR %d\n", ret);
-			return;
+			dev_err(ppe_dev->dev, "ERROR) %d\n", ret);
+			return ret;
 		}
 
-		if (i < PPE_AC_UNI_QUEUE_CFG_TBL_NUM) {
-			reg = PPE_AC_UNI_QUEUE_CNT_TBL_ADDR + PPE_AC_UNI_QUEUE_CNT_TBL_INC * i;
+		if (i < PPE_AC_UNICAST_QUEUE_CFG_TBL_ENTRIES) {
+			reg = PPE_AC_UNICAST_QUEUE_CNT_TBL_ADDR +
+			      PPE_AC_UNICAST_QUEUE_CNT_TBL_INC * i;
 			ret = regmap_read(ppe_dev->regmap, reg, &val);
 			if (ret) {
-				seq_printf(seq, "ERROR %d\n", ret);
-				return;
+				dev_err(ppe_dev->dev, "ERROR) %d\n", ret);
+				return ret;
 			}
 
-			pend_cnt = FIELD_GET(PPE_AC_UNI_QUEUE_CNT_TBL_PEND_CNT, val);
+			pend_cnt = FIELD_GET(PPE_AC_UNICAST_QUEUE_CNT_TBL_PEND_CNT, val);
 		} else {
-			reg = PPE_AC_MUL_QUEUE_CNT_TBL_ADDR +
-			      PPE_AC_MUL_QUEUE_CNT_TBL_INC * (i - PPE_AC_UNI_QUEUE_CFG_TBL_NUM);
+			reg = PPE_AC_MULTICAST_QUEUE_CNT_TBL_ADDR +
+			      PPE_AC_MULTICAST_QUEUE_CNT_TBL_INC *
+			      (i - PPE_AC_UNICAST_QUEUE_CFG_TBL_ENTRIES);
 			ret = regmap_read(ppe_dev->regmap, reg, &val);
 			if (ret) {
-				seq_printf(seq, "ERROR %d\n", ret);
-				return;
+				dev_err(ppe_dev->dev, "ERROR) %d\n", ret);
+				return ret;
 			}
 
-			pend_cnt = FIELD_GET(PPE_AC_MUL_QUEUE_CNT_TBL_PEND_CNT, val);
+			pend_cnt = FIELD_GET(PPE_AC_MULTICAST_QUEUE_CNT_TBL_PEND_CNT, val);
 		}
 
 		if (pkt_cnt > 0 || pend_cnt > 0) {
-			tag++;
-			if (!(tag % 4)) {
-				seq_putc(seq, '\n');
-				PREFIX_S("", "");
-			}
+			if (!((++tag) % 4))
+				seq_printf(seq, "\n%-24s", "");
 
-			CNT_TWO_TYPE(pkt_cnt, pend_cnt, "queue", i);
+			seq_printf(seq, "%10u/%u(%s=%04d)", pkt_cnt, pend_cnt, "queue", i);
 		}
 	}
 
 	seq_putc(seq, '\n');
+
+	return 0;
 }
 
-/* Display the packet counter of PPE. */
+/* Display the various packet counters of PPE. */
 static int ppe_packet_counter_show(struct seq_file *seq, void *v)
 {
-	struct ppe_device *ppe_dev = seq->private;
-
-	ppe_prx_drop_counter_get(ppe_dev, seq);
-	ppe_prx_bm_drop_counter_get(ppe_dev, seq);
-	ppe_prx_bm_port_counter_get(ppe_dev, seq);
-	ppe_ipx_pkt_counter_get(ppe_dev, seq);
-	ppe_port_rx_counter_get(ppe_dev, seq);
-	ppe_vp_rx_counter_get(ppe_dev, seq);
-	ppe_pre_l2_counter_get(ppe_dev, seq);
-	ppe_vlan_counter_get(ppe_dev, seq);
-	ppe_cpu_code_counter_get(ppe_dev, seq);
-	ppe_eg_vsi_counter_get(ppe_dev, seq);
-	ppe_vp_tx_counter_get(ppe_dev, seq);
-	ppe_port_tx_counter_get(ppe_dev, seq);
-	ppe_queue_tx_counter_get(ppe_dev, seq);
+	struct ppe_debugfs_entry *entry = seq->private;
+	struct ppe_device *ppe_dev = entry->ppe;
+	int ret;
 
-	return 0;
-}
+	switch (entry->counter_type) {
+	case PPE_CNT_BM:
+		ret = ppe_bm_counter_get(ppe_dev, seq);
+		break;
+	case PPE_CNT_PARSE:
+		ret = ppe_parse_pkt_counter_get(ppe_dev, seq);
+		break;
+	case PPE_CNT_PORT_RX:
+		ret = ppe_port_rx_counter_get(ppe_dev, seq);
+		break;
+	case PPE_CNT_VLAN_RX:
+		ret = ppe_vlan_rx_counter_get(ppe_dev, seq);
+		break;
+	case PPE_CNT_L2_FWD:
+		ret = ppe_l2_counter_get(ppe_dev, seq);
+		break;
+	case PPE_CNT_CPU_CODE:
+		ret = ppe_cpu_code_counter_get(ppe_dev, seq);
+		break;
+	case PPE_CNT_VLAN_TX:
+		ret = ppe_vlan_tx_counter_get(ppe_dev, seq);
+		break;
+	case PPE_CNT_PORT_TX:
+		ret = ppe_port_tx_counter_get(ppe_dev, seq);
+		break;
+	case PPE_CNT_QM:
+		ret = ppe_queue_counter_get(ppe_dev, seq);
+		break;
+	default:
+		ret = -EINVAL;
+		break;
+	}
 
-static int ppe_packet_counter_open(struct inode *inode, struct file *file)
-{
-	return single_open(file, ppe_packet_counter_show, inode->i_private);
+	return ret;
 }
 
-static ssize_t ppe_packet_counter_clear(struct file *file,
+/* Flush the various packet counters of PPE. */
+static ssize_t ppe_packet_counter_write(struct file *file,
 					const char __user *buf,
 					size_t count, loff_t *pos)
 {
-	struct ppe_device *ppe_dev = file_inode(file)->i_private;
+	struct ppe_debugfs_entry *entry = file_inode(file)->i_private;
+	struct ppe_device *ppe_dev = entry->ppe;
 	u32 reg;
 	int i;
 
-	for (i = 0; i < PPE_DROP_CNT_NUM; i++) {
-		reg = PPE_DROP_CNT_ADDR + i * PPE_DROP_CNT_INC;
-		ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_1WORD);
-	}
+	switch (entry->counter_type) {
+	case PPE_CNT_BM:
+		for (i = 0; i < PPE_DROP_CNT_TBL_ENTRIES; i++) {
+			reg = PPE_DROP_CNT_TBL_ADDR + i * PPE_DROP_CNT_TBL_INC;
+			ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_1WORD);
+		}
 
-	for (i = 0; i < PPE_DROP_STAT_NUM; i++) {
-		reg = PPE_DROP_STAT_ADDR + PPE_DROP_STAT_INC * i;
-		ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD);
-	}
+		for (i = 0; i < PPE_DROP_STAT_TBL_ENTRIES; i++) {
+			reg = PPE_DROP_STAT_TBL_ADDR + PPE_DROP_STAT_TBL_INC * i;
+			ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD);
+		}
 
-	for (i = 0; i < PPE_IPR_PKT_CNT_NUM; i++) {
-		reg = PPE_IPR_PKT_CNT_ADDR + i * PPE_IPR_PKT_CNT_INC;
-		ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_1WORD);
+		break;
+	case PPE_CNT_PARSE:
+		for (i = 0; i < PPE_IPR_PKT_CNT_TBL_ENTRIES; i++) {
+			reg = PPE_IPR_PKT_CNT_TBL_ADDR + i * PPE_IPR_PKT_CNT_TBL_INC;
+			ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_1WORD);
 
-		reg = PPE_TPR_PKT_CNT_ADDR + i * PPE_IPR_PKT_CNT_INC;
-		ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_1WORD);
-	}
+			reg = PPE_TPR_PKT_CNT_TBL_ADDR + i * PPE_TPR_PKT_CNT_TBL_INC;
+			ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_1WORD);
+		}
 
-	for (i = 0; i < PPE_VLAN_CNT_TBL_NUM; i++) {
-		reg = PPE_VLAN_CNT_TBL_ADDR + PPE_VLAN_CNT_TBL_INC * i;
-		ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD);
-	}
+		break;
+	case PPE_CNT_PORT_RX:
+		for (i = 0; i < PPE_PORT_RX_CNT_TBL_ENTRIES; i++) {
+			reg = PPE_PORT_RX_CNT_TBL_ADDR + PPE_PORT_RX_CNT_TBL_INC * i;
+			ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_5WORD);
+		}
 
-	for (i = 0; i < PPE_PRE_L2_CNT_TBL_NUM; i++) {
-		reg = PPE_PRE_L2_CNT_TBL_ADDR + PPE_PRE_L2_CNT_TBL_INC * i;
-		ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_5WORD);
-	}
+		for (i = 0; i < PPE_PHY_PORT_RX_CNT_TBL_ENTRIES; i++) {
+			reg = PPE_PHY_PORT_RX_CNT_TBL_ADDR + PPE_PHY_PORT_RX_CNT_TBL_INC * i;
+			ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_5WORD);
+		}
 
-	for (i = 0; i < PPE_PORT_TX_COUNTER_TBL_NUM; i++) {
-		reg = PPE_PORT_TX_DROP_CNT_TBL_ADDR + PPE_PORT_TX_DROP_CNT_TBL_INC * i;
-		ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD);
+		break;
+	case PPE_CNT_VLAN_RX:
+		for (i = 0; i < PPE_VLAN_CNT_TBL_ENTRIES; i++) {
+			reg = PPE_VLAN_CNT_TBL_ADDR + PPE_VLAN_CNT_TBL_INC * i;
+			ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD);
+		}
 
-		reg = PPE_PORT_TX_COUNTER_TBL_ADDR + PPE_PORT_TX_COUNTER_TBL_INC * i;
-		ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD);
-	}
+		break;
+	case PPE_CNT_L2_FWD:
+		for (i = 0; i < PPE_PRE_L2_CNT_TBL_ENTRIES; i++) {
+			reg = PPE_PRE_L2_CNT_TBL_ADDR + PPE_PRE_L2_CNT_TBL_INC * i;
+			ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_5WORD);
+		}
 
-	for (i = 0; i < PPE_EG_VSI_COUNTER_TBL_NUM; i++) {
-		reg = PPE_EG_VSI_COUNTER_TBL_ADDR + PPE_EG_VSI_COUNTER_TBL_INC * i;
-		ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD);
-	}
+		break;
+	case PPE_CNT_CPU_CODE:
+		for (i = 0; i < PPE_DROP_CPU_CNT_TBL_ENTRIES; i++) {
+			reg = PPE_DROP_CPU_CNT_TBL_ADDR + PPE_DROP_CPU_CNT_TBL_INC * i;
+			ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD);
+		}
 
-	for (i = 0; i < PPE_VPORT_TX_COUNTER_TBL_NUM; i++) {
-		reg = PPE_VPORT_TX_COUNTER_TBL_ADDR + PPE_VPORT_TX_COUNTER_TBL_INC * i;
-		ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD);
+		break;
+	case PPE_CNT_VLAN_TX:
+		for (i = 0; i < PPE_EG_VSI_COUNTER_TBL_ENTRIES; i++) {
+			reg = PPE_EG_VSI_COUNTER_TBL_ADDR + PPE_EG_VSI_COUNTER_TBL_INC * i;
+			ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD);
+		}
 
-		reg = PPE_VPORT_TX_DROP_CNT_TBL_ADDR + PPE_VPORT_TX_DROP_CNT_TBL_INC * i;
-		ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD);
-	}
+		break;
+	case PPE_CNT_PORT_TX:
+		for (i = 0; i < PPE_PORT_TX_COUNTER_TBL_ENTRIES; i++) {
+			reg = PPE_PORT_TX_DROP_CNT_TBL_ADDR + PPE_PORT_TX_DROP_CNT_TBL_INC * i;
+			ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD);
 
-	for (i = 0; i < PPE_QUEUE_TX_COUNTER_TBL_NUM; i++) {
-		reg = PPE_QUEUE_TX_COUNTER_TBL_ADDR + PPE_QUEUE_TX_COUNTER_TBL_INC * i;
-		ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD);
-	}
+			reg = PPE_PORT_TX_COUNTER_TBL_ADDR + PPE_PORT_TX_COUNTER_TBL_INC * i;
+			ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD);
+		}
 
-	ppe_tbl_pkt_cnt_clear(ppe_dev, PPE_EPE_DBG_IN_CNT_ADDR, PPE_PKT_CNT_SIZE_1WORD);
-	ppe_tbl_pkt_cnt_clear(ppe_dev, PPE_EPE_DBG_OUT_CNT_ADDR, PPE_PKT_CNT_SIZE_1WORD);
+		for (i = 0; i < PPE_VPORT_TX_COUNTER_TBL_ENTRIES; i++) {
+			reg = PPE_VPORT_TX_COUNTER_TBL_ADDR + PPE_VPORT_TX_COUNTER_TBL_INC * i;
+			ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD);
 
-	for (i = 0; i < PPE_DROP_CPU_CNT_TBL_NUM; i++) {
-		reg = PPE_DROP_CPU_CNT_TBL_ADDR + PPE_DROP_CPU_CNT_TBL_INC * i;
-		ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD);
-	}
+			reg = PPE_VPORT_TX_DROP_CNT_TBL_ADDR + PPE_VPORT_TX_DROP_CNT_TBL_INC * i;
+			ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD);
+		}
 
-	for (i = 0; i < PPE_PORT_RX_CNT_TBL_NUM; i++) {
-		reg = PPE_PORT_RX_CNT_TBL_ADDR + PPE_PORT_RX_CNT_TBL_INC * i;
-		ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_5WORD);
-	}
+		break;
+	case PPE_CNT_QM:
+		for (i = 0; i < PPE_QUEUE_TX_COUNTER_TBL_ENTRIES; i++) {
+			reg = PPE_QUEUE_TX_COUNTER_TBL_ADDR + PPE_QUEUE_TX_COUNTER_TBL_INC * i;
+			ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_3WORD);
+		}
 
-	for (i = 0; i < PPE_PHY_PORT_RX_CNT_TBL_NUM; i++) {
-		reg = PPE_PHY_PORT_RX_CNT_TBL_ADDR + PPE_PHY_PORT_RX_CNT_TBL_INC * i;
-		ppe_tbl_pkt_cnt_clear(ppe_dev, reg, PPE_PKT_CNT_SIZE_5WORD);
+		break;
+	default:
+		break;
 	}
 
 	return count;
 }
-
-static const struct file_operations ppe_debugfs_packet_counter_fops = {
-	.owner   = THIS_MODULE,
-	.open    = ppe_packet_counter_open,
-	.read    = seq_read,
-	.llseek  = seq_lseek,
-	.release = single_release,
-	.write   = ppe_packet_counter_clear,
-};
+DEFINE_SHOW_STORE_ATTRIBUTE(ppe_packet_counter);
 
 void ppe_debugfs_setup(struct ppe_device *ppe_dev)
 {
-	int ret;
+	struct ppe_debugfs_entry *entry;
+	int i, ret;
 
 	ppe_dev->debugfs_root = debugfs_create_dir("ppe", NULL);
-	debugfs_create_file("packet_counter", 0444,
-			    ppe_dev->debugfs_root,
-			    ppe_dev,
-			    &ppe_debugfs_packet_counter_fops);
+	if (IS_ERR(ppe_dev->debugfs_root))
+		return;
+
+	for (i = 0; i < ARRAY_SIZE(debugfs_files); i++) {
+		entry = devm_kzalloc(ppe_dev->dev, sizeof(*entry), GFP_KERNEL);
+		if (!entry)
+			return;
+
+		entry->ppe = ppe_dev;
+		entry->counter_type = debugfs_files[i].counter_type;
+
+		debugfs_create_file(debugfs_files[i].name, 0444,
+				    ppe_dev->debugfs_root, entry,
+				    &ppe_packet_counter_fops);
+	}
 
 	if (!ppe_dev->debugfs_root) {
 		dev_err(ppe_dev->dev, "Error in PPE debugfs setup\n");
diff --git a/drivers/net/ethernet/qualcomm/ppe/ppe_debugfs.h b/drivers/net/ethernet/qualcomm/ppe/ppe_debugfs.h
index a979fcf9d..ba0a5b3af 100644
--- a/drivers/net/ethernet/qualcomm/ppe/ppe_debugfs.h
+++ b/drivers/net/ethernet/qualcomm/ppe/ppe_debugfs.h
@@ -1,6 +1,6 @@
 /* SPDX-License-Identifier: GPL-2.0-only
  *
- * Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+ * Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
  */
 
 /* PPE debugfs counters setup. */
diff --git a/drivers/net/ethernet/qualcomm/ppe/ppe_port.c b/drivers/net/ethernet/qualcomm/ppe/ppe_port.c
index 3c47d04e8..0df0baae9 100644
--- a/drivers/net/ethernet/qualcomm/ppe/ppe_port.c
+++ b/drivers/net/ethernet/qualcomm/ppe/ppe_port.c
@@ -1,16 +1,17 @@
 // SPDX-License-Identifier: GPL-2.0-only
 /*
- * Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+ * Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
  */
 
 /* PPE Port MAC initialization and PPE port MAC functions. */
 
 #include <linux/clk.h>
+#include <linux/of.h>
 #include <linux/of_net.h>
-#include <linux/pcs/pcs-qcom-ipq-uniphy.h>
+#include <linux/pcs/pcs-qcom-ipq9574.h>
 #include <linux/phylink.h>
-#include <linux/reset.h>
 #include <linux/regmap.h>
+#include <linux/reset.h>
 #include <linux/rtnetlink.h>
 
 #include "edma_port.h"
@@ -117,7 +118,7 @@ enum ppe_xgmib_stats_type {
 	xgmib_rx_broadcast_g,
 	xgmib_rx_multicast_g,
 	xgmib_rx_crc_err,
-	xgmib_rx_runt_err,
+	xgmib_rx_frag_err,
 	xgmib_rx_jabber_err,
 	xgmib_rx_undersize_g,
 	xgmib_rx_oversize_g,
@@ -142,9 +143,9 @@ enum ppe_xgmib_stats_type {
 
 /* PPE port clock and reset name */
 static const char * const ppe_port_clk_rst_name[] = {
-	[PPE_PORT_CLK_RST_MAC] = "port_mac",
-	[PPE_PORT_CLK_RST_RX] = "port_rx",
-	[PPE_PORT_CLK_RST_TX] = "port_tx",
+	[PPE_PORT_CLK_RST_MAC] = "mac",
+	[PPE_PORT_CLK_RST_RX] = "rx",
+	[PPE_PORT_CLK_RST_TX] = "tx",
 };
 
 /* PPE GMAC MIB statistics description information */
@@ -219,7 +220,7 @@ static const struct ppe_mac_mib_info xgmib_info[] = {
 	PPE_MAC_MIB_DESC(8, XGMAC_RXBROAD_G_ADDR, "rx_broadcast_g"),
 	PPE_MAC_MIB_DESC(8, XGMAC_RXMULTI_G_ADDR, "rx_multicast_g"),
 	PPE_MAC_MIB_DESC(8, XGMAC_RXCRC_ERR_ADDR, "rx_crc_err"),
-	PPE_MAC_MIB_DESC(4, XGMAC_RXRUNT_ERR_ADDR, "rx_runt_err"),
+	PPE_MAC_MIB_DESC(4, XGMAC_RXFRAG_ERR_ADDR, "rx_frag_err"),
 	PPE_MAC_MIB_DESC(4, XGMAC_RXJABBER_ERR_ADDR, "rx_jabber_err"),
 	PPE_MAC_MIB_DESC(4, XGMAC_RXUNDERSIZE_G_ADDR, "rx_undersize_g"),
 	PPE_MAC_MIB_DESC(4, XGMAC_RXOVERSIZE_G_ADDR, "rx_oversize_g"),
@@ -257,7 +258,8 @@ static void ppe_port_gmib_update(struct ppe_port *ppe_port)
 
 		ret = regmap_read(ppe_dev->regmap, reg, &val);
 		if (ret) {
-			dev_warn(ppe_dev->dev, "%s: %d\n", __func__, ret);
+			dev_warn(ppe_dev->dev, "PPE port GMIB read fail %d\n",
+				 ret);
 			continue;
 		}
 
@@ -265,8 +267,8 @@ static void ppe_port_gmib_update(struct ppe_port *ppe_port)
 		if (mib->size == 8) {
 			ret = regmap_read(ppe_dev->regmap, reg + 4, &val);
 			if (ret) {
-				dev_warn(ppe_dev->dev, "%s: %d\n",
-					 __func__, ret);
+				dev_warn(ppe_dev->dev,
+					 "PPE port GMIB read fail %d\n", ret);
 				continue;
 			}
 
@@ -288,6 +290,43 @@ static void ppe_port_gmib_stats_poll(struct work_struct *work)
 			      msecs_to_jiffies(PPE_GMIB_POLL_INTERVAL_MS));
 }
 
+/* Start PPE port GMAC MIB statistics polling work */
+static int ppe_port_gmib_work_start(struct ppe_port *ppe_port)
+{
+	struct ppe_device *ppe_dev = ppe_port->ppe_dev;
+
+	if (!ppe_port->gmib_stats) {
+		u64 *gstats;
+		/* Allocate array memory to store GMIB statistics */
+		gstats = devm_kzalloc(ppe_dev->dev,
+				      sizeof(*gstats) * ARRAY_SIZE(gmib_info),
+				      GFP_KERNEL);
+		if (!gstats)
+			return -ENOMEM;
+
+		ppe_port->gmib_stats = gstats;
+
+		/* Init GMIB statistics polling work */
+		spin_lock_init(&ppe_port->gmib_stats_lock);
+		INIT_DELAYED_WORK(&ppe_port->gmib_read,
+				  ppe_port_gmib_stats_poll);
+	}
+
+	/* Start GMIB statistics polling work */
+	schedule_delayed_work(&ppe_port->gmib_read, 0);
+
+	return 0;
+}
+
+/* Stop PPE port GMAC MIB statistics polling work */
+static void ppe_port_gmib_work_stop(struct ppe_port *ppe_port)
+{
+	if (ppe_port->gmib_stats) {
+		/* Stop GMIB statistics polling work */
+		cancel_delayed_work_sync(&ppe_port->gmib_read);
+	}
+}
+
 /* Get the XGMAC MIB counter based on the specific MIB stats type */
 static u64 ppe_port_xgmib_get(struct ppe_port *ppe_port,
 			      enum ppe_xgmib_stats_type xgmib_type)
@@ -304,7 +343,7 @@ static u64 ppe_port_xgmib_get(struct ppe_port *ppe_port,
 
 	ret = regmap_read(ppe_dev->regmap, reg, &val);
 	if (ret) {
-		dev_warn(ppe_dev->dev, "%s: %d\n", __func__, ret);
+		dev_warn(ppe_dev->dev, "PPE port XGMIB read fail %d\n", ret);
 		goto data_return;
 	}
 
@@ -312,7 +351,8 @@ static u64 ppe_port_xgmib_get(struct ppe_port *ppe_port,
 	if (mib->size == 8) {
 		ret = regmap_read(ppe_dev->regmap, reg + 4, &val);
 		if (ret) {
-			dev_warn(ppe_dev->dev, "%s: %d\n", __func__, ret);
+			dev_warn(ppe_dev->dev, "PPE port XGMIB read fail %d\n",
+				 ret);
 			goto data_return;
 		}
 
@@ -338,10 +378,12 @@ int ppe_port_get_sset_count(struct ppe_port *ppe_port, int sset)
 	if (sset != ETH_SS_STATS)
 		return 0;
 
-	if (ppe_port->mac_type == PPE_MAC_TYPE_GMAC)
-		return ARRAY_SIZE(gmib_info);
-	else
-		return ARRAY_SIZE(xgmib_info);
+	/* The MAC type is invisible to the upper interface. The interface
+	 * can switch between GMAC and XGMAC in different interface modes.
+	 * Therefore, the unified XGMIB statistics format is used, and GMIB
+	 * statistics will be merged into the XGMIB statistics.
+	 */
+	return ARRAY_SIZE(xgmib_info);
 }
 
 /**
@@ -361,15 +403,9 @@ void ppe_port_get_strings(struct ppe_port *ppe_port, u32 stringset, u8 *data)
 	if (stringset != ETH_SS_STATS)
 		return;
 
-	if (ppe_port->mac_type == PPE_MAC_TYPE_GMAC) {
-		for (i = 0; i < ARRAY_SIZE(gmib_info); i++)
-			strscpy(data + i * ETH_GSTRING_LEN, gmib_info[i].name,
-				ETH_GSTRING_LEN);
-	} else {
-		for (i = 0; i < ARRAY_SIZE(xgmib_info); i++)
-			strscpy(data + i * ETH_GSTRING_LEN, xgmib_info[i].name,
-				ETH_GSTRING_LEN);
-	}
+	for (i = 0; i < ARRAY_SIZE(xgmib_info); i++)
+		strscpy(data + i * ETH_GSTRING_LEN, xgmib_info[i].name,
+			ETH_GSTRING_LEN);
 }
 
 /**
@@ -385,17 +421,64 @@ void ppe_port_get_ethtool_stats(struct ppe_port *ppe_port, u64 *data)
 {
 	int i;
 
-	if (ppe_port->mac_type == PPE_MAC_TYPE_GMAC) {
+	for (i = 0; i < ARRAY_SIZE(xgmib_info); i++)
+		data[i] = ppe_port_xgmib_get(ppe_port, i);
+
+	/* Merge the GMIB statistics into the XGMIB statistics to show
+	 * the total counters for this interface.
+	 */
+	if (ppe_port->gmib_stats) {
+		u64 *gsrc = ppe_port->gmib_stats;
+
 		spin_lock(&ppe_port->gmib_stats_lock);
 
 		ppe_port_gmib_update(ppe_port);
-		for (i = 0; i < ARRAY_SIZE(gmib_info); i++)
-			data[i] = ppe_port->gmib_stats[i];
+
+		data[xgmib_tx_bytes] += gsrc[gmib_tx_bytes];
+		data[xgmib_tx_frames] += gsrc[gmib_tx_broadcast];
+		data[xgmib_tx_frames] += gsrc[gmib_tx_multicast];
+		data[xgmib_tx_frames] += gsrc[gmib_tx_unicast];
+		data[xgmib_tx_broadcast_g] += gsrc[gmib_tx_broadcast];
+		data[xgmib_tx_multicast_g] += gsrc[gmib_tx_multicast];
+		data[xgmib_tx_pkt64] += gsrc[gmib_tx_pkt64];
+		data[xgmib_tx_pkt65to127] += gsrc[gmib_tx_pkt65to127];
+		data[xgmib_tx_pkt128to255] += gsrc[gmib_tx_pkt128to255];
+		data[xgmib_tx_pkt256to511] += gsrc[gmib_tx_pkt256to511];
+		data[xgmib_tx_pkt512to1023] += gsrc[gmib_tx_pkt512to1023];
+		data[xgmib_tx_pkt1024tomax] += gsrc[gmib_tx_pkt1024to1518];
+		data[xgmib_tx_pkt1024tomax] += gsrc[gmib_tx_pkt1519tomax];
+		data[xgmib_tx_unicast] += gsrc[gmib_tx_unicast];
+		data[xgmib_tx_multicast] += gsrc[gmib_tx_multicast];
+		data[xgmib_tx_broadcast] += gsrc[gmib_tx_broadcast];
+		data[xgmib_tx_underflow_err] += gsrc[gmib_tx_underrun];
+		data[xgmib_tx_bytes_g] += gsrc[gmib_tx_bytes];
+		data[xgmib_tx_frames_g] += gsrc[gmib_tx_broadcast];
+		data[xgmib_tx_frames_g] += gsrc[gmib_tx_multicast];
+		data[xgmib_tx_frames_g] += gsrc[gmib_tx_unicast];
+		data[xgmib_tx_pause] += gsrc[gmib_tx_pause];
+
+		data[xgmib_rx_frames] += gsrc[gmib_rx_broadcast];
+		data[xgmib_rx_frames] += gsrc[gmib_rx_multicast];
+		data[xgmib_rx_frames] += gsrc[gmib_rx_unicast];
+		data[xgmib_rx_bytes] += gsrc[gmib_rx_bytes_g];
+		data[xgmib_rx_bytes] += gsrc[gmib_rx_bytes_b];
+		data[xgmib_rx_bytes_g] += gsrc[gmib_rx_bytes_g];
+		data[xgmib_rx_broadcast_g] += gsrc[gmib_rx_broadcast];
+		data[xgmib_rx_multicast_g] += gsrc[gmib_rx_multicast];
+		data[xgmib_rx_crc_err] += gsrc[gmib_rx_fcserr];
+		data[xgmib_rx_crc_err] += gsrc[gmib_rx_frag];
+		data[xgmib_rx_frag_err] += gsrc[gmib_rx_frag];
+		data[xgmib_rx_pkt64] += gsrc[gmib_rx_pkt64];
+		data[xgmib_rx_pkt65to127] += gsrc[gmib_rx_pkt65to127];
+		data[xgmib_rx_pkt128to255] += gsrc[gmib_rx_pkt128to255];
+		data[xgmib_rx_pkt256to511] += gsrc[gmib_rx_pkt256to511];
+		data[xgmib_rx_pkt512to1023] += gsrc[gmib_rx_pkt512to1023];
+		data[xgmib_rx_pkt1024tomax] += gsrc[gmib_rx_pkt1024to1518];
+		data[xgmib_rx_pkt1024tomax] += gsrc[gmib_rx_pkt1519tomax];
+		data[xgmib_rx_unicast_g] += gsrc[gmib_rx_unicast];
+		data[xgmib_rx_pause] += gsrc[gmib_rx_pause];
 
 		spin_unlock(&ppe_port->gmib_stats_lock);
-	} else {
-		for (i = 0; i < ARRAY_SIZE(xgmib_info); i++)
-			data[i] = ppe_port_xgmib_get(ppe_port, i);
 	}
 }
 
@@ -410,56 +493,57 @@ void ppe_port_get_ethtool_stats(struct ppe_port *ppe_port, u64 *data)
 void ppe_port_get_stats64(struct ppe_port *ppe_port,
 			  struct rtnl_link_stats64 *s)
 {
-	if (ppe_port->mac_type == PPE_MAC_TYPE_GMAC) {
-		u64 *src = ppe_port->gmib_stats;
+	s->multicast = ppe_port_xgmib_get(ppe_port, xgmib_rx_multicast_g);
 
-		spin_lock(&ppe_port->gmib_stats_lock);
-
-		ppe_port_gmib_update(ppe_port);
+	s->rx_packets = s->multicast;
+	s->rx_packets += ppe_port_xgmib_get(ppe_port, xgmib_rx_unicast_g);
+	s->rx_packets += ppe_port_xgmib_get(ppe_port, xgmib_rx_broadcast_g);
 
-		s->rx_packets = src[gmib_rx_unicast] +
-			src[gmib_rx_broadcast] + src[gmib_rx_multicast];
+	s->tx_packets = ppe_port_xgmib_get(ppe_port, xgmib_tx_frames);
+	s->rx_bytes = ppe_port_xgmib_get(ppe_port, xgmib_rx_bytes);
+	s->tx_bytes = ppe_port_xgmib_get(ppe_port, xgmib_tx_bytes);
 
-		s->tx_packets = src[gmib_tx_unicast] +
-			src[gmib_tx_broadcast] + src[gmib_tx_multicast];
+	s->rx_crc_errors = ppe_port_xgmib_get(ppe_port, xgmib_rx_crc_err);
+	s->rx_fifo_errors = ppe_port_xgmib_get(ppe_port,
+					       xgmib_rx_fifo_overflow);
 
-		s->rx_bytes = src[gmib_rx_bytes_g];
-		s->tx_bytes = src[gmib_tx_bytes];
-		s->multicast = src[gmib_rx_multicast];
+	s->rx_length_errors = ppe_port_xgmib_get(ppe_port, xgmib_rx_len_err);
+	s->rx_errors = s->rx_crc_errors +
+		s->rx_fifo_errors + s->rx_length_errors;
+	s->rx_dropped = s->rx_errors;
 
-		s->rx_crc_errors = src[gmib_rx_fcserr] + src[gmib_rx_frag];
-		s->rx_frame_errors = src[gmib_rx_alignerr];
-		s->rx_errors = s->rx_crc_errors + s->rx_frame_errors;
-		s->rx_dropped = src[gmib_rx_toolong] + s->rx_errors;
+	s->tx_fifo_errors = ppe_port_xgmib_get(ppe_port,
+					       xgmib_tx_underflow_err);
+	s->tx_errors = s->tx_packets -
+		ppe_port_xgmib_get(ppe_port, xgmib_tx_frames_g);
 
-		s->tx_fifo_errors = src[gmib_tx_underrun];
-		s->tx_aborted_errors = src[gmib_tx_abortcol];
-		s->tx_errors = s->tx_fifo_errors + s->tx_aborted_errors;
-		s->collisions = src[gmib_tx_collisions];
+	if (ppe_port->gmib_stats) {
+		u64 *gsrc = ppe_port->gmib_stats;
+		u64 temp;
 
-		spin_unlock(&ppe_port->gmib_stats_lock);
-	} else {
-		s->multicast = ppe_port_xgmib_get(ppe_port, xgmib_rx_multicast_g);
-
-		s->rx_packets = s->multicast;
-		s->rx_packets += ppe_port_xgmib_get(ppe_port, xgmib_rx_unicast_g);
-		s->rx_packets += ppe_port_xgmib_get(ppe_port, xgmib_rx_broadcast_g);
-
-		s->tx_packets = ppe_port_xgmib_get(ppe_port, xgmib_tx_frames);
-		s->rx_bytes = ppe_port_xgmib_get(ppe_port, xgmib_rx_bytes);
-		s->tx_bytes = ppe_port_xgmib_get(ppe_port, xgmib_tx_bytes);
+		spin_lock(&ppe_port->gmib_stats_lock);
 
-		s->rx_crc_errors = ppe_port_xgmib_get(ppe_port, xgmib_rx_crc_err);
-		s->rx_fifo_errors = ppe_port_xgmib_get(ppe_port, xgmib_rx_fifo_overflow);
+		ppe_port_gmib_update(ppe_port);
 
-		s->rx_length_errors = ppe_port_xgmib_get(ppe_port, xgmib_rx_len_err);
-		s->rx_errors = s->rx_crc_errors +
-			s->rx_fifo_errors + s->rx_length_errors;
-		s->rx_dropped = s->rx_errors;
+		s->multicast += gsrc[gmib_rx_multicast];
+		s->rx_packets += gsrc[gmib_rx_unicast];
+		s->rx_packets += gsrc[gmib_rx_broadcast];
+		s->rx_packets += gsrc[gmib_rx_multicast];
+		s->tx_packets += gsrc[gmib_tx_unicast];
+		s->tx_packets += gsrc[gmib_tx_broadcast];
+		s->tx_packets += gsrc[gmib_tx_multicast];
+		s->rx_bytes += gsrc[gmib_rx_bytes_g];
+		s->tx_bytes += gsrc[gmib_tx_bytes];
+		temp = gsrc[gmib_rx_fcserr] + gsrc[gmib_rx_frag];
+		s->rx_crc_errors += temp;
+		temp += gsrc[gmib_rx_alignerr];
+		s->rx_errors += temp;
+		s->rx_dropped += temp;
+		s->tx_fifo_errors += gsrc[gmib_tx_underrun];
+		s->tx_errors += gsrc[gmib_tx_underrun];
+		s->tx_errors += gsrc[gmib_tx_abortcol];
 
-		s->tx_fifo_errors = ppe_port_xgmib_get(ppe_port, xgmib_tx_underflow_err);
-		s->tx_errors = s->tx_packets -
-			ppe_port_xgmib_get(ppe_port, xgmib_tx_frames_g);
+		spin_unlock(&ppe_port->gmib_stats_lock);
 	}
 }
 
@@ -479,73 +563,37 @@ int ppe_port_set_mac_address(struct ppe_port *ppe_port, const u8 *addr)
 	u32 reg, val;
 	int ret;
 
-	if (ppe_port->mac_type == PPE_MAC_TYPE_GMAC) {
-		reg = PPE_PORT_GMAC_ADDR(port);
-		val = (addr[5] << 8) | addr[4];
-		ret = regmap_write(ppe_dev->regmap, reg + GMAC_GOL_ADDR0_ADDR, val);
-		if (ret)
-			return ret;
-
-		val = (addr[0] << 24) | (addr[1] << 16) |
-		      (addr[2] << 8) | addr[3];
-		ret = regmap_write(ppe_dev->regmap, reg + GMAC_GOL_ADDR1_ADDR, val);
-		if (ret)
-			return ret;
-	} else {
-		reg = PPE_PORT_XGMAC_ADDR(port);
-		val = (addr[5] << 8) | addr[4] | XGMAC_ADDR_EN;
-		ret = regmap_write(ppe_dev->regmap, reg + XGMAC_ADDR0_H_ADDR, val);
-		if (ret)
-			return ret;
-
-		val = (addr[3] << 24) | (addr[2] << 16) |
-		      (addr[1] << 8) | addr[0];
-		ret = regmap_write(ppe_dev->regmap, reg + XGMAC_ADDR0_L_ADDR, val);
-		if (ret)
-			return ret;
-	}
-
-	return 0;
-}
-
-/**
- * ppe_port_set_mac_eee() - Set EEE configuration for PPE port MAC
- * @ppe_port: PPE port
- * @eee: EEE settings
- *
- * Description: Set port MAC EEE settings for the given PPE port.
- *
- * Return: 0 upon success or a negative error upon failure.
- */
-int ppe_port_set_mac_eee(struct ppe_port *ppe_port, struct ethtool_eee *eee)
-{
-	struct ppe_device *ppe_dev = ppe_port->ppe_dev;
-	int port = ppe_port->port_id;
-	u32 val;
-	int ret;
-
-	ret = regmap_read(ppe_dev->regmap, PPE_LPI_EN_ADDR, &val);
+	/* Set address to GMAC */
+	reg = PPE_PORT_GMAC_ADDR(port);
+	val = (addr[5] << 8) | addr[4];
+	ret = regmap_write(ppe_dev->regmap, reg + GMAC_GOL_ADDR0_ADDR, val);
 	if (ret)
 		return ret;
 
-	if (eee->tx_lpi_enabled)
-		val |= PPE_LPI_PORT_EN(port);
-	else
-		val &= ~PPE_LPI_PORT_EN(port);
+	val = (addr[0] << 24) | (addr[1] << 16) | (addr[2] << 8) | addr[3];
+	ret = regmap_write(ppe_dev->regmap, reg + GMAC_GOL_ADDR1_ADDR, val);
+	if (ret)
+		return ret;
 
-	ret = regmap_write(ppe_dev->regmap, PPE_LPI_EN_ADDR, val);
+	/* Set address to XGMAC */
+	reg = PPE_PORT_XGMAC_ADDR(port);
+	val = (addr[5] << 8) | addr[4] | XGMAC_ADDR_EN;
+	ret = regmap_write(ppe_dev->regmap, reg + XGMAC_ADDR0_H_ADDR, val);
+	if (ret)
+		return ret;
 
-	return ret;
+	val = (addr[3] << 24) | (addr[2] << 16) | (addr[1] << 8) | addr[0];
+	return regmap_write(ppe_dev->regmap, reg + XGMAC_ADDR0_L_ADDR, val);
 }
 
 /**
- * ppe_port_set_maxframe() - Set port maximum frame size
+ * ppe_port_set_maxframe() - Set maximum frame size including MTU and MRU of port
  * @ppe_port: PPE port structure
- * @maxframe_size: Maximum frame size supported by PPE port
+ * @maxframe_size: Maximum frame size configured to the PPE port
  *
- * Description: Set MTU of network interface specified by @ppe_port.
+ * Set maximum frame size of network interface specified by @ppe_port.
  *
- * Return: 0 upon success or a negative error upon failure.
+ * Return: 0 on success, negative error code on failure.
  */
 int ppe_port_set_maxframe(struct ppe_port *ppe_port, int maxframe_size)
 {
@@ -585,42 +633,23 @@ int ppe_port_set_maxframe(struct ppe_port *ppe_port, int maxframe_size)
 /* PPE port and MAC reset */
 static int ppe_port_mac_reset(struct ppe_port *ppe_port)
 {
-	struct ppe_device *ppe_dev = ppe_port->ppe_dev;
-	int ret;
-
-	ret = reset_control_assert(ppe_port->rstcs[PPE_PORT_CLK_RST_MAC]);
-	if (ret)
-		goto error;
-
-	ret = reset_control_assert(ppe_port->rstcs[PPE_PORT_CLK_RST_RX]);
-	if (ret)
-		goto error;
-
-	ret = reset_control_assert(ppe_port->rstcs[PPE_PORT_CLK_RST_TX]);
-	if (ret)
-		goto error;
-
-	/* 150ms delay is required by hardware to reset PPE port and MAC */
-	msleep(150);
+	int ret, i;
 
-	ret = reset_control_deassert(ppe_port->rstcs[PPE_PORT_CLK_RST_MAC]);
-	if (ret)
-		goto error;
+	for (i = PPE_PORT_CLK_RST_MAC; i < PPE_PORT_CLK_RST_MAX; i++) {
+		ret = reset_control_assert(ppe_port->rstcs[PPE_PORT_CLK_RST_MAC]);
+		if (ret)
+			return ret;
+	}
 
-	ret = reset_control_deassert(ppe_port->rstcs[PPE_PORT_CLK_RST_RX]);
-	if (ret)
-		goto error;
+	fsleep(10000);
 
-	ret = reset_control_deassert(ppe_port->rstcs[PPE_PORT_CLK_RST_TX]);
-	if (ret)
-		goto error;
-
-	return ret;
+	for (i = PPE_PORT_CLK_RST_MAC; i < PPE_PORT_CLK_RST_MAX; i++) {
+		ret = reset_control_deassert(ppe_port->rstcs[PPE_PORT_CLK_RST_MAC]);
+		if (ret)
+			return ret;
+	}
 
-error:
-	dev_err(ppe_dev->dev, "%s: port %d reset fail %d\n",
-		__func__, ppe_port->port_id, ret);
-	return ret;
+	return 0;
 }
 
 /* PPE port MAC configuration for phylink */
@@ -643,31 +672,28 @@ static void ppe_port_mac_config(struct phylink_config *config,
 	case PHY_INTERFACE_MODE_10G_QXGMII:
 		mac_type = PPE_MAC_TYPE_XGMAC;
 		break;
+	case PHY_INTERFACE_MODE_SGMII:
 	case PHY_INTERFACE_MODE_QSGMII:
 	case PHY_INTERFACE_MODE_PSGMII:
-	case PHY_INTERFACE_MODE_SGMII:
 	case PHY_INTERFACE_MODE_1000BASEX:
 		mac_type = PPE_MAC_TYPE_GMAC;
 		break;
 	default:
-		dev_err(ppe_dev->dev, "%s: Unsupport interface %s\n",
-			__func__, phy_modes(state->interface));
+		dev_err(ppe_dev->dev, "Unsupported interface %s\n",
+			phy_modes(state->interface));
 		return;
 	}
 
-	/* Reset Port MAC for GMAC */
-	if (mac_type == PPE_MAC_TYPE_GMAC) {
-		ret = ppe_port_mac_reset(ppe_port);
-		if (ret)
-			goto err_mac_config;
-	}
+	/* Reset Port MAC */
+	ret = ppe_port_mac_reset(ppe_port);
+	if (ret)
+		goto err_mac_config;
 
 	/* Port mux to select GMAC or XGMAC */
 	mask = PPE_PORT_SEL_XGMAC(port);
 	val = mac_type == PPE_MAC_TYPE_GMAC ? 0 : mask;
 	ret = regmap_update_bits(ppe_dev->regmap,
-				 PPE_PORT_MUX_CTRL_ADDR,
-				 mask, val);
+				 PPE_PORT_MUX_CTRL_ADDR, mask, val);
 	if (ret)
 		goto err_mac_config;
 
@@ -676,8 +702,111 @@ static void ppe_port_mac_config(struct phylink_config *config,
 	return;
 
 err_mac_config:
-	dev_err(ppe_dev->dev, "%s: port %d MAC config fail %d\n",
-		__func__, port, ret);
+	dev_err(ppe_dev->dev, "PPE port %d MAC config fail %d\n", port, ret);
+}
+
+/* Get port MAC speed clock rate for GMII/GMII+ interface */
+static unsigned long ppe_port_mac_clock_rate_get_gmii(int speed)
+{
+	unsigned long rate = 0;
+
+	switch (speed) {
+	case SPEED_2500:
+		rate = 312500000;
+		break;
+	case SPEED_1000:
+		rate = 125000000;
+		break;
+	case SPEED_100:
+		rate = 25000000;
+		break;
+	case SPEED_10:
+		rate = 2500000;
+		break;
+	default:
+		break;
+	}
+
+	return rate;
+}
+
+/* Get port MAC speed clock rate for XGMII interface */
+static unsigned long ppe_port_mac_clock_rate_get_xgmii(int speed)
+{
+	unsigned long rate = 0;
+
+	switch (speed) {
+	case SPEED_10000:
+		rate = 312500000;
+		break;
+	case SPEED_5000:
+		rate = 156250000;
+		break;
+	case SPEED_2500:
+		rate = 78125000;
+		break;
+	case SPEED_1000:
+		rate = 125000000;
+		break;
+	case SPEED_100:
+		rate = 12500000;
+		break;
+	case SPEED_10:
+		rate = 1250000;
+		break;
+	default:
+		break;
+	}
+
+	return rate;
+}
+
+/* Set PPE port MAC speed clock to appropriate rate */
+static int ppe_port_mac_speed_clock_rate_set(struct ppe_port *ppe_port,
+					     phy_interface_t interface,
+					     int speed)
+{
+	struct ppe_device *ppe_dev = ppe_port->ppe_dev;
+	struct device *device = ppe_dev->dev;
+	unsigned long rate;
+	int ret;
+
+	switch (interface) {
+	case PHY_INTERFACE_MODE_SGMII:
+	case PHY_INTERFACE_MODE_QSGMII:
+	case PHY_INTERFACE_MODE_PSGMII:
+	case PHY_INTERFACE_MODE_2500BASEX:
+		rate = ppe_port_mac_clock_rate_get_gmii(speed);
+		break;
+	case PHY_INTERFACE_MODE_USXGMII:
+	case PHY_INTERFACE_MODE_10GBASER:
+	case PHY_INTERFACE_MODE_10G_QXGMII:
+		rate = ppe_port_mac_clock_rate_get_xgmii(speed);
+		break;
+	default:
+		dev_err(device,
+			"Unsupported interface %s\n", phy_modes(interface));
+		return -EOPNOTSUPP;
+	}
+
+	if (rate == 0) {
+		dev_err(device, "Invalid port speed clock rate\n");
+		return -EINVAL;
+	}
+
+	ret = clk_set_rate(ppe_port->clks[PPE_PORT_CLK_RST_RX], rate);
+	if (ret) {
+		dev_err(device, "Failed to set PPE port RX clock rate\n");
+		return ret;
+	}
+
+	ret = clk_set_rate(ppe_port->clks[PPE_PORT_CLK_RST_TX], rate);
+	if (ret) {
+		dev_err(device, "Failed to set PPE port TX clock rate\n");
+		return ret;
+	}
+
+	return 0;
 }
 
 /* PPE port GMAC link up configuration */
@@ -688,6 +817,11 @@ static int ppe_port_gmac_link_up(struct ppe_port *ppe_port, int speed,
 	int ret, port = ppe_port->port_id;
 	u32 reg, val;
 
+	/* Start GMAC MIB statistics polling work task */
+	ret = ppe_port_gmib_work_start(ppe_port);
+	if (ret)
+		return ret;
+
 	/* Set GMAC speed */
 	switch (speed) {
 	case SPEED_1000:
@@ -700,8 +834,8 @@ static int ppe_port_gmac_link_up(struct ppe_port *ppe_port, int speed,
 		val = GMAC_SPEED_10;
 		break;
 	default:
-		dev_err(ppe_dev->dev, "%s: Invalid GMAC speed %s\n",
-			__func__, phy_speed_to_str(speed));
+		dev_err(ppe_dev->dev, "Invalid GMAC speed %s\n",
+			phy_speed_to_str(speed));
 		return -EINVAL;
 	}
 
@@ -720,10 +854,8 @@ static int ppe_port_gmac_link_up(struct ppe_port *ppe_port, int speed,
 	if (rx_pause)
 		val |= GMAC_RXFCEN;
 
-	ret = regmap_update_bits(ppe_dev->regmap, reg + GMAC_ENABLE_ADDR,
-				 GMAC_ENABLE_ALL, val);
-
-	return ret;
+	return regmap_update_bits(ppe_dev->regmap, reg + GMAC_ENABLE_ADDR,
+				  GMAC_ENABLE_ALL, val);
 }
 
 /* PPE port XGMAC link up configuration */
@@ -764,8 +896,8 @@ static int ppe_port_xgmac_link_up(struct ppe_port *ppe_port,
 		val = XGMAC_SPEED_10;
 		break;
 	default:
-		dev_err(ppe_dev->dev, "%s: Invalid XGMAC speed %s\n",
-			__func__, phy_speed_to_str(speed));
+		dev_err(ppe_dev->dev, "Invalid XGMAC speed %s\n",
+			phy_speed_to_str(speed));
 		return -EINVAL;
 	}
 
@@ -792,10 +924,8 @@ static int ppe_port_xgmac_link_up(struct ppe_port *ppe_port,
 		return ret;
 
 	/* Enable XGMAC RX*/
-	ret = regmap_update_bits(ppe_dev->regmap, reg + XGMAC_RX_CONFIG_ADDR,
-				 XGMAC_RXEN, XGMAC_RXEN);
-
-	return ret;
+	return regmap_set_bits(ppe_dev->regmap, reg + XGMAC_RX_CONFIG_ADDR,
+			       XGMAC_RXEN);
 }
 
 /* PPE port MAC link up configuration for phylink */
@@ -813,9 +943,12 @@ static void ppe_port_mac_link_up(struct phylink_config *config,
 	int ret, port = ppe_port->port_id;
 	u32 reg, val;
 
-	/* Start GMIB statistics polling */
-	schedule_delayed_work(&ppe_port->gmib_read, 0);
+	/* Set PPE port MAC speed clock */
+	ret = ppe_port_mac_speed_clock_rate_set(ppe_port, interface, speed);
+	if (ret)
+		goto err_port_mac_link_up;
 
+	/* Configure PPE MAC according current speed */
 	if (mac_type == PPE_MAC_TYPE_GMAC)
 		ret = ppe_port_gmac_link_up(ppe_port,
 					    speed, duplex, tx_pause, rx_pause);
@@ -836,17 +969,47 @@ static void ppe_port_mac_link_up(struct phylink_config *config,
 
 	/* Enable PPE port TX */
 	reg = PPE_PORT_BRIDGE_CTRL_ADDR + PPE_PORT_BRIDGE_CTRL_INC * port;
-	ret = regmap_update_bits(ppe_dev->regmap, reg,
-				 PPE_PORT_BRIDGE_TXMAC_EN,
-				 PPE_PORT_BRIDGE_TXMAC_EN);
+	ret = regmap_set_bits(ppe_dev->regmap, reg, PPE_PORT_BRIDGE_TXMAC_EN);
 	if (ret)
 		goto err_port_mac_link_up;
 
 	return;
 
 err_port_mac_link_up:
-	dev_err(ppe_dev->dev, "%s: port %d link up fail %d\n",
-		__func__, port, ret);
+	dev_err(ppe_dev->dev, "PPE port %d link up fail %d\n", port, ret);
+}
+
+/* PPE port GMAC link down configuration */
+static int ppe_port_gmac_link_down(struct ppe_port *ppe_port)
+{
+	struct ppe_device *ppe_dev = ppe_port->ppe_dev;
+	int port = ppe_port->port_id;
+	u32 reg;
+
+	/* Stop GMAC MIB statistics polling work task */
+	ppe_port_gmib_work_stop(ppe_port);
+
+	/* Disable GMAC RX and TX */
+	reg = PPE_PORT_GMAC_ADDR(port) + GMAC_ENABLE_ADDR;
+	return regmap_clear_bits(ppe_dev->regmap, reg, GMAC_TRXEN);
+}
+
+/* PPE port XGMAC link down configuration */
+static int ppe_port_xgmac_link_down(struct ppe_port *ppe_port)
+{
+	struct ppe_device *ppe_dev = ppe_port->ppe_dev;
+	int ret, port = ppe_port->port_id;
+	u32 reg;
+
+	/* Disable XGMAC RX and TX */
+	reg = PPE_PORT_XGMAC_ADDR(port);
+	ret = regmap_clear_bits(ppe_dev->regmap,
+				reg + XGMAC_RX_CONFIG_ADDR, XGMAC_RXEN);
+	if (ret)
+		return ret;
+
+	return regmap_clear_bits(ppe_dev->regmap,
+				 reg + XGMAC_TX_CONFIG_ADDR, XGMAC_TXEN);
 }
 
 /* PPE port MAC link down configuration for phylink */
@@ -861,48 +1024,28 @@ static void ppe_port_mac_link_down(struct phylink_config *config,
 	int ret, port = ppe_port->port_id;
 	u32 reg;
 
-	/* Stop GMIB statistics polling */
-	cancel_delayed_work_sync(&ppe_port->gmib_read);
-
 	/* Disable PPE port TX */
 	reg = PPE_PORT_BRIDGE_CTRL_ADDR + PPE_PORT_BRIDGE_CTRL_INC * port;
-	ret = regmap_update_bits(ppe_dev->regmap, reg,
-				 PPE_PORT_BRIDGE_TXMAC_EN, 0);
+	ret = regmap_clear_bits(ppe_dev->regmap, reg, PPE_PORT_BRIDGE_TXMAC_EN);
 	if (ret)
 		goto err_port_mac_link_down;
 
-	/* Disable PPE MAC */
-	if (mac_type == PPE_MAC_TYPE_GMAC) {
-		reg = PPE_PORT_GMAC_ADDR(port) + GMAC_ENABLE_ADDR;
-		ret = regmap_update_bits(ppe_dev->regmap, reg, GMAC_TRXEN, 0);
-		if (ret)
-			goto err_port_mac_link_down;
-	} else {
-		reg = PPE_PORT_XGMAC_ADDR(port);
-		ret = regmap_update_bits(ppe_dev->regmap,
-					 reg + XGMAC_RX_CONFIG_ADDR,
-					 XGMAC_RXEN, 0);
-		if (ret)
-			goto err_port_mac_link_down;
-
-		ret = regmap_update_bits(ppe_dev->regmap,
-					 reg + XGMAC_TX_CONFIG_ADDR,
-					 XGMAC_TXEN, 0);
-		if (ret)
-			goto err_port_mac_link_down;
-	}
+	if (mac_type == PPE_MAC_TYPE_GMAC)
+		ret = ppe_port_gmac_link_down(ppe_port);
+	else
+		ret = ppe_port_xgmac_link_down(ppe_port);
+	if (ret)
+		goto err_port_mac_link_down;
 
 	return;
 
 err_port_mac_link_down:
-	dev_err(ppe_dev->dev, "%s: port %d link down fail %d\n",
-		__func__, port, ret);
+	dev_err(ppe_dev->dev, "PPE port %d link down fail %d\n", port, ret);
 }
 
 /* PPE port MAC PCS selection for phylink */
-static
-struct phylink_pcs *ppe_port_mac_select_pcs(struct phylink_config *config,
-					    phy_interface_t interface)
+static struct phylink_pcs *ppe_port_mac_select_pcs(struct phylink_config *config,
+						   phy_interface_t interface)
 {
 	struct ppe_port *ppe_port = container_of(config, struct ppe_port,
 						 phylink_config);
@@ -920,8 +1063,8 @@ struct phylink_pcs *ppe_port_mac_select_pcs(struct phylink_config *config,
 					 PPE_PORT_MUX_CTRL_ADDR,
 					 PPE_PORT5_SEL_PCS1, val);
 		if (ret) {
-			dev_err(ppe_dev->dev, "%s: port5 select PCS fail %d\n",
-				__func__, ret);
+			dev_err(ppe_dev->dev, "PPE port5 select PCS fail %d\n",
+				ret);
 			return NULL;
 		}
 	}
@@ -936,6 +1079,17 @@ static const struct phylink_mac_ops ppe_phylink_ops = {
 	.mac_select_pcs = ppe_port_mac_select_pcs,
 };
 
+static const phy_interface_t mac_interfaces[] = {
+	PHY_INTERFACE_MODE_SGMII,
+	PHY_INTERFACE_MODE_QSGMII,
+	PHY_INTERFACE_MODE_PSGMII,
+	PHY_INTERFACE_MODE_1000BASEX,
+	PHY_INTERFACE_MODE_2500BASEX,
+	PHY_INTERFACE_MODE_USXGMII,
+	PHY_INTERFACE_MODE_10GBASER,
+	PHY_INTERFACE_MODE_10G_QXGMII,
+};
+
 /**
  * ppe_port_phylink_setup() - Set phylink instance for the given PPE port
  * @ppe_port: PPE port
@@ -950,18 +1104,18 @@ int ppe_port_phylink_setup(struct ppe_port *ppe_port, struct net_device *netdev)
 {
 	struct ppe_device *ppe_dev = ppe_port->ppe_dev;
 	struct device_node *pcs_node;
-	int ret;
+	int i, ret;
 
-	/* Create PCS */
+	/* Get PCS instance */
 	pcs_node = of_parse_phandle(ppe_port->np, "pcs-handle", 0);
 	if (!pcs_node)
 		return -ENODEV;
 
-	ppe_port->pcs = ipq_unipcs_create(pcs_node);
+	ppe_port->pcs = ipq_pcs_get(pcs_node);
 	of_node_put(pcs_node);
 	if (IS_ERR(ppe_port->pcs)) {
-		dev_err(ppe_dev->dev, "%s: port %d failed to create PCS\n",
-			__func__, ppe_port->port_id);
+		dev_err(ppe_dev->dev, "PPE port %d failed to create PCS\n",
+			ppe_port->port_id);
 		return PTR_ERR(ppe_port->pcs);
 	}
 
@@ -971,22 +1125,10 @@ int ppe_port_phylink_setup(struct ppe_port *ppe_port, struct net_device *netdev)
 	ppe_port->phylink_config.mac_capabilities = MAC_ASYM_PAUSE |
 		MAC_SYM_PAUSE | MAC_10 | MAC_100 | MAC_1000 |
 		MAC_2500FD | MAC_5000FD | MAC_10000FD;
-	__set_bit(PHY_INTERFACE_MODE_QSGMII,
-		  ppe_port->phylink_config.supported_interfaces);
-	__set_bit(PHY_INTERFACE_MODE_PSGMII,
-		  ppe_port->phylink_config.supported_interfaces);
-	__set_bit(PHY_INTERFACE_MODE_SGMII,
-		  ppe_port->phylink_config.supported_interfaces);
-	__set_bit(PHY_INTERFACE_MODE_1000BASEX,
-		  ppe_port->phylink_config.supported_interfaces);
-	__set_bit(PHY_INTERFACE_MODE_2500BASEX,
-		  ppe_port->phylink_config.supported_interfaces);
-	__set_bit(PHY_INTERFACE_MODE_USXGMII,
-		  ppe_port->phylink_config.supported_interfaces);
-	__set_bit(PHY_INTERFACE_MODE_10GBASER,
-		  ppe_port->phylink_config.supported_interfaces);
-	__set_bit(PHY_INTERFACE_MODE_10G_QXGMII,
-		  ppe_port->phylink_config.supported_interfaces);
+
+	for (i = 0; i < ARRAY_SIZE(mac_interfaces); i++)
+		__set_bit(mac_interfaces[i],
+			  ppe_port->phylink_config.supported_interfaces);
 
 	/* Create phylink */
 	ppe_port->phylink = phylink_create(&ppe_port->phylink_config,
@@ -994,8 +1136,8 @@ int ppe_port_phylink_setup(struct ppe_port *ppe_port, struct net_device *netdev)
 					   ppe_port->interface,
 					   &ppe_phylink_ops);
 	if (IS_ERR(ppe_port->phylink)) {
-		dev_err(ppe_dev->dev, "%s: port %d failed to create phylink\n",
-			__func__, ppe_port->port_id);
+		dev_err(ppe_dev->dev, "PPE port %d failed to create phylink\n",
+			ppe_port->port_id);
 		ret = PTR_ERR(ppe_port->phylink);
 		goto err_free_pcs;
 	}
@@ -1003,8 +1145,8 @@ int ppe_port_phylink_setup(struct ppe_port *ppe_port, struct net_device *netdev)
 	/* Connect phylink */
 	ret = phylink_of_phy_connect(ppe_port->phylink, ppe_port->np, 0);
 	if (ret) {
-		dev_err(ppe_dev->dev, "%s: port %d failed to connect phylink\n",
-			__func__, ppe_port->port_id);
+		dev_err(ppe_dev->dev, "PPE port %d failed to connect phylink\n",
+			ppe_port->port_id);
 		goto err_free_phylink;
 	}
 
@@ -1014,7 +1156,7 @@ int ppe_port_phylink_setup(struct ppe_port *ppe_port, struct net_device *netdev)
 	phylink_destroy(ppe_port->phylink);
 	ppe_port->phylink = NULL;
 err_free_pcs:
-	ipq_unipcs_destroy(ppe_port->pcs);
+	ipq_pcs_put(ppe_port->pcs);
 	ppe_port->pcs = NULL;
 	return ret;
 }
@@ -1037,9 +1179,9 @@ void ppe_port_phylink_destroy(struct ppe_port *ppe_port)
 		ppe_port->phylink = NULL;
 	}
 
-	/* Destroy PCS */
+	/* Release PCS instance */
 	if (ppe_port->pcs) {
-		ipq_unipcs_destroy(ppe_port->pcs);
+		ipq_pcs_put(ppe_port->pcs);
 		ppe_port->pcs = NULL;
 	}
 }
@@ -1050,7 +1192,7 @@ static int ppe_port_clock_init(struct ppe_port *ppe_port)
 	struct device_node *port_node = ppe_port->np;
 	struct reset_control *rstc;
 	struct clk *clk;
-	int i, j, ret;
+	int i, ret;
 
 	for (i = 0; i < PPE_PORT_CLK_RST_MAX; i++) {
 		/* Get PPE port resets which will be used to reset PPE
@@ -1084,10 +1226,11 @@ static int ppe_port_clock_init(struct ppe_port *ppe_port)
 err_clk_get:
 	reset_control_put(rstc);
 err_rst:
-	for (j = 0; j < i; j++) {
-		clk_disable_unprepare(ppe_port->clks[j]);
-		clk_put(ppe_port->clks[j]);
-		reset_control_put(ppe_port->rstcs[j]);
+	while (i) {
+		i--;
+		clk_disable_unprepare(ppe_port->clks[i]);
+		clk_put(ppe_port->clks[i]);
+		reset_control_put(ppe_port->rstcs[i]);
 	}
 
 	return ret;
@@ -1114,12 +1257,12 @@ static int ppe_port_mac_hw_init(struct ppe_port *ppe_port)
 
 	/* GMAC RX and TX are initialized as disabled */
 	reg = PPE_PORT_GMAC_ADDR(port);
-	ret = regmap_update_bits(ppe_dev->regmap,
-				 reg + GMAC_ENABLE_ADDR, GMAC_TRXEN, 0);
+	ret = regmap_clear_bits(ppe_dev->regmap,
+				reg + GMAC_ENABLE_ADDR, GMAC_TRXEN);
 	if (ret)
 		return ret;
 
-	/* GMAC max frame size configuration */
+	/* GMAC jumbo frame size and max frame size configuration */
 	val = FIELD_PREP(GMAC_JUMBO_SIZE_M, PPE_PORT_MAC_MAX_FRAME_SIZE);
 	ret = regmap_update_bits(ppe_dev->regmap, reg + GMAC_JUMBO_SIZE_ADDR,
 				 GMAC_JUMBO_SIZE_M, val);
@@ -1128,13 +1271,13 @@ static int ppe_port_mac_hw_init(struct ppe_port *ppe_port)
 
 	val = FIELD_PREP(GMAC_MAXFRAME_SIZE_M, PPE_PORT_MAC_MAX_FRAME_SIZE);
 	val |= FIELD_PREP(GMAC_TX_THD_M, 0x1);
-	ret = regmap_update_bits(ppe_dev->regmap, reg + GMAC_CTRL_ADDR,
+	ret = regmap_update_bits(ppe_dev->regmap, reg + GMAC_CTRL0_ADDR,
 				 GMAC_CTRL_MASK, val);
 	if (ret)
 		return ret;
 
 	val = FIELD_PREP(GMAC_HIGH_IPG_M, 0xc);
-	ret = regmap_update_bits(ppe_dev->regmap, reg + GMAC_DBG_CTRL_ADDR,
+	ret = regmap_update_bits(ppe_dev->regmap, reg + GMAC_CTRL1_ADDR,
 				 GMAC_HIGH_IPG_M, val);
 	if (ret)
 		return ret;
@@ -1142,13 +1285,13 @@ static int ppe_port_mac_hw_init(struct ppe_port *ppe_port)
 	/* Enable and reset GMAC MIB counters and set as read clear
 	 * mode, the GMAC MIB counters will be cleared after reading.
 	 */
-	ret = regmap_update_bits(ppe_dev->regmap, reg + GMAC_MIB_CTRL_ADDR,
-				 GMAC_MIB_CTRL_MASK, GMAC_MIB_CTRL_MASK);
+	ret = regmap_set_bits(ppe_dev->regmap, reg + GMAC_MIB_CTRL_ADDR,
+			      GMAC_MIB_CTRL_MASK);
 	if (ret)
 		return ret;
 
-	ret = regmap_update_bits(ppe_dev->regmap, reg + GMAC_MIB_CTRL_ADDR,
-				 GMAC_MIB_RST, 0);
+	ret = regmap_clear_bits(ppe_dev->regmap, reg + GMAC_MIB_CTRL_ADDR,
+				GMAC_MIB_RST);
 	if (ret)
 		return ret;
 
@@ -1179,31 +1322,8 @@ static int ppe_port_mac_hw_init(struct ppe_port *ppe_port)
 		return ret;
 
 	/* Enable and reset XGMAC MIB counters */
-	ret = regmap_update_bits(ppe_dev->regmap, reg + XGMAC_MMC_CTRL_ADDR,
-				 XGMAC_MCF | XGMAC_CNTRST, XGMAC_CNTRST);
-
-	return ret;
-}
-
-/* PPE port MAC MIB work task initialization */
-static int ppe_port_mac_mib_work_init(struct ppe_port *ppe_port)
-{
-	struct ppe_device *ppe_dev = ppe_port->ppe_dev;
-	u64 *gstats;
-
-	gstats = devm_kzalloc(ppe_dev->dev,
-			      sizeof(*gstats) * ARRAY_SIZE(gmib_info),
-			      GFP_KERNEL);
-	if (!gstats)
-		return -ENOMEM;
-
-	ppe_port->gmib_stats = gstats;
-
-	spin_lock_init(&ppe_port->gmib_stats_lock);
-	INIT_DELAYED_WORK(&ppe_port->gmib_read,
-			  ppe_port_gmib_stats_poll);
-
-	return 0;
+	return regmap_update_bits(ppe_dev->regmap, reg + XGMAC_MMC_CTRL_ADDR,
+				  XGMAC_MCF | XGMAC_CNTRST, XGMAC_CNTRST);
 }
 
 /**
@@ -1218,8 +1338,8 @@ static int ppe_port_mac_mib_work_init(struct ppe_port *ppe_port)
 int ppe_port_mac_init(struct ppe_device *ppe_dev)
 {
 	struct device_node *ports_node, *port_node;
-	int port, num, ret, j, i = 0;
 	struct ppe_ports *ppe_ports;
+	int port, num, ret, i = 0;
 	phy_interface_t phy_mode;
 
 	ports_node = of_get_child_by_name(ppe_dev->dev->of_node,
@@ -1259,6 +1379,7 @@ int ppe_port_mac_init(struct ppe_device *ppe_dev)
 		ppe_ports->port[i].port_id = port;
 		ppe_ports->port[i].np = port_node;
 		ppe_ports->port[i].interface = phy_mode;
+		ppe_ports->port[i].mac_type = PPE_MAC_TYPE_GMAC;
 
 		ret = ppe_port_clock_init(&ppe_ports->port[i]);
 		if (ret) {
@@ -1272,12 +1393,6 @@ int ppe_port_mac_init(struct ppe_device *ppe_dev)
 			goto err_port_node;
 		}
 
-		ret = ppe_port_mac_mib_work_init(&ppe_ports->port[i]);
-		if (ret) {
-			dev_err(ppe_dev->dev, "Failed to initialize MAC MIB work\n");
-			goto err_port_node;
-		}
-
 		ret = edma_port_setup(&ppe_ports->port[i]);
 		if (ret) {
 			dev_err(ppe_dev->dev, "QCOM EDMA port setup failed\n");
@@ -1299,8 +1414,11 @@ int ppe_port_mac_init(struct ppe_device *ppe_dev)
 	}
 
 err_port_clk:
-	for (j = 0; j < i; j++)
-		ppe_port_clock_deinit(&ppe_ports->port[j]);
+	while (i) {
+		i--;
+		ppe_port_clock_deinit(&ppe_ports->port[i]);
+	}
+
 err_port_node:
 	of_node_put(port_node);
 err_ports_node:
@@ -1322,10 +1440,7 @@ void ppe_port_mac_deinit(struct ppe_device *ppe_dev)
 
 	for (i = 0; i < ppe_dev->ports->num; i++) {
 		ppe_port = &ppe_dev->ports->port[i];
-
-		/* Destroy all phylinks and edma ports */
 		edma_port_destroy(ppe_port);
-
 		ppe_port_clock_deinit(ppe_port);
 	}
 }
diff --git a/drivers/net/ethernet/qualcomm/ppe/ppe_port.h b/drivers/net/ethernet/qualcomm/ppe/ppe_port.h
index 79a1c3beb..bd3e88cf1 100644
--- a/drivers/net/ethernet/qualcomm/ppe/ppe_port.h
+++ b/drivers/net/ethernet/qualcomm/ppe/ppe_port.h
@@ -1,16 +1,23 @@
 /* SPDX-License-Identifier: GPL-2.0-only
  *
- * Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+ * Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
  */
 
 #ifndef __PPE_PORT_H__
 #define __PPE_PORT_H__
 
+#include <linux/compiler.h>
+#include <linux/phy.h>
 #include <linux/phylink.h>
 
-struct ethtool_eee;
+struct clk;
+struct device_node;
+struct net_device;
+struct reset_control;
 struct rtnl_link_stats64;
 
+struct ppe_device;
+
 /**
  * enum ppe_port_clk_rst_type - PPE port clock and reset ID type
  * @PPE_PORT_CLK_RST_MAC: The clock and reset ID for port MAC
@@ -37,14 +44,14 @@ enum ppe_mac_type {
 
 /**
  * struct ppe_port - Private data for each PPE port
- * @phylink: Linux phylink instance
- * @phylink_config: Linux phylink configurations
- * @pcs: Linux phylink PCS instance
  * @np: Port device tree node
  * @ppe_dev: Back pointer to PPE device private data
  * @interface: Port interface mode
  * @mac_type: Port MAC type, GMAC or XGMAC
  * @port_id: Port ID
+ * @phylink: Linux phylink instance
+ * @phylink_config: Linux phylink configurations
+ * @pcs: Linux phylink PCS instance
  * @clks: Port clocks
  * @rstcs: Port resets
  * @gmib_read: Delay work task for GMAC MIB statistics polling function
@@ -52,14 +59,14 @@ enum ppe_mac_type {
  * @gmib_stats_lock: Lock to protect GMAC MIB statistics
  */
 struct ppe_port {
-	struct phylink *phylink;
-	struct phylink_config phylink_config;
-	struct phylink_pcs *pcs;
 	struct device_node *np;
 	struct ppe_device *ppe_dev;
 	phy_interface_t interface;
 	enum ppe_mac_type mac_type;
 	int port_id;
+	struct phylink *phylink;
+	struct phylink_config phylink_config;
+	struct phylink_pcs *pcs;
 	struct clk *clks[PPE_PORT_CLK_RST_MAX];
 	struct reset_control *rstcs[PPE_PORT_CLK_RST_MAX];
 	struct delayed_work gmib_read;
@@ -88,6 +95,5 @@ void ppe_port_get_ethtool_stats(struct ppe_port *ppe_port, u64 *data);
 void ppe_port_get_stats64(struct ppe_port *ppe_port,
 			  struct rtnl_link_stats64 *s);
 int ppe_port_set_mac_address(struct ppe_port *ppe_port, const u8 *addr);
-int ppe_port_set_mac_eee(struct ppe_port *ppe_port, struct ethtool_eee *eee);
 int ppe_port_set_maxframe(struct ppe_port *ppe_port, int maxframe_size);
 #endif
diff --git a/drivers/net/ethernet/qualcomm/ppe/ppe_regs.h b/drivers/net/ethernet/qualcomm/ppe/ppe_regs.h
index f2a60776a..594ee62a6 100644
--- a/drivers/net/ethernet/qualcomm/ppe/ppe_regs.h
+++ b/drivers/net/ethernet/qualcomm/ppe/ppe_regs.h
@@ -1,12 +1,14 @@
 /* SPDX-License-Identifier: GPL-2.0-only
  *
- * Copyright (c) 2024 Qualcomm Innovation Center, Inc. All rights reserved.
+ * Copyright (c) 2025 Qualcomm Innovation Center, Inc. All rights reserved.
  */
 
 /* PPE hardware register and table declarations. */
 #ifndef __PPE_REGS_H__
 #define __PPE_REGS_H__
 
+#include <linux/bitfield.h>
+
 /* PPE port mux select control register */
 #define PPE_PORT_MUX_CTRL_ADDR			0x10
 #define PPE_PORT6_SEL_XGMAC			BIT(13)
@@ -18,118 +20,85 @@
 #define PPE_PORT5_SEL_PCS1			BIT(4)
 #define PPE_PORT_SEL_XGMAC(x)			(BIT(8) << ((x) - 1))
 
-/* PPE port LPI enable register */
-#define PPE_LPI_EN_ADDR				0x400
-#define PPE_LPI_PORT1_EN			BIT(0)
-#define PPE_LPI_PORT2_EN			BIT(1)
-#define PPE_LPI_PORT3_EN			BIT(2)
-#define PPE_LPI_PORT4_EN			BIT(3)
-#define PPE_LPI_PORT5_EN			BIT(4)
-#define PPE_LPI_PORT6_EN			BIT(5)
-#define PPE_LPI_PORT_EN(x)			(BIT(0) << ((x) - 1))
-
-/* There are 15 BM ports and 4 BM groups supported by PPE,
- * BM port (0-7) is matched to EDMA port 0, BM port (8-13) is matched
- * to PPE physical port 1-6, BM port 14 is matched to EIP.
- */
+/* PPE scheduler configurations for buffer manager block. */
 #define PPE_BM_SCH_CTRL_ADDR			0xb000
-#define PPE_BM_SCH_CTRL_NUM			1
 #define PPE_BM_SCH_CTRL_INC			4
 #define PPE_BM_SCH_CTRL_SCH_DEPTH		GENMASK(7, 0)
 #define PPE_BM_SCH_CTRL_SCH_OFFSET		GENMASK(14, 8)
 #define PPE_BM_SCH_CTRL_SCH_EN			BIT(31)
 
-#define PPE_RX_FIFO_CFG_ADDR			0xb004
-#define PPE_RX_FIFO_CFG_NUM			8
-#define PPE_RX_FIFO_CFG_INC			4
-#define PPE_RX_FIFO_CFG_THRSH			GENMASK(2, 0)
-
-#define PPE_DROP_CNT_ADDR			0xb024
-#define PPE_DROP_CNT_NUM			8
-#define PPE_DROP_CNT_INC			4
-
-/* BM port drop counter */
-#define PPE_DROP_STAT_ADDR			0xe000
-#define PPE_DROP_STAT_NUM			30
-#define PPE_DROP_STAT_INC			0x10
-
-#define PPE_EPE_DBG_IN_CNT_ADDR			0x26054
-#define PPE_EPE_DBG_IN_CNT_NUM			1
-#define PPE_EPE_DBG_IN_CNT_INC			0x4
+/* PPE drop counters. */
+#define PPE_DROP_CNT_TBL_ADDR			0xb024
+#define PPE_DROP_CNT_TBL_ENTRIES		8
+#define PPE_DROP_CNT_TBL_INC			4
 
-#define PPE_EPE_DBG_OUT_CNT_ADDR		0x26070
-#define PPE_EPE_DBG_OUT_CNT_NUM			1
-#define PPE_EPE_DBG_OUT_CNT_INC			0x4
+/* BM port drop counters. */
+#define PPE_DROP_STAT_TBL_ADDR			0xe000
+#define PPE_DROP_STAT_TBL_ENTRIES		30
+#define PPE_DROP_STAT_TBL_INC			0x10
 
-/* Egress VLAN counter */
+/* Egress VLAN counters. */
 #define PPE_EG_VSI_COUNTER_TBL_ADDR		0x41000
-#define PPE_EG_VSI_COUNTER_TBL_NUM		64
+#define PPE_EG_VSI_COUNTER_TBL_ENTRIES		64
 #define PPE_EG_VSI_COUNTER_TBL_INC		0x10
 
-/* Port TX counter */
+/* Port TX counters. */
 #define PPE_PORT_TX_COUNTER_TBL_ADDR		0x45000
-#define PPE_PORT_TX_COUNTER_TBL_NUM		8
+#define PPE_PORT_TX_COUNTER_TBL_ENTRIES		8
 #define PPE_PORT_TX_COUNTER_TBL_INC		0x10
 
-/* Virtual port TX counter */
+/* Virtual port TX counters. */
 #define PPE_VPORT_TX_COUNTER_TBL_ADDR		0x47000
-#define PPE_VPORT_TX_COUNTER_TBL_NUM		256
+#define PPE_VPORT_TX_COUNTER_TBL_ENTRIES	256
 #define PPE_VPORT_TX_COUNTER_TBL_INC		0x10
 
-/* Queue counter */
+/* Queue counters. */
 #define PPE_QUEUE_TX_COUNTER_TBL_ADDR		0x4a000
-#define PPE_QUEUE_TX_COUNTER_TBL_NUM		300
+#define PPE_QUEUE_TX_COUNTER_TBL_ENTRIES	300
 #define PPE_QUEUE_TX_COUNTER_TBL_INC		0x10
 
-/* RSS configs contributes to the random RSS hash value generated, which
- * is used to configure the queue offset.
+/* RSS settings are to calculate the random RSS hash value generated during
+ * packet receive to ARM cores. This hash is then used to generate the queue
+ * offset used to determine the queue used to transmit the packet to ARM cores.
  */
 #define PPE_RSS_HASH_MASK_ADDR			0xb4318
-#define PPE_RSS_HASH_MASK_NUM			1
-#define PPE_RSS_HASH_MASK_INC			4
 #define PPE_RSS_HASH_MASK_HASH_MASK		GENMASK(20, 0)
 #define PPE_RSS_HASH_MASK_FRAGMENT		BIT(28)
 
 #define PPE_RSS_HASH_SEED_ADDR			0xb431c
-#define PPE_RSS_HASH_SEED_NUM			1
-#define PPE_RSS_HASH_SEED_INC			4
 #define PPE_RSS_HASH_SEED_VAL			GENMASK(31, 0)
 
 #define PPE_RSS_HASH_MIX_ADDR			0xb4320
-#define PPE_RSS_HASH_MIX_NUM			11
+#define PPE_RSS_HASH_MIX_ENTRIES		11
 #define PPE_RSS_HASH_MIX_INC			4
 #define PPE_RSS_HASH_MIX_VAL			GENMASK(4, 0)
 
 #define PPE_RSS_HASH_FIN_ADDR			0xb4350
-#define PPE_RSS_HASH_FIN_NUM			5
+#define PPE_RSS_HASH_FIN_ENTRIES		5
 #define PPE_RSS_HASH_FIN_INC			4
 #define PPE_RSS_HASH_FIN_INNER			GENMASK(4, 0)
 #define PPE_RSS_HASH_FIN_OUTER			GENMASK(9, 5)
 
 #define PPE_RSS_HASH_MASK_IPV4_ADDR		0xb4380
-#define PPE_RSS_HASH_MASK_IPV4_NUM		1
-#define PPE_RSS_HASH_MASK_IPV4_INC		4
 #define PPE_RSS_HASH_MASK_IPV4_HASH_MASK	GENMASK(20, 0)
 #define PPE_RSS_HASH_MASK_IPV4_FRAGMENT		BIT(28)
 
 #define PPE_RSS_HASH_SEED_IPV4_ADDR		0xb4384
-#define PPE_RSS_HASH_SEED_IPV4_NUM		1
-#define PPE_RSS_HASH_SEED_IPV4_INC		4
 #define PPE_RSS_HASH_SEED_IPV4_VAL		GENMASK(31, 0)
 
 #define PPE_RSS_HASH_MIX_IPV4_ADDR		0xb4390
-#define PPE_RSS_HASH_MIX_IPV4_NUM		5
+#define PPE_RSS_HASH_MIX_IPV4_ENTRIES		5
 #define PPE_RSS_HASH_MIX_IPV4_INC		4
 #define PPE_RSS_HASH_MIX_IPV4_VAL		GENMASK(4, 0)
 
 #define PPE_RSS_HASH_FIN_IPV4_ADDR		0xb43b0
-#define PPE_RSS_HASH_FIN_IPV4_NUM		5
+#define PPE_RSS_HASH_FIN_IPV4_ENTRIES		5
 #define PPE_RSS_HASH_FIN_IPV4_INC		4
 #define PPE_RSS_HASH_FIN_IPV4_INNER		GENMASK(4, 0)
 #define PPE_RSS_HASH_FIN_IPV4_OUTER		GENMASK(9, 5)
 
 #define PPE_BM_SCH_CFG_TBL_ADDR			0xc000
-#define PPE_BM_SCH_CFG_TBL_NUM			128
+#define PPE_BM_SCH_CFG_TBL_ENTRIES		128
 #define PPE_BM_SCH_CFG_TBL_INC			0x10
 #define PPE_BM_SCH_CFG_TBL_PORT_NUM		GENMASK(3, 0)
 #define PPE_BM_SCH_CFG_TBL_DIR			BIT(4)
@@ -137,35 +106,40 @@
 #define PPE_BM_SCH_CFG_TBL_SECOND_PORT_VALID	BIT(6)
 #define PPE_BM_SCH_CFG_TBL_SECOND_PORT		GENMASK(11, 8)
 
-/* PPE service code configuration on the ingress direction. */
+/* PPE service code configuration for the ingress direction functions,
+ * including bypass configuration for relevant PPE switch core functions
+ * such as flow entry lookup bypass.
+ */
 #define PPE_SERVICE_TBL_ADDR			0x15000
-#define PPE_SERVICE_TBL_NUM			256
+#define PPE_SERVICE_TBL_ENTRIES			256
 #define PPE_SERVICE_TBL_INC			0x10
 #define PPE_SERVICE_W0_BYPASS_BITMAP		GENMASK(31, 0)
 #define PPE_SERVICE_W1_RX_CNT_EN		BIT(0)
 
 #define PPE_SERVICE_SET_BYPASS_BITMAP(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_SERVICE_W0_BYPASS_BITMAP)
+	FIELD_MODIFY(PPE_SERVICE_W0_BYPASS_BITMAP, tbl_cfg, value)
 #define PPE_SERVICE_SET_RX_CNT_EN(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)(tbl_cfg) + 0x1, value, PPE_SERVICE_W1_RX_CNT_EN)
-
-#define PPE_PORT_EG_VLAN_ADDR			0x20020
-#define PPE_PORT_EG_VLAN_NUM			8
-#define PPE_PORT_EG_VLAN_INC			4
-#define PPE_PORT_EG_VLAN_VLAN_TYPE		BIT(0)
-#define PPE_PORT_EG_VLAN_CTAG_MODE		GENMASK(2, 1)
-#define PPE_PORT_EG_VLAN_STAG_MODE		GENMASK(4, 3)
-#define PPE_PORT_EG_VLAN_VSI_TAG_MODE_EN	BIT(5)
-#define PPE_PORT_EG_VLAN_PCP_PROP_CMD		BIT(6)
-#define PPE_PORT_EG_VLAN_DEI_PROP_CMD		BIT(7)
-#define PPE_PORT_EG_VLAN_TX_COUNTING_EN		BIT(8)
-
+	FIELD_MODIFY(PPE_SERVICE_W1_RX_CNT_EN, tbl_cfg + 0x1, value)
+
+/* PPE port egress VLAN configurations. */
+#define PPE_PORT_EG_VLAN_TBL_ADDR		0x20020
+#define PPE_PORT_EG_VLAN_TBL_ENTRIES		8
+#define PPE_PORT_EG_VLAN_TBL_INC		4
+#define PPE_PORT_EG_VLAN_TBL_VLAN_TYPE		BIT(0)
+#define PPE_PORT_EG_VLAN_TBL_CTAG_MODE		GENMASK(2, 1)
+#define PPE_PORT_EG_VLAN_TBL_STAG_MODE		GENMASK(4, 3)
+#define PPE_PORT_EG_VLAN_TBL_VSI_TAG_MODE_EN	BIT(5)
+#define PPE_PORT_EG_VLAN_TBL_PCP_PROP_CMD	BIT(6)
+#define PPE_PORT_EG_VLAN_TBL_DEI_PROP_CMD	BIT(7)
+#define PPE_PORT_EG_VLAN_TBL_TX_COUNTING_EN	BIT(8)
+
+/* PPE queue counters enable/disable control. */
 #define PPE_EG_BRIDGE_CONFIG_ADDR		0x20044
 #define PPE_EG_BRIDGE_CONFIG_QUEUE_CNT_EN	BIT(2)
 
 /* PPE service code configuration on the egress direction. */
 #define PPE_EG_SERVICE_TBL_ADDR			0x43000
-#define PPE_EG_SERVICE_TBL_NUM			256
+#define PPE_EG_SERVICE_TBL_ENTRIES		256
 #define PPE_EG_SERVICE_TBL_INC			0x10
 #define PPE_EG_SERVICE_W0_UPDATE_ACTION		GENMASK(31, 0)
 #define PPE_EG_SERVICE_W1_NEXT_SERVCODE		GENMASK(7, 0)
@@ -174,30 +148,27 @@
 #define PPE_EG_SERVICE_W1_TX_CNT_EN		BIT(15)
 
 #define PPE_EG_SERVICE_SET_UPDATE_ACTION(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_EG_SERVICE_W0_UPDATE_ACTION)
+	FIELD_MODIFY(PPE_EG_SERVICE_W0_UPDATE_ACTION, tbl_cfg, value)
 #define PPE_EG_SERVICE_SET_NEXT_SERVCODE(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)(tbl_cfg) + 0x1, value, PPE_EG_SERVICE_W1_NEXT_SERVCODE)
+	FIELD_MODIFY(PPE_EG_SERVICE_W1_NEXT_SERVCODE, tbl_cfg + 0x1, value)
 #define PPE_EG_SERVICE_SET_HW_SERVICE(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)(tbl_cfg) + 0x1, value, PPE_EG_SERVICE_W1_HW_SERVICE)
+	FIELD_MODIFY(PPE_EG_SERVICE_W1_HW_SERVICE, tbl_cfg + 0x1, value)
 #define PPE_EG_SERVICE_SET_OFFSET_SEL(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)(tbl_cfg) + 0x1, value, PPE_EG_SERVICE_W1_OFFSET_SEL)
+	FIELD_MODIFY(PPE_EG_SERVICE_W1_OFFSET_SEL, tbl_cfg + 0x1, value)
 #define PPE_EG_SERVICE_SET_TX_CNT_EN(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)(tbl_cfg) + 0x1, value, PPE_EG_SERVICE_W1_TX_CNT_EN)
+	FIELD_MODIFY(PPE_EG_SERVICE_W1_TX_CNT_EN, tbl_cfg + 0x1, value)
 
 /* PPE port bridge configuration */
 #define PPE_PORT_BRIDGE_CTRL_ADDR		0x60300
-#define PPE_PORT_BRIDGE_CTRL_NUM		8
+#define PPE_PORT_BRIDGE_CTRL_ENTRIES		8
 #define PPE_PORT_BRIDGE_CTRL_INC		4
 #define PPE_PORT_BRIDGE_NEW_LRN_EN		BIT(0)
-#define PPE_PORT_BRIDGE_NEW_FWD_CMD		GENMASK(2, 1)
 #define PPE_PORT_BRIDGE_STA_MOVE_LRN_EN		BIT(3)
-#define PPE_PORT_BRIDGE_STA_MOVE_FWD_CMD	GENMASK(5, 4)
-#define PPE_PORT_BRIDGE_ISOLATION_BITMAP	GENMASK(15, 8)
 #define PPE_PORT_BRIDGE_TXMAC_EN		BIT(16)
-#define PPE_PORT_BRIDGE_PROMISC_EN		BIT(17)
 
+/* PPE port control configurations for the traffic to the multicast queues. */
 #define PPE_MC_MTU_CTRL_TBL_ADDR		0x60a00
-#define PPE_MC_MTU_CTRL_TBL_NUM			8
+#define PPE_MC_MTU_CTRL_TBL_ENTRIES		8
 #define PPE_MC_MTU_CTRL_TBL_INC			4
 #define PPE_MC_MTU_CTRL_TBL_MTU			GENMASK(13, 0)
 #define PPE_MC_MTU_CTRL_TBL_MTU_CMD		GENMASK(15, 14)
@@ -205,7 +176,7 @@
 
 /* PPE VSI configurations */
 #define PPE_VSI_TBL_ADDR			0x63800
-#define PPE_VSI_TBL_NUM				64
+#define PPE_VSI_TBL_ENTRIES			64
 #define PPE_VSI_TBL_INC				0x10
 #define PPE_VSI_W0_MEMBER_PORT_BITMAP		GENMASK(7, 0)
 #define PPE_VSI_W0_UUC_BITMAP			GENMASK(15, 8)
@@ -217,25 +188,25 @@
 #define PPE_VSI_W1_STATION_MOVE_FWD_CMD		GENMASK(5, 4)
 
 #define PPE_VSI_SET_MEMBER_PORT_BITMAP(tbl_cfg, value)		\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_VSI_W0_MEMBER_PORT_BITMAP)
+	FIELD_MODIFY(PPE_VSI_W0_MEMBER_PORT_BITMAP, tbl_cfg, value)
 #define PPE_VSI_SET_UUC_BITMAP(tbl_cfg, value)			\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_VSI_W0_UUC_BITMAP)
+	FIELD_MODIFY(PPE_VSI_W0_UUC_BITMAP, tbl_cfg, value)
 #define PPE_VSI_SET_UMC_BITMAP(tbl_cfg, value)			\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_VSI_W0_UMC_BITMAP)
+	FIELD_MODIFY(PPE_VSI_W0_UMC_BITMAP, tbl_cfg, value)
 #define PPE_VSI_SET_BC_BITMAP(tbl_cfg, value)			\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_VSI_W0_BC_BITMAP)
+	FIELD_MODIFY(PPE_VSI_W0_BC_BITMAP, tbl_cfg, value)
 #define PPE_VSI_SET_NEW_ADDR_LRN_EN(tbl_cfg, value)		\
-	u32p_replace_bits((u32 *)(tbl_cfg) + 0x1, value, PPE_VSI_W1_NEW_ADDR_LRN_EN)
+	FIELD_MODIFY(PPE_VSI_W1_NEW_ADDR_LRN_EN, tbl_cfg + 0x1, value)
 #define PPE_VSI_SET_NEW_ADDR_FWD_CMD(tbl_cfg, value)		\
-	u32p_replace_bits((u32 *)(tbl_cfg) + 0x1, value, PPE_VSI_W1_NEW_ADDR_FWD_CMD)
+	FIELD_MODIFY(PPE_VSI_W1_NEW_ADDR_FWD_CMD, tbl_cfg + 0x1, value)
 #define PPE_VSI_SET_STATION_MOVE_LRN_EN(tbl_cfg, value)		\
-	u32p_replace_bits((u32 *)(tbl_cfg) + 0x1, value, PPE_VSI_W1_STATION_MOVE_LRN_EN)
+	FIELD_MODIFY(PPE_VSI_W1_STATION_MOVE_LRN_EN, tbl_cfg + 0x1, value)
 #define PPE_VSI_SET_STATION_MOVE_FWD_CMD(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)(tbl_cfg) + 0x1, value, PPE_VSI_W1_STATION_MOVE_FWD_CMD)
+	FIELD_MODIFY(PPE_VSI_W1_STATION_MOVE_FWD_CMD, tbl_cfg + 0x1, value)
 
-/* PPE port control configuration, the MTU and MRU configs. */
+/* PPE port control configurations for the traffic to the unicast queues. */
 #define PPE_MRU_MTU_CTRL_TBL_ADDR		0x65000
-#define PPE_MRU_MTU_CTRL_TBL_NUM		256
+#define PPE_MRU_MTU_CTRL_TBL_ENTRIES		256
 #define PPE_MRU_MTU_CTRL_TBL_INC		0x10
 #define PPE_MRU_MTU_CTRL_W0_MRU			GENMASK(13, 0)
 #define PPE_MRU_MTU_CTRL_W0_MRU_CMD		GENMASK(15, 14)
@@ -248,20 +219,21 @@
 #define PPE_MRU_MTU_CTRL_W2_INNER_PREC_HIGH	GENMASK(1, 0)
 
 #define PPE_MRU_MTU_CTRL_SET_MRU(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_MRU_MTU_CTRL_W0_MRU)
+	FIELD_MODIFY(PPE_MRU_MTU_CTRL_W0_MRU, tbl_cfg, value)
 #define PPE_MRU_MTU_CTRL_SET_MRU_CMD(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_MRU_MTU_CTRL_W0_MRU_CMD)
+	FIELD_MODIFY(PPE_MRU_MTU_CTRL_W0_MRU_CMD, tbl_cfg, value)
 #define PPE_MRU_MTU_CTRL_SET_MTU(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_MRU_MTU_CTRL_W0_MTU)
+	FIELD_MODIFY(PPE_MRU_MTU_CTRL_W0_MTU, tbl_cfg, value)
 #define PPE_MRU_MTU_CTRL_SET_MTU_CMD(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_MRU_MTU_CTRL_W0_MTU_CMD)
+	FIELD_MODIFY(PPE_MRU_MTU_CTRL_W0_MTU_CMD, tbl_cfg, value)
 #define PPE_MRU_MTU_CTRL_SET_RX_CNT_EN(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)(tbl_cfg) + 0x1, value, PPE_MRU_MTU_CTRL_W1_RX_CNT_EN)
+	FIELD_MODIFY(PPE_MRU_MTU_CTRL_W1_RX_CNT_EN, tbl_cfg + 0x1, value)
 #define PPE_MRU_MTU_CTRL_SET_TX_CNT_EN(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)(tbl_cfg) + 0x1, value, PPE_MRU_MTU_CTRL_W1_TX_CNT_EN)
+	FIELD_MODIFY(PPE_MRU_MTU_CTRL_W1_TX_CNT_EN, tbl_cfg + 0x1, value)
 
+/* PPE service code configuration for destination port and counter. */
 #define PPE_IN_L2_SERVICE_TBL_ADDR		0x66000
-#define PPE_IN_L2_SERVICE_TBL_NUM		256
+#define PPE_IN_L2_SERVICE_TBL_ENTRIES		256
 #define PPE_IN_L2_SERVICE_TBL_INC		0x10
 #define PPE_IN_L2_SERVICE_TBL_DST_PORT_ID_VALID	BIT(0)
 #define PPE_IN_L2_SERVICE_TBL_DST_PORT_ID	GENMASK(4, 1)
@@ -272,69 +244,75 @@
 
 /* L2 Port configurations */
 #define PPE_L2_VP_PORT_TBL_ADDR			0x98000
-#define PPE_L2_VP_PORT_TBL_NUM			256
+#define PPE_L2_VP_PORT_TBL_ENTRIES		256
 #define PPE_L2_VP_PORT_TBL_INC			0x10
 #define PPE_L2_VP_PORT_W0_INVALID_VSI_FWD_EN	BIT(0)
 #define PPE_L2_VP_PORT_W0_DST_INFO		GENMASK(9, 2)
 
 #define PPE_L2_PORT_SET_INVALID_VSI_FWD_EN(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_L2_VP_PORT_W0_INVALID_VSI_FWD_EN)
+	FIELD_MODIFY(PPE_L2_VP_PORT_W0_INVALID_VSI_FWD_EN, tbl_cfg, value)
 #define PPE_L2_PORT_SET_DST_INFO(tbl_cfg, value)		\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_L2_VP_PORT_W0_DST_INFO)
+	FIELD_MODIFY(PPE_L2_VP_PORT_W0_DST_INFO, tbl_cfg, value)
 
-/* Port RX and RX drop counter */
+/* Port RX and RX drop counters. */
 #define PPE_PORT_RX_CNT_TBL_ADDR		0x150000
-#define PPE_PORT_RX_CNT_TBL_NUM			256
+#define PPE_PORT_RX_CNT_TBL_ENTRIES		256
 #define PPE_PORT_RX_CNT_TBL_INC			0x20
 
-/* Physical port RX and RX drop counter */
+/* Physical port RX and RX drop counters. */
 #define PPE_PHY_PORT_RX_CNT_TBL_ADDR		0x156000
-#define PPE_PHY_PORT_RX_CNT_TBL_NUM		8
+#define PPE_PHY_PORT_RX_CNT_TBL_ENTRIES		8
 #define PPE_PHY_PORT_RX_CNT_TBL_INC		0x20
 
-/* Counter for the packet to CPU port */
+/* Counters for the packet to CPU port. */
 #define PPE_DROP_CPU_CNT_TBL_ADDR		0x160000
-#define PPE_DROP_CPU_CNT_TBL_NUM		1280
+#define PPE_DROP_CPU_CNT_TBL_ENTRIES		1280
 #define PPE_DROP_CPU_CNT_TBL_INC		0x10
 
-/* VLAN counter */
+/* VLAN counters. */
 #define PPE_VLAN_CNT_TBL_ADDR			0x178000
-#define PPE_VLAN_CNT_TBL_NUM			64
+#define PPE_VLAN_CNT_TBL_ENTRIES		64
 #define PPE_VLAN_CNT_TBL_INC			0x10
 
-/* PPE L2 counter */
+/* PPE L2 counters. */
 #define PPE_PRE_L2_CNT_TBL_ADDR			0x17c000
-#define PPE_PRE_L2_CNT_TBL_NUM			64
+#define PPE_PRE_L2_CNT_TBL_ENTRIES		64
 #define PPE_PRE_L2_CNT_TBL_INC			0x20
 
-/* Port TX drop counter */
+/* Port TX drop counters. */
 #define PPE_PORT_TX_DROP_CNT_TBL_ADDR		0x17d000
-#define PPE_PORT_TX_DROP_CNT_TBL_NUM		8
+#define PPE_PORT_TX_DROP_CNT_TBL_ENTRIES	8
 #define PPE_PORT_TX_DROP_CNT_TBL_INC		0x10
 
-/* Virtual port TX counter */
+/* Virtual port TX counters. */
 #define PPE_VPORT_TX_DROP_CNT_TBL_ADDR		0x17e000
-#define PPE_VPORT_TX_DROP_CNT_TBL_NUM		256
+#define PPE_VPORT_TX_DROP_CNT_TBL_ENTRIES	256
 #define PPE_VPORT_TX_DROP_CNT_TBL_INC		0x10
 
-#define PPE_TPR_PKT_CNT_ADDR			0x1d0080
+/* Counters for the tunnel packet. */
+#define PPE_TPR_PKT_CNT_TBL_ADDR		0x1d0080
+#define PPE_TPR_PKT_CNT_TBL_ENTRIES		8
+#define PPE_TPR_PKT_CNT_TBL_INC			4
 
-#define PPE_IPR_PKT_CNT_ADDR			0x1e0080
-#define PPE_IPR_PKT_CNT_NUM			8
-#define PPE_IPR_PKT_CNT_INC			4
+/* Counters for the all packet received. */
+#define PPE_IPR_PKT_CNT_TBL_ADDR		0x1e0080
+#define PPE_IPR_PKT_CNT_TBL_ENTRIES		8
+#define PPE_IPR_PKT_CNT_TBL_INC			4
 
+/* PPE service code configuration for the tunnel packet. */
 #define PPE_TL_SERVICE_TBL_ADDR			0x306000
-#define PPE_TL_SERVICE_TBL_NUM			256
+#define PPE_TL_SERVICE_TBL_ENTRIES		256
 #define PPE_TL_SERVICE_TBL_INC			4
 #define PPE_TL_SERVICE_TBL_BYPASS_BITMAP	GENMASK(31, 0)
 
+/* Port scheduler global config. */
 #define PPE_PSCH_SCH_DEPTH_CFG_ADDR		0x400000
-#define PPE_PSCH_SCH_DEPTH_CFG_NUM		1
 #define PPE_PSCH_SCH_DEPTH_CFG_INC		4
 #define PPE_PSCH_SCH_DEPTH_CFG_SCH_DEPTH	GENMASK(7, 0)
 
+/* PPE queue level scheduler configurations. */
 #define PPE_L0_FLOW_MAP_TBL_ADDR		0x402000
-#define PPE_L0_FLOW_MAP_TBL_NUM			300
+#define PPE_L0_FLOW_MAP_TBL_ENTRIES		300
 #define PPE_L0_FLOW_MAP_TBL_INC			0x10
 #define PPE_L0_FLOW_MAP_TBL_FLOW_ID		GENMASK(5, 0)
 #define PPE_L0_FLOW_MAP_TBL_C_PRI		GENMASK(8, 6)
@@ -343,40 +321,42 @@
 #define PPE_L0_FLOW_MAP_TBL_E_NODE_WT		GENMASK(31, 22)
 
 #define PPE_L0_C_FLOW_CFG_TBL_ADDR		0x404000
-#define PPE_L0_C_FLOW_CFG_TBL_NUM		512
+#define PPE_L0_C_FLOW_CFG_TBL_ENTRIES		512
 #define PPE_L0_C_FLOW_CFG_TBL_INC		0x10
 #define PPE_L0_C_FLOW_CFG_TBL_NODE_ID		GENMASK(7, 0)
 #define PPE_L0_C_FLOW_CFG_TBL_NODE_CREDIT_UNIT	BIT(8)
 
 #define PPE_L0_E_FLOW_CFG_TBL_ADDR		0x406000
-#define PPE_L0_E_FLOW_CFG_TBL_NUM		512
+#define PPE_L0_E_FLOW_CFG_TBL_ENTRIES		512
 #define PPE_L0_E_FLOW_CFG_TBL_INC		0x10
 #define PPE_L0_E_FLOW_CFG_TBL_NODE_ID		GENMASK(7, 0)
 #define PPE_L0_E_FLOW_CFG_TBL_NODE_CREDIT_UNIT	BIT(8)
 
 #define PPE_L0_FLOW_PORT_MAP_TBL_ADDR		0x408000
-#define PPE_L0_FLOW_PORT_MAP_TBL_NUM		300
+#define PPE_L0_FLOW_PORT_MAP_TBL_ENTRIES	300
 #define PPE_L0_FLOW_PORT_MAP_TBL_INC		0x10
 #define PPE_L0_FLOW_PORT_MAP_TBL_PORT_NUM	GENMASK(3, 0)
 
 #define PPE_L0_COMP_CFG_TBL_ADDR		0x428000
-#define PPE_L0_COMP_CFG_TBL_NUM			300
+#define PPE_L0_COMP_CFG_TBL_ENTRIES		300
 #define PPE_L0_COMP_CFG_TBL_INC			0x10
 #define PPE_L0_COMP_CFG_TBL_SHAPER_METER_LEN	GENMASK(1, 0)
 #define PPE_L0_COMP_CFG_TBL_NODE_METER_LEN	GENMASK(3, 2)
 
-/* PPE queue bitmap. */
+/* PPE queue to Ethernet DMA ring mapping table. */
 #define PPE_RING_Q_MAP_TBL_ADDR			0x42a000
-#define PPE_RING_Q_MAP_TBL_NUM			24
+#define PPE_RING_Q_MAP_TBL_ENTRIES		24
 #define PPE_RING_Q_MAP_TBL_INC			0x40
 
+/* Table addresses for per-queue dequeue setting. */
 #define PPE_DEQ_OPR_TBL_ADDR			0x430000
-#define PPE_DEQ_OPR_TBL_NUM			300
+#define PPE_DEQ_OPR_TBL_ENTRIES			300
 #define PPE_DEQ_OPR_TBL_INC			0x10
 #define PPE_DEQ_OPR_TBL_DEQ_DISABLE		BIT(0)
 
+/* PPE flow level scheduler configurations. */
 #define PPE_L1_FLOW_MAP_TBL_ADDR		0x440000
-#define PPE_L1_FLOW_MAP_TBL_NUM			64
+#define PPE_L1_FLOW_MAP_TBL_ENTRIES		64
 #define PPE_L1_FLOW_MAP_TBL_INC			0x10
 #define PPE_L1_FLOW_MAP_TBL_FLOW_ID		GENMASK(3, 0)
 #define PPE_L1_FLOW_MAP_TBL_C_PRI		GENMASK(6, 4)
@@ -385,30 +365,31 @@
 #define PPE_L1_FLOW_MAP_TBL_E_NODE_WT		GENMASK(29, 20)
 
 #define PPE_L1_C_FLOW_CFG_TBL_ADDR		0x442000
-#define PPE_L1_C_FLOW_CFG_TBL_NUM		64
+#define PPE_L1_C_FLOW_CFG_TBL_ENTRIES		64
 #define PPE_L1_C_FLOW_CFG_TBL_INC		0x10
 #define PPE_L1_C_FLOW_CFG_TBL_NODE_ID		GENMASK(5, 0)
 #define PPE_L1_C_FLOW_CFG_TBL_NODE_CREDIT_UNIT	BIT(6)
 
 #define PPE_L1_E_FLOW_CFG_TBL_ADDR		0x444000
-#define PPE_L1_E_FLOW_CFG_TBL_NUM		64
+#define PPE_L1_E_FLOW_CFG_TBL_ENTRIES		64
 #define PPE_L1_E_FLOW_CFG_TBL_INC		0x10
 #define PPE_L1_E_FLOW_CFG_TBL_NODE_ID		GENMASK(5, 0)
 #define PPE_L1_E_FLOW_CFG_TBL_NODE_CREDIT_UNIT	BIT(6)
 
 #define PPE_L1_FLOW_PORT_MAP_TBL_ADDR		0x446000
-#define PPE_L1_FLOW_PORT_MAP_TBL_NUM		64
+#define PPE_L1_FLOW_PORT_MAP_TBL_ENTRIES	64
 #define PPE_L1_FLOW_PORT_MAP_TBL_INC		0x10
 #define PPE_L1_FLOW_PORT_MAP_TBL_PORT_NUM	GENMASK(3, 0)
 
 #define PPE_L1_COMP_CFG_TBL_ADDR		0x46a000
-#define PPE_L1_COMP_CFG_TBL_NUM			64
+#define PPE_L1_COMP_CFG_TBL_ENTRIES		64
 #define PPE_L1_COMP_CFG_TBL_INC			0x10
 #define PPE_L1_COMP_CFG_TBL_SHAPER_METER_LEN	GENMASK(1, 0)
 #define PPE_L1_COMP_CFG_TBL_NODE_METER_LEN	GENMASK(3, 2)
 
+/* PPE port scheduler configurations for egress. */
 #define PPE_PSCH_SCH_CFG_TBL_ADDR		0x47a000
-#define PPE_PSCH_SCH_CFG_TBL_NUM		128
+#define PPE_PSCH_SCH_CFG_TBL_ENTRIES		128
 #define PPE_PSCH_SCH_CFG_TBL_INC		0x10
 #define PPE_PSCH_SCH_CFG_TBL_DES_PORT		GENMASK(3, 0)
 #define PPE_PSCH_SCH_CFG_TBL_ENS_PORT		GENMASK(7, 4)
@@ -416,30 +397,40 @@
 #define PPE_PSCH_SCH_CFG_TBL_DES_SECOND_PORT_EN	BIT(16)
 #define PPE_PSCH_SCH_CFG_TBL_DES_SECOND_PORT	GENMASK(20, 17)
 
+/* There are 15 BM ports and 4 BM groups supported by PPE.
+ * BM port (0-7) is for EDMA port 0, BM port (8-13) is for
+ * PPE physical port 1-6 and BM port 14 is for EIP port.
+ */
 #define PPE_BM_PORT_FC_MODE_ADDR		0x600100
+#define PPE_BM_PORT_FC_MODE_ENTRIES		15
 #define PPE_BM_PORT_FC_MODE_INC			0x4
 #define PPE_BM_PORT_FC_MODE_EN			BIT(0)
 
 #define PPE_BM_PORT_GROUP_ID_ADDR		0x600180
+#define PPE_BM_PORT_GROUP_ID_ENTRIES		15
 #define PPE_BM_PORT_GROUP_ID_INC		0x4
 #define PPE_BM_PORT_GROUP_ID_SHARED_GROUP_ID	GENMASK(1, 0)
 
-#define PPE_BM_USED_CNT_ADDR			0x6001c0
-#define PPE_BM_USED_CNT_NUM			15
-#define PPE_BM_USED_CNT_INC			0x4
+/* Counters for PPE buffers used for packets cached. */
+#define PPE_BM_USED_CNT_TBL_ADDR		0x6001c0
+#define PPE_BM_USED_CNT_TBL_ENTRIES		15
+#define PPE_BM_USED_CNT_TBL_INC			0x4
 #define PPE_BM_USED_CNT_VAL			GENMASK(10, 0)
 
-#define PPE_BM_REACT_CNT_ADDR			0x600240
-#define PPE_BM_REACT_CNT_NUM			15
-#define PPE_BM_REACT_CNT_INC			0x4
+/* Counters for PPE buffers used for packets received after pause frame sent. */
+#define PPE_BM_REACT_CNT_TBL_ADDR		0x600240
+#define PPE_BM_REACT_CNT_TBL_ENTRIES		15
+#define PPE_BM_REACT_CNT_TBL_INC		0x4
 #define PPE_BM_REACT_CNT_VAL			GENMASK(8, 0)
 
 #define PPE_BM_SHARED_GROUP_CFG_ADDR		0x600290
+#define PPE_BM_SHARED_GROUP_CFG_ENTRIES		4
 #define PPE_BM_SHARED_GROUP_CFG_INC		0x4
 #define PPE_BM_SHARED_GROUP_CFG_SHARED_LIMIT	GENMASK(10, 0)
 
-#define PPE_BM_PORT_FC_CFG_ADDR			0x601000
-#define PPE_BM_PORT_FC_CFG_INC			0x10
+#define PPE_BM_PORT_FC_CFG_TBL_ADDR		0x601000
+#define PPE_BM_PORT_FC_CFG_TBL_ENTRIES		15
+#define PPE_BM_PORT_FC_CFG_TBL_INC		0x10
 #define PPE_BM_PORT_FC_W0_REACT_LIMIT		GENMASK(8, 0)
 #define PPE_BM_PORT_FC_W0_RESUME_THRESHOLD	GENMASK(17, 9)
 #define PPE_BM_PORT_FC_W0_RESUME_OFFSET		GENMASK(28, 18)
@@ -450,98 +441,103 @@
 #define PPE_BM_PORT_FC_W1_PRE_ALLOC		GENMASK(22, 12)
 
 #define PPE_BM_PORT_FC_SET_REACT_LIMIT(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_BM_PORT_FC_W0_REACT_LIMIT)
+	FIELD_MODIFY(PPE_BM_PORT_FC_W0_REACT_LIMIT, tbl_cfg, value)
 #define PPE_BM_PORT_FC_SET_RESUME_THRESHOLD(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_BM_PORT_FC_W0_RESUME_THRESHOLD)
+	FIELD_MODIFY(PPE_BM_PORT_FC_W0_RESUME_THRESHOLD, tbl_cfg, value)
 #define PPE_BM_PORT_FC_SET_RESUME_OFFSET(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_BM_PORT_FC_W0_RESUME_OFFSET)
+	FIELD_MODIFY(PPE_BM_PORT_FC_W0_RESUME_OFFSET, tbl_cfg, value)
 #define PPE_BM_PORT_FC_SET_CEILING_LOW(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_BM_PORT_FC_W0_CEILING_LOW)
+	FIELD_MODIFY(PPE_BM_PORT_FC_W0_CEILING_LOW, tbl_cfg, value)
 #define PPE_BM_PORT_FC_SET_CEILING_HIGH(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)(tbl_cfg) + 0x1, value, PPE_BM_PORT_FC_W1_CEILING_HIGH)
+	FIELD_MODIFY(PPE_BM_PORT_FC_W1_CEILING_HIGH, tbl_cfg + 0x1, value)
 #define PPE_BM_PORT_FC_SET_WEIGHT(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)(tbl_cfg) + 0x1, value, PPE_BM_PORT_FC_W1_WEIGHT)
+	FIELD_MODIFY(PPE_BM_PORT_FC_W1_WEIGHT, tbl_cfg + 0x1, value)
 #define PPE_BM_PORT_FC_SET_DYNAMIC(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)(tbl_cfg) + 0x1, value, PPE_BM_PORT_FC_W1_DYNAMIC)
+	FIELD_MODIFY(PPE_BM_PORT_FC_W1_DYNAMIC, tbl_cfg + 0x1, value)
 #define PPE_BM_PORT_FC_SET_PRE_ALLOC(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)(tbl_cfg) + 0x1, value, PPE_BM_PORT_FC_W1_PRE_ALLOC)
+	FIELD_MODIFY(PPE_BM_PORT_FC_W1_PRE_ALLOC, tbl_cfg + 0x1, value)
 
+/* The queue base configurations based on destination port,
+ * service code or CPU code.
+ */
 #define PPE_UCAST_QUEUE_MAP_TBL_ADDR		0x810000
-#define PPE_UCAST_QUEUE_MAP_TBL_NUM		3072
+#define PPE_UCAST_QUEUE_MAP_TBL_ENTRIES		3072
 #define PPE_UCAST_QUEUE_MAP_TBL_INC		0x10
 #define PPE_UCAST_QUEUE_MAP_TBL_PROFILE_ID	GENMASK(3, 0)
 #define PPE_UCAST_QUEUE_MAP_TBL_QUEUE_ID	GENMASK(11, 4)
 
+/* The queue offset configurations based on RSS hash value. */
 #define PPE_UCAST_HASH_MAP_TBL_ADDR		0x830000
-#define PPE_UCAST_HASH_MAP_TBL_NUM		4096
+#define PPE_UCAST_HASH_MAP_TBL_ENTRIES		4096
 #define PPE_UCAST_HASH_MAP_TBL_INC		0x10
 #define PPE_UCAST_HASH_MAP_TBL_HASH		GENMASK(7, 0)
 
+/* The queue offset configurations based on PPE internal priority. */
 #define PPE_UCAST_PRIORITY_MAP_TBL_ADDR		0x842000
-#define PPE_UCAST_PRIORITY_MAP_TBL_NUM		256
+#define PPE_UCAST_PRIORITY_MAP_TBL_ENTRIES	256
 #define PPE_UCAST_PRIORITY_MAP_TBL_INC		0x10
 #define PPE_UCAST_PRIORITY_MAP_TBL_CLASS	GENMASK(3, 0)
 
 /* PPE unicast queue (0-255) configurations. */
-#define PPE_AC_UNI_QUEUE_CFG_TBL_ADDR		0x848000
-#define PPE_AC_UNI_QUEUE_CFG_TBL_NUM		256
-#define PPE_AC_UNI_QUEUE_CFG_TBL_INC		0x10
-#define PPE_AC_UNI_QUEUE_CFG_W0_EN		BIT(0)
-#define PPE_AC_UNI_QUEUE_CFG_W0_WRED_EN		BIT(1)
-#define PPE_AC_UNI_QUEUE_CFG_W0_FC_EN		BIT(2)
-#define PPE_AC_UNI_QUEUE_CFG_W0_COLOR_AWARE	BIT(3)
-#define PPE_AC_UNI_QUEUE_CFG_W0_GRP_ID		GENMASK(5, 4)
-#define PPE_AC_UNI_QUEUE_CFG_W0_PRE_LIMIT	GENMASK(16, 6)
-#define PPE_AC_UNI_QUEUE_CFG_W0_DYNAMIC		BIT(17)
-#define PPE_AC_UNI_QUEUE_CFG_W0_WEIGHT		GENMASK(20, 18)
-#define PPE_AC_UNI_QUEUE_CFG_W0_THRESHOLD	GENMASK(31, 21)
-#define PPE_AC_UNI_QUEUE_CFG_W3_GRN_RESUME	GENMASK(23, 13)
-
-#define PPE_AC_UNI_QUEUE_SET_EN(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_AC_UNI_QUEUE_CFG_W0_EN)
-#define PPE_AC_UNI_QUEUE_SET_GRP_ID(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_AC_UNI_QUEUE_CFG_W0_GRP_ID)
-#define PPE_AC_UNI_QUEUE_SET_PRE_LIMIT(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_AC_UNI_QUEUE_CFG_W0_PRE_LIMIT)
-#define PPE_AC_UNI_QUEUE_SET_DYNAMIC(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_AC_UNI_QUEUE_CFG_W0_DYNAMIC)
-#define PPE_AC_UNI_QUEUE_SET_WEIGHT(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_AC_UNI_QUEUE_CFG_W0_WEIGHT)
-#define PPE_AC_UNI_QUEUE_SET_THRESHOLD(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_AC_UNI_QUEUE_CFG_W0_THRESHOLD)
-#define PPE_AC_UNI_QUEUE_SET_GRN_RESUME(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)(tbl_cfg) + 0x3, value, PPE_AC_UNI_QUEUE_CFG_W3_GRN_RESUME)
+#define PPE_AC_UNICAST_QUEUE_CFG_TBL_ADDR	0x848000
+#define PPE_AC_UNICAST_QUEUE_CFG_TBL_ENTRIES	256
+#define PPE_AC_UNICAST_QUEUE_CFG_TBL_INC	0x10
+#define PPE_AC_UNICAST_QUEUE_CFG_W0_EN		BIT(0)
+#define PPE_AC_UNICAST_QUEUE_CFG_W0_WRED_EN	BIT(1)
+#define PPE_AC_UNICAST_QUEUE_CFG_W0_FC_EN	BIT(2)
+#define PPE_AC_UNICAST_QUEUE_CFG_W0_CLR_AWARE	BIT(3)
+#define PPE_AC_UNICAST_QUEUE_CFG_W0_GRP_ID	GENMASK(5, 4)
+#define PPE_AC_UNICAST_QUEUE_CFG_W0_PRE_LIMIT	GENMASK(16, 6)
+#define PPE_AC_UNICAST_QUEUE_CFG_W0_DYNAMIC	BIT(17)
+#define PPE_AC_UNICAST_QUEUE_CFG_W0_WEIGHT	GENMASK(20, 18)
+#define PPE_AC_UNICAST_QUEUE_CFG_W0_THRESHOLD	GENMASK(31, 21)
+#define PPE_AC_UNICAST_QUEUE_CFG_W3_GRN_RESUME	GENMASK(23, 13)
+
+#define PPE_AC_UNICAST_QUEUE_SET_EN(tbl_cfg, value)	\
+	FIELD_MODIFY(PPE_AC_UNICAST_QUEUE_CFG_W0_EN, tbl_cfg, value)
+#define PPE_AC_UNICAST_QUEUE_SET_GRP_ID(tbl_cfg, value)	\
+	FIELD_MODIFY(PPE_AC_UNICAST_QUEUE_CFG_W0_GRP_ID, tbl_cfg, value)
+#define PPE_AC_UNICAST_QUEUE_SET_PRE_LIMIT(tbl_cfg, value)	\
+	FIELD_MODIFY(PPE_AC_UNICAST_QUEUE_CFG_W0_PRE_LIMIT, tbl_cfg, value)
+#define PPE_AC_UNICAST_QUEUE_SET_DYNAMIC(tbl_cfg, value)	\
+	FIELD_MODIFY(PPE_AC_UNICAST_QUEUE_CFG_W0_DYNAMIC, tbl_cfg, value)
+#define PPE_AC_UNICAST_QUEUE_SET_WEIGHT(tbl_cfg, value)	\
+	FIELD_MODIFY(PPE_AC_UNICAST_QUEUE_CFG_W0_WEIGHT, tbl_cfg, value)
+#define PPE_AC_UNICAST_QUEUE_SET_THRESHOLD(tbl_cfg, value)	\
+	FIELD_MODIFY(PPE_AC_UNICAST_QUEUE_CFG_W0_THRESHOLD, tbl_cfg, value)
+#define PPE_AC_UNICAST_QUEUE_SET_GRN_RESUME(tbl_cfg, value)	\
+	FIELD_MODIFY(PPE_AC_UNICAST_QUEUE_CFG_W3_GRN_RESUME, tbl_cfg + 0x3, value)
 
 /* PPE multicast queue (256-299) configurations. */
-#define PPE_AC_MUL_QUEUE_CFG_TBL_ADDR		0x84a000
-#define PPE_AC_MUL_QUEUE_CFG_TBL_NUM		44
-#define PPE_AC_MUL_QUEUE_CFG_TBL_INC		0x10
-#define PPE_AC_MUL_QUEUE_CFG_W0_EN		BIT(0)
-#define PPE_AC_MUL_QUEUE_CFG_W0_FC_EN		BIT(1)
-#define PPE_AC_MUL_QUEUE_CFG_W0_COLOR_AWARE	BIT(2)
-#define PPE_AC_MUL_QUEUE_CFG_W0_GRP_ID		GENMASK(4, 3)
-#define PPE_AC_MUL_QUEUE_CFG_W0_PRE_LIMIT	GENMASK(15, 5)
-#define PPE_AC_MUL_QUEUE_CFG_W0_THRESHOLD	GENMASK(26, 16)
-#define PPE_AC_MUL_QUEUE_CFG_W2_RESUME		GENMASK(17, 7)
-
-#define PPE_AC_MUL_QUEUE_SET_EN(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_AC_MUL_QUEUE_CFG_W0_EN)
-#define PPE_AC_MUL_QUEUE_SET_GRN_GRP_ID(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_AC_MUL_QUEUE_CFG_W0_GRP_ID)
-#define PPE_AC_MUL_QUEUE_SET_GRN_PRE_LIMIT(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_AC_MUL_QUEUE_CFG_W0_PRE_LIMIT)
-#define PPE_AC_MUL_QUEUE_SET_GRN_THRESHOLD(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)tbl_cfg, value, PPE_AC_MUL_QUEUE_CFG_W0_THRESHOLD)
-#define PPE_AC_MUL_QUEUE_SET_GRN_RESUME(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)(tbl_cfg) + 0x2, value, PPE_AC_MUL_QUEUE_CFG_W2_RESUME)
+#define PPE_AC_MULTICAST_QUEUE_CFG_TBL_ADDR	0x84a000
+#define PPE_AC_MULTICAST_QUEUE_CFG_TBL_ENTRIES	44
+#define PPE_AC_MULTICAST_QUEUE_CFG_TBL_INC	0x10
+#define PPE_AC_MULTICAST_QUEUE_CFG_W0_EN	BIT(0)
+#define PPE_AC_MULTICAST_QUEUE_CFG_W0_FC_EN	BIT(1)
+#define PPE_AC_MULTICAST_QUEUE_CFG_W0_CLR_AWARE	BIT(2)
+#define PPE_AC_MULTICAST_QUEUE_CFG_W0_GRP_ID	GENMASK(4, 3)
+#define PPE_AC_MULTICAST_QUEUE_CFG_W0_PRE_LIMIT	GENMASK(15, 5)
+#define PPE_AC_MULTICAST_QUEUE_CFG_W0_THRESHOLD	GENMASK(26, 16)
+#define PPE_AC_MULTICAST_QUEUE_CFG_W2_RESUME	GENMASK(17, 7)
+
+#define PPE_AC_MULTICAST_QUEUE_SET_EN(tbl_cfg, value)	\
+	FIELD_MODIFY(PPE_AC_MULTICAST_QUEUE_CFG_W0_EN, tbl_cfg, value)
+#define PPE_AC_MULTICAST_QUEUE_SET_GRN_GRP_ID(tbl_cfg, value)	\
+	FIELD_MODIFY(PPE_AC_MULTICAST_QUEUE_CFG_W0_GRP_ID, tbl_cfg, value)
+#define PPE_AC_MULTICAST_QUEUE_SET_GRN_PRE_LIMIT(tbl_cfg, value)	\
+	FIELD_MODIFY(PPE_AC_MULTICAST_QUEUE_CFG_W0_PRE_LIMIT, tbl_cfg, value)
+#define PPE_AC_MULTICAST_QUEUE_SET_GRN_THRESHOLD(tbl_cfg, value)	\
+	FIELD_MODIFY(PPE_AC_MULTICAST_QUEUE_CFG_W0_THRESHOLD, tbl_cfg, value)
+#define PPE_AC_MULTICAST_QUEUE_SET_GRN_RESUME(tbl_cfg, value)	\
+	FIELD_MODIFY(PPE_AC_MULTICAST_QUEUE_CFG_W2_RESUME, tbl_cfg + 0x2, value)
 
 /* PPE admission control group (0-3) configurations */
 #define PPE_AC_GRP_CFG_TBL_ADDR			0x84c000
-#define PPE_AC_GRP_CFG_TBL_NUM			0x4
+#define PPE_AC_GRP_CFG_TBL_ENTRIES		0x4
 #define PPE_AC_GRP_CFG_TBL_INC			0x10
 #define PPE_AC_GRP_W0_AC_EN			BIT(0)
 #define PPE_AC_GRP_W0_AC_FC_EN			BIT(1)
-#define PPE_AC_GRP_W0_COLOR_AWARE		BIT(2)
+#define PPE_AC_GRP_W0_CLR_AWARE			BIT(2)
 #define PPE_AC_GRP_W0_THRESHOLD_LOW		GENMASK(31, 25)
 #define PPE_AC_GRP_W1_THRESHOLD_HIGH		GENMASK(3, 0)
 #define PPE_AC_GRP_W1_BUF_LIMIT			GENMASK(14, 4)
@@ -549,20 +545,23 @@
 #define PPE_AC_GRP_W2_PRE_ALLOC			GENMASK(26, 16)
 
 #define PPE_AC_GRP_SET_BUF_LIMIT(tbl_cfg, value)	\
-	u32p_replace_bits((u32 *)(tbl_cfg) + 0x1, value, PPE_AC_GRP_W1_BUF_LIMIT)
+	FIELD_MODIFY(PPE_AC_GRP_W1_BUF_LIMIT, tbl_cfg + 0x1, value)
 
-#define PPE_AC_UNI_QUEUE_CNT_TBL_ADDR		0x84e000
-#define PPE_AC_UNI_QUEUE_CNT_TBL_NUM		256
-#define PPE_AC_UNI_QUEUE_CNT_TBL_INC		0x10
-#define PPE_AC_UNI_QUEUE_CNT_TBL_PEND_CNT	GENMASK(12, 0)
+/* Counters for packets handled by unicast queues (0-255). */
+#define PPE_AC_UNICAST_QUEUE_CNT_TBL_ADDR	0x84e000
+#define PPE_AC_UNICAST_QUEUE_CNT_TBL_ENTRIES	256
+#define PPE_AC_UNICAST_QUEUE_CNT_TBL_INC	0x10
+#define PPE_AC_UNICAST_QUEUE_CNT_TBL_PEND_CNT	GENMASK(12, 0)
 
-#define PPE_AC_MUL_QUEUE_CNT_TBL_ADDR		0x852000
-#define PPE_AC_MUL_QUEUE_CNT_TBL_NUM		44
-#define PPE_AC_MUL_QUEUE_CNT_TBL_INC		0x10
-#define PPE_AC_MUL_QUEUE_CNT_TBL_PEND_CNT	GENMASK(12, 0)
+/* Counters for packets handled by multicast queues (256-299). */
+#define PPE_AC_MULTICAST_QUEUE_CNT_TBL_ADDR	0x852000
+#define PPE_AC_MULTICAST_QUEUE_CNT_TBL_ENTRIES	44
+#define PPE_AC_MULTICAST_QUEUE_CNT_TBL_INC	0x10
+#define PPE_AC_MULTICAST_QUEUE_CNT_TBL_PEND_CNT	GENMASK(12, 0)
 
+/* Table addresses for per-queue enqueue setting. */
 #define PPE_ENQ_OPR_TBL_ADDR			0x85c000
-#define PPE_ENQ_OPR_TBL_NUM			300
+#define PPE_ENQ_OPR_TBL_ENTRIES			300
 #define PPE_ENQ_OPR_TBL_INC			0x10
 #define PPE_ENQ_OPR_TBL_ENQ_DISABLE		BIT(0)
 
@@ -602,7 +601,7 @@
 #define GMAC_ADDR_BYTE3				GENMASK(7, 0)
 
 /* GMAC control register */
-#define GMAC_CTRL_ADDR				0x18
+#define GMAC_CTRL0_ADDR				0x18
 #define GMAC_TX_THD_M				GENMASK(27, 24)
 #define GMAC_MAXFRAME_SIZE_M			GENMASK(21, 8)
 #define GMAC_CRS_SEL				BIT(6)
@@ -611,7 +610,7 @@
 	(GMAC_TX_THD_M | GMAC_MAXFRAME_SIZE_M | GMAC_CRS_SEL)
 
 /* GMAC debug control register */
-#define GMAC_DBG_CTRL_ADDR			0x1c
+#define GMAC_CTRL1_ADDR				0x1c
 #define GMAC_HIGH_IPG_M				GENMASK(15, 8)
 
 /* GMAC jumbo size register */
@@ -766,7 +765,7 @@
 #define XGMAC_RXBROAD_G_ADDR			0x918
 #define XGMAC_RXMULTI_G_ADDR			0x920
 #define XGMAC_RXCRC_ERR_ADDR			0x928
-#define XGMAC_RXRUNT_ERR_ADDR			0x930
+#define XGMAC_RXFRAG_ERR_ADDR			0x930
 #define XGMAC_RXJABBER_ERR_ADDR			0x934
 #define XGMAC_RXUNDERSIZE_G_ADDR		0x938
 #define XGMAC_RXOVERSIZE_G_ADDR			0x93C
@@ -880,7 +879,7 @@
 #define EDMA_REG_TX_MOD_TIMER(n)	(0x99008 + (0x1000 * (n)))
 #define EDMA_REG_TX_INT_CTRL(n)		(0x9900c + (0x1000 * (n)))
 
-/* EDMA_QID2RID_TABLE_MEM register field masks */
+/* EDMA_QID2RID_TABLE_MEM register (Rx queue to ring ID mapping) field masks */
 #define EDMA_RX_RING_ID_QUEUE0_MASK	GENMASK(7, 0)
 #define EDMA_RX_RING_ID_QUEUE1_MASK	GENMASK(15, 8)
 #define EDMA_RX_RING_ID_QUEUE2_MASK	GENMASK(23, 16)
@@ -908,7 +907,7 @@
 /* Rx Descriptor ring pre-header base address mask */
 #define EDMA_RXDESC_PREHEADER_BA_MASK		0xffffffff
 
-/* Tx descriptor prod ring index mask */
+/* Tx descriptor producer ring index mask */
 #define EDMA_TXDESC_PROD_IDX_MASK		0xffff
 
 /* Tx descriptor consumer ring index mask */
@@ -923,7 +922,7 @@
 #define EDMA_TXDESC_CTRL_TXEN_MASK		BIT(0)
 #define EDMA_TXDESC_CTRL_FC_GRP_ID_MASK		GENMASK(3, 1)
 
-/* Tx completion ring prod index mask */
+/* Tx completion ring producer index mask */
 #define EDMA_TXCMPL_PROD_IDX_MASK		0xffff
 
 /* Tx completion ring urgent threshold mask */
@@ -934,7 +933,7 @@
 #define EDMA_TX_MOD_TIMER_INIT_MASK		0xffff
 #define EDMA_TX_MOD_TIMER_INIT_SHIFT		0
 
-/* Rx fill ring prod index mask */
+/* Rx fill ring producer index mask */
 #define EDMA_RXFILL_PROD_IDX_MASK		0xffff
 
 /* Rx fill ring consumer index mask */
@@ -952,10 +951,10 @@
 /* Rx fill ring enable bit */
 #define EDMA_RXFILL_RING_EN			0x1
 
-/* Rx desc ring prod index mask */
+/* Rx desc ring producer index mask */
 #define EDMA_RXDESC_PROD_IDX_MASK		0xffff
 
-/* Rx descriptor ring cons index mask */
+/* Rx descriptor ring consumer index mask */
 #define EDMA_RXDESC_CONS_IDX_MASK		0xffff
 
 /* Rx descriptor ring size masks */
@@ -993,23 +992,23 @@
 /* EDMA Ring mask */
 #define EDMA_RING_DMA_MASK			0xffffffff
 
-/* RXDESC threshold interrupt. */
+/* Rx desc threshold interrupt. */
 #define EDMA_RXDESC_UGT_INT_STAT		0x2
 
-/* RXDESC timer interrupt */
+/* Rx desc timer interrupt */
 #define EDMA_RXDESC_PKT_INT_STAT		0x1
 
-/* RXDESC Interrupt status mask */
+/* Rx desc interrupt status mask */
 #define EDMA_RXDESC_RING_INT_STATUS_MASK \
 	(EDMA_RXDESC_UGT_INT_STAT | EDMA_RXDESC_PKT_INT_STAT)
 
-/* TXCMPL threshold interrupt. */
+/* Tx cmpl threshold interrupt. */
 #define EDMA_TXCMPL_UGT_INT_STAT		0x2
 
-/* TXCMPL timer interrupt */
+/* Tx cmpl timer interrupt */
 #define EDMA_TXCMPL_PKT_INT_STAT		0x1
 
-/* TXCMPL Interrupt status mask */
+/* Tx cmpl interrupt status mask */
 #define EDMA_TXCMPL_RING_INT_STATUS_MASK \
 	(EDMA_TXCMPL_UGT_INT_STAT | EDMA_TXCMPL_PKT_INT_STAT)
 
-- 
2.34.1

